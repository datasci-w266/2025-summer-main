{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoJTp7u3BCED"
   },
   "source": [
    "# ModernBert\n",
    "\n",
    "ModernBERT was published in late 2024 and has shown substantial improvements over other BERT family models (BERT, RoBERTa, ALBERT, etc.). This notebook showcases the improvements of ModernBERT compared to BERT. Specifically, we will look at the differences in tokenization, long-context capability, model architecture, model outputs, and inference speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjE9cXscBCBd"
   },
   "source": [
    "Here are some helpful resources:\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/en/model_doc/modernbert\n",
    "\n",
    "https://huggingface.co/blog/modernbert\n",
    "\n",
    "https://huggingface.co/answerdotai/ModernBERT-base\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBC8gpQ5DIRa"
   },
   "source": [
    "# Import libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbBenrubMO1f",
    "outputId": "0e20b049-0856-4847-a632-3285efe36a09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers\n",
    "!pip install -q -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "05q2b5scAiPv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHO7UTRwDKnt"
   },
   "source": [
    "# Load models & tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "a8cd01179d654e159bc05cf486730240",
      "543a8bd8951d48feb50da1c0195d288e",
      "df5c9811ed3a44f1aa5083f3e7c171f2",
      "d2128a8bff1b460cb1ac24a1ca1de136",
      "6930737908d04267a1b700fbe97858df",
      "a05536b4ca0441ff90d94632b2b1a810",
      "714eebb7116849b89bc739abf4883ebe",
      "9738898ed85b4bab807641e13b5862c1",
      "0d6d3137d636459db012065cc086db6c",
      "3dad417afca046f4bee65d68b9fa6908",
      "d279db4e1f6a44fb887daead29a56543",
      "e664d2dc949247dab2537be3fc2e99f6",
      "c84ac2cdb32244ab855be67eae76bbd4",
      "465438bd3cc54d09a8514529eafb6869",
      "020d984df5374875b7846db5d7d922e6",
      "0ad1fd94fc304db7a834e0b9e1cc70ad",
      "99fd36d8b83b481b83c6718df74e6201",
      "f4ab13dd0f13456592250ffccbcdd64f",
      "96556cec18ab447e898b240d0e7bde55",
      "482995fcccbe4862b922afccf7080147",
      "893dc027e9ce417da1d708378dd2f852",
      "a5ac868eb65d4e1fa83ef3cd76fa9ec1",
      "47ec014e68f445ad9f779f12c4f1b227",
      "bb34486908e34de0a8f8d4788116e86b",
      "62415ce1edfa4056856c53ead0a53836",
      "db022a6e77b34c53b6e4e64eb1042e5d",
      "7de4cb7491e94285957d433a13db0b16",
      "faf3ec8a621d43e1aff967798417eb68",
      "e4635aac37864438a8470e6d92de84f2",
      "bad07629e75a4d72aa8fac7691a98f6b",
      "cc72eb580cc0403793ffd8698093b5eb",
      "fdb50ad499704db98e22107b4bccea25",
      "9d2f6a68d9c64469b27f9ddc5c3a4ea1",
      "97181748ff6647b7bf1d5d637976f518",
      "058844604fa7438a88e93cb5baff162c",
      "f9fad2ad955c4e73b58933a487ec4993",
      "833a21cc01cf49a4bb5651b4322c8d6a",
      "00869a2b3ae0497da6593e1b787bbae7",
      "52a33c09a6524da6ad6ef0e633bf46c4",
      "8b2f5d060187460390738af7cc9ab648",
      "4b519f8eece44aa8bede7c2b3bda76ac",
      "09e3669d4c4b478a80f1ef8c65e035cf",
      "aab7071044944834a33e2295c210cf99",
      "f62b0f06991147ec843e4684b23fccfd",
      "93f6146c4e1c4ddd94c75d96dc467664",
      "8ea917d9e2d54621bfb19168136ac3ea",
      "ee48e2fbaf644b5eafcce0830bff9f2f",
      "c326714c595c4946b61697966c3f5dd7",
      "d5d83eaca8a34c9bbe008cfbbde413cb",
      "3728e0996cba4472bed99615c36917a6",
      "55e8f701e3f04c17b1300b6588728b8b",
      "ecb66adea5ad47828c298e738550c06f",
      "764cc4e954c24addbb616fbce537cbff",
      "a7b03b1d773f4deda4ba87a836e6e45f",
      "86b0a097ba2e43628df58bf294abb322"
     ]
    },
    "id": "OyFPKU6EDU4C",
    "outputId": "df1e83c5-bcfa-404f-a01d-a15cf29fbb11"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8cd01179d654e159bc05cf486730240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e664d2dc949247dab2537be3fc2e99f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ec014e68f445ad9f779f12c4f1b227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97181748ff6647b7bf1d5d637976f518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f6146c4e1c4ddd94c75d96dc467664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's first load the BERT model\n",
    "bert_checkpoint = \"bert-base-cased\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_checkpoint)\n",
    "bert_model = BertModel.from_pretrained(bert_checkpoint)\n",
    "\n",
    "# BERT tokenizer and model can also be loaded with AutoTokenizer and AutoModel\n",
    "#bert_tokenizer = AutoTokenizer.from_pretrained(bert_checkpoint)\n",
    "#bert_model = AutoModel.from_pretrained(bert_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "1528322d54344644abaf18c753380292",
      "212421b9d9ba4aee9286899c59e4040e",
      "65ab5f830f4247faa0946f0ef93e0821",
      "d4594f4d4ed84b2db2e3e402493e4903",
      "0e895362994346e3bc45bbf9ce9a49d7",
      "20d5699750ab42219c84bd50b5443113",
      "dda624b410f644349869c1d555d3fe25",
      "df28c769b8a94401a67b8c03e2daee43",
      "85e90dbf27654260bfcfc2f3dc78c23e",
      "561b27855bcb4b5394dc7a2a6cd962a9",
      "6ac46a6956fe4eb18c599905aa3deddd",
      "a2aeff11aeb04ab39952db62d2d33464",
      "d1dc11b9d8624858a357f3a6c5f6cd0a",
      "55db8545a90c4249a036f4b86722a46c",
      "91b0adcd22b7430290c111e009d93021",
      "04b3857b4ada49dab096fa7071ee2ca3",
      "84f963951bc14ad5aaf8028d1c16550f",
      "18c5e676814c4dce892828411f1baa6f",
      "f42cbd8f3f394cdfb080feab23952e8c",
      "6c00f586606e49bbb184f7777e5aa069",
      "25fa0c604551476697932937a357f72d",
      "aaca190d6f594dd6a73957e1e105bcf1",
      "e4cd6a2ee9264848a092ffd1601e16a8",
      "00b475356e1944489e8544d963436d5b",
      "af85c638e0274d5887035861dddd0cc5",
      "803fab873ac7440b9381ca186fc47a0b",
      "5fe03b3fd6a24baa9d283c22153ea3ef",
      "d6ef7f65d26a4d909b2298cbf2652aec",
      "a9c6535a55a24b27959e179e55c09935",
      "57360cfe81154aefa2c7c309ae43a4a0",
      "81c7c5bb6bf64e9e88164b90dfb1f4b2",
      "8b2347fc8fc94bddbf061407a316cec7",
      "c028eb3b2e96410f9ab1d4219d9248ad",
      "15220a373fb84f038492a8c6c8ced01c",
      "a2b996583b6e44c19449347d4c0c4599",
      "4cc1a760c20945e3869b55fb9c5028ad",
      "6cad77ca0cd64d0080562f7218b5b969",
      "abb9ac7df35f4bab9f022b2a4e849e2f",
      "58797f39539842ed81c7d7257621b43a",
      "aad03b881fcb4689a19017d900bdb80d",
      "97745f29938b4a77b8a2ffbf66068bb9",
      "f398d928c52d47a0b3c5dcd6f27a9a0f",
      "9b44ac30d1a94c1e96c38f1a2c501ebf",
      "d4fb42cc553645dab0a4294318e1acf3",
      "f5bad1ec4cff46b5939ce7041435177b",
      "91249e9a4bc845af88f2bec06138eb00",
      "18e1add33cc84ee0afe17ebd02e34743",
      "7b7d24c36e294490a464deff9eca73e9",
      "b0fdcff01bc84eb38a1e82e27146ac24",
      "5cc9ea1f316e4149a35a33ad892cf454",
      "200cdeb3a28c42f484b85a15abcd3c9c",
      "71d9edda4e394693b84b05c1b250f39a",
      "3cb59635acf94dd690e8723045111247",
      "2323d9d062f845a599769719067cac8e",
      "79949ce92da74c64bf8f1f2715b131f2"
     ]
    },
    "id": "PeE7JMzdDLo-",
    "outputId": "a71f6251-c3fa-4867-8a8d-a183e2c43b5b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1528322d54344644abaf18c753380292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2aeff11aeb04ab39952db62d2d33464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cd6a2ee9264848a092ffd1601e16a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15220a373fb84f038492a8c6c8ced01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bad1ec4cff46b5939ce7041435177b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Then we will load the ModernBERT model\n",
    "mbert_checkpoint = \"answerdotai/ModernBERT-base\"\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(mbert_checkpoint)\n",
    "mbert_model = AutoModel.from_pretrained(mbert_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3w95OY56Dr8Z"
   },
   "source": [
    "# Tokenizer\n",
    "\n",
    "Let's first take a look at the difference between the tokenizers for the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Z5VaDgbHCgaX"
   },
   "outputs": [],
   "source": [
    "text = \"This is MIDS 266. Let's learn some NLP!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uptoXfkfDxne",
    "outputId": "eed00311-ab0b-4904-9b9f-9872a53f83e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1188,  1110, 26574, 13675,  1744,  1545,   119,  2421,   112,\n",
       "           188,  3858,  1199, 21239,  2101,   106,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs = bert_tokenizer(text, return_tensors=\"pt\")\n",
    "bert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5sgWdXbJXuq",
    "outputId": "9754f25f-7672-4fcc-e72c-1e37ad5bba0b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50281,  1552,   310,   353, 15782, 30610,    15,  1281,   434,  3037,\n",
       "           690,   427, 13010,     2, 50282]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbert_inputs = mbert_tokenizer(text, return_tensors=\"pt\")\n",
    "mbert_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4U2j3XZ2JuCs"
   },
   "source": [
    "What do we notice at first glance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi9L8BZkKJ8i"
   },
   "source": [
    "Let's now take a closer look at the `input_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIrNFeOWKJZO",
    "outputId": "5a04c7c6-9cc6-428d-9de3-6c7a617f3067"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 17])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "szGrYMWMJXxT",
    "outputId": "843d722e-6367-4e3f-f2b1-1c47936b17d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbert_inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bSIxPDFQ9Xw"
   },
   "source": [
    "We can see that the two models have different length of input_ids for texts with the same word count!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Yb7JOUbSl2U"
   },
   "source": [
    "Let's then take a look at how the tokenization differs between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P3RC7LlYRPKC",
    "outputId": "ed749ece-c684-491a-fc1a-a7c25b7ac621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'MI', '##DS', '26', '##6', '.', 'Let', \"'\", 's', 'learn', 'some', 'NL', '##P', '!']\n"
     ]
    }
   ],
   "source": [
    "btokens = bert_tokenizer.tokenize(text)\n",
    "print(btokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8jr25jVRUX4",
    "outputId": "714a8be2-b58c-4089-f896-2211169b7527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'Ġis', 'ĠM', 'IDS', 'Ġ266', '.', 'ĠLet', \"'s\", 'Ġlearn', 'Ġsome', 'ĠN', 'LP', '!']\n"
     ]
    }
   ],
   "source": [
    "mtokens = mbert_tokenizer.tokenize(text)\n",
    "print(mtokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuvm7hjzTB_J"
   },
   "source": [
    "Wow! The tokens look quite different between the two models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfZFf9cIStME"
   },
   "source": [
    "We already know BERT uses CLS and SEP tokens, does ModernBERT do the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j3PLmWmQQ45k",
    "outputId": "1a59e805-2175-49a0-ff4a-d01c8dc52ffc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS] This is MIDS 266. Let's learn some NLP! [SEP]\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(bert_tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5IHm_tyrU_8i",
    "outputId": "4ab176d2-fea9-4051-b107-7fbc25582f97"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1188,  1110, 26574, 13675,  1744,  1545,   119,  2421,   112,\n",
       "           188,  3858,  1199, 21239,  2101,   106,   102]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "l0Khe8cTS4to",
    "outputId": "ab7ea471-6dd2-44c1-ee5c-cf83a1b88299"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"[CLS]This is MIDS 266. Let's learn some NLP![SEP]\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbert_tokenizer.decode(mbert_tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JH7FSAK4VKHU",
    "outputId": "d8998573-c6fe-41ce-836c-6dda2018bfc7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50281,  1552,   310,   353, 15782, 30610,    15,  1281,   434,  3037,\n",
       "           690,   427, 13010,     2, 50282]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbert_inputs.input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu5rE7nqTG-Y"
   },
   "source": [
    "Similar to other BERT family models, ModernBERT also uses CLS and SEP tokens. Can you guess the input_id for these special tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atBW3qOLUWyL"
   },
   "source": [
    "Let's now try batch encode, what's different now?\n",
    "\n",
    "Read the [ModernBert Config](https://huggingface.co/docs/transformers/main/en/model_doc/modernbert#transformers.ModernBertConfig) to identify other special tokens and the input ids for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_JRuJ4lUB2a",
    "outputId": "5e542d8b-8f0f-4f8e-d895-44cf61631c24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1188, 1110, 1632,  106,  102,    0,    0,    0,    0],\n",
       "        [ 101, 1188, 1110, 6434,  106,  102,    0,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_input = bert_tokenizer.batch_encode_plus(\n",
    "    ['This is great!', 'This is terrible!'],\n",
    "    max_length=10,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "bert_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J0bfO5EuUFw0",
    "outputId": "3cb1e192-033f-4acc-a24b-96ccd44417a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50281,  1552,   310,  1270,     2, 50282, 50283, 50283, 50283, 50283],\n",
       "        [50281,  1552,   310, 11527,     2, 50282, 50283, 50283, 50283, 50283]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbert_input = mbert_tokenizer.batch_encode_plus(\n",
    "    ['This is great!', 'This is terrible!'],\n",
    "    max_length=10,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "mbert_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnzgfXT4ikRq"
   },
   "source": [
    "# Long Context Inputs\n",
    "\n",
    "For long context illustration, we will use the [long-context retrieval (MLDR)](https://huggingface.co/datasets/sentence-transformers/mldr) dataset. This dataset has 10K triples of anchor-positive-negative datapoints, and is ideal for information retrieval. We will cover information retrieval in Week 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwDJZJADj_Kg"
   },
   "source": [
    "We can directly load the dataset from HuggingFace Hub. For this exercise, we will only take the first 5 datapoints as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "35e0aacae77d49a0b0cf92db090a4b4e",
      "8e16cf55301449e49a2617b59939c1e5",
      "0ce011ab925448fdba8ae15771817341",
      "ec16e4fa892740e59d089808b6228a2f",
      "4bc09d194172437abe1d90ef3c46802b",
      "4e8f5dfedb20407f8eb1c3343f7bebdf",
      "6a8b285f00bf4b0091abe88604a7cbea",
      "840507221a754b8db8c720c441ea447d",
      "1ecfb23aeae8481ebd49d60d8f97fe53",
      "e4f4b88631bf489ea633132713b31fbf",
      "7b76999cc8b04ca8a82b3f2efd6a684e",
      "1580c5fbd2754ca3818378b3bc349cb6",
      "f27af4aafe944be8b75640217e14e29a",
      "14cc8b19234248ca85f201585af017bf",
      "0b0d1c32d7c94b05ae4f68e2869f3186",
      "4446e1fc84ef4e24826b5b7bb0404f3b",
      "d98cc98fb7dd4d7f944b6f8dcb8cdea7",
      "abe30513be54490a977c3b60aa0ce657",
      "fd3f5022fa1547199a7ea473b975d02c",
      "343f981bb6c145ea99714d30ba44603c",
      "5b7f952884f8487ca6e6946413acac60",
      "82703c7b2115421b87dfe326af9d73d8",
      "bd51e9cbffab41508c5521bab6b93fcc",
      "2270139a0d1940bc92f6ef37d81f4604",
      "f099a247877f4ec7b80baa89e94e3aac",
      "0c3a3c616ee946bc8e23559fbc482a56",
      "29a3096fba52444ab050240ee00bd9d4",
      "73ea98583e3c473796b9120c0a203544",
      "1e2d7e5688a949d19a446446027324e5",
      "092442ad65c84240affe71a65fc1154f",
      "57a82b0803fa470e90a0d9e28d188fc3",
      "9b7b9ef3ad0145d3aadbfac165d1be29",
      "0390aef1d5344241ac65dc639cdd4de0"
     ]
    },
    "id": "Kr_0fbXmixPN",
    "outputId": "6c706188-4fee-48c0-d3af-6bd37ebb23e1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e0aacae77d49a0b0cf92db090a4b4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/236k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1580c5fbd2754ca3818378b3bc349cb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "en-triplet/train-00000-of-00001.parquet:   0%|          | 0.00/211M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd51e9cbffab41508c5521bab6b93fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"sentence-transformers/mldr\", \"en-triplet\", split=\"train\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jt3xyTsvjJ_j",
    "outputId": "67a98a23-28f2-42b5-a2aa-6e8e2344e5f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['anchor', 'positive', 'negative'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YD8-rMjdkoJ8"
   },
   "source": [
    "This dataset is designed for long context retrieval, let's take a look at the first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "il7HELlKimRo",
    "outputId": "6841afaf-f120-4ed6-c8ee-3d493983a6f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12379"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dataset[0][\"positive\"]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRvMdfYrk_sf"
   },
   "source": [
    "Wow this is surely a long text, what happens if we try to tokenize it for BERT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wV-hR6pHimK7",
    "outputId": "d99a835e-92ae-4d70-efa2-155545c08adc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2450 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1287,  5665,  ...,  4052, 12762,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs = bert_tokenizer(text, return_tensors=\"pt\")\n",
    "bert_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTCD4KmulF7i"
   },
   "source": [
    "Uhoh, we see a warning that the sequence length exceeds the max sequence length allowed for the model.\n",
    "\n",
    "What happens if we directly use this long text in the BERT model without any preprocessing? All the texts after the 512th token will be lost!\n",
    "\n",
    "Think about what you can do to combat this issue if you'd like to use BERT model on this dataset? What disadvantage would this impose compared to using a model that has long context capabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8n_3ny7lfLv"
   },
   "source": [
    "Now let's tokenize it for ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1P6ifdkHimAD",
    "outputId": "bec71685-9078-4d33-bbec-1a5a5a71ea5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[50281,  8732, 26456,  ..., 13416, 20759, 50282]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbert_inputs = mbert_tokenizer(text, return_tensors=\"pt\")\n",
    "mbert_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQQK4n0Olm7O",
    "outputId": "87b89c45-6402-4f3b-abbd-a1b068a2c316"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2671])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbert_inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tNa3MXelpve"
   },
   "source": [
    "No more warnings about the sequence length exceeding the maximum sequence length. This is because ModernBERT allows for long context up to 8192 tokens! This is huge!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avVr_sbtTind"
   },
   "source": [
    "# Model Architecture\n",
    "\n",
    "Let's now take a look at the model architecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StEbfZ4zToni",
    "outputId": "17c6404b-9f74-4e38-b76b-d492075fbf1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight\n",
      "embeddings.position_embeddings.weight\n",
      "embeddings.token_type_embeddings.weight\n",
      "embeddings.LayerNorm.weight\n",
      "embeddings.LayerNorm.bias\n",
      "encoder.layer.0.attention.self.query.weight\n",
      "encoder.layer.0.attention.self.query.bias\n",
      "encoder.layer.0.attention.self.key.weight\n",
      "encoder.layer.0.attention.self.key.bias\n",
      "encoder.layer.0.attention.self.value.weight\n",
      "encoder.layer.0.attention.self.value.bias\n",
      "encoder.layer.0.attention.output.dense.weight\n",
      "encoder.layer.0.attention.output.dense.bias\n",
      "encoder.layer.0.attention.output.LayerNorm.weight\n",
      "encoder.layer.0.attention.output.LayerNorm.bias\n",
      "encoder.layer.0.intermediate.dense.weight\n",
      "encoder.layer.0.intermediate.dense.bias\n",
      "encoder.layer.0.output.dense.weight\n",
      "encoder.layer.0.output.dense.bias\n",
      "encoder.layer.0.output.LayerNorm.weight\n",
      "encoder.layer.0.output.LayerNorm.bias\n",
      "encoder.layer.1.attention.self.query.weight\n",
      "encoder.layer.1.attention.self.query.bias\n",
      "encoder.layer.1.attention.self.key.weight\n",
      "encoder.layer.1.attention.self.key.bias\n",
      "encoder.layer.1.attention.self.value.weight\n",
      "encoder.layer.1.attention.self.value.bias\n",
      "encoder.layer.1.attention.output.dense.weight\n",
      "encoder.layer.1.attention.output.dense.bias\n",
      "encoder.layer.1.attention.output.LayerNorm.weight\n",
      "encoder.layer.1.attention.output.LayerNorm.bias\n",
      "encoder.layer.1.intermediate.dense.weight\n",
      "encoder.layer.1.intermediate.dense.bias\n",
      "encoder.layer.1.output.dense.weight\n",
      "encoder.layer.1.output.dense.bias\n",
      "encoder.layer.1.output.LayerNorm.weight\n",
      "encoder.layer.1.output.LayerNorm.bias\n",
      "encoder.layer.2.attention.self.query.weight\n",
      "encoder.layer.2.attention.self.query.bias\n",
      "encoder.layer.2.attention.self.key.weight\n",
      "encoder.layer.2.attention.self.key.bias\n",
      "encoder.layer.2.attention.self.value.weight\n",
      "encoder.layer.2.attention.self.value.bias\n",
      "encoder.layer.2.attention.output.dense.weight\n",
      "encoder.layer.2.attention.output.dense.bias\n",
      "encoder.layer.2.attention.output.LayerNorm.weight\n",
      "encoder.layer.2.attention.output.LayerNorm.bias\n",
      "encoder.layer.2.intermediate.dense.weight\n",
      "encoder.layer.2.intermediate.dense.bias\n",
      "encoder.layer.2.output.dense.weight\n",
      "encoder.layer.2.output.dense.bias\n",
      "encoder.layer.2.output.LayerNorm.weight\n",
      "encoder.layer.2.output.LayerNorm.bias\n",
      "encoder.layer.3.attention.self.query.weight\n",
      "encoder.layer.3.attention.self.query.bias\n",
      "encoder.layer.3.attention.self.key.weight\n",
      "encoder.layer.3.attention.self.key.bias\n",
      "encoder.layer.3.attention.self.value.weight\n",
      "encoder.layer.3.attention.self.value.bias\n",
      "encoder.layer.3.attention.output.dense.weight\n",
      "encoder.layer.3.attention.output.dense.bias\n",
      "encoder.layer.3.attention.output.LayerNorm.weight\n",
      "encoder.layer.3.attention.output.LayerNorm.bias\n",
      "encoder.layer.3.intermediate.dense.weight\n",
      "encoder.layer.3.intermediate.dense.bias\n",
      "encoder.layer.3.output.dense.weight\n",
      "encoder.layer.3.output.dense.bias\n",
      "encoder.layer.3.output.LayerNorm.weight\n",
      "encoder.layer.3.output.LayerNorm.bias\n",
      "encoder.layer.4.attention.self.query.weight\n",
      "encoder.layer.4.attention.self.query.bias\n",
      "encoder.layer.4.attention.self.key.weight\n",
      "encoder.layer.4.attention.self.key.bias\n",
      "encoder.layer.4.attention.self.value.weight\n",
      "encoder.layer.4.attention.self.value.bias\n",
      "encoder.layer.4.attention.output.dense.weight\n",
      "encoder.layer.4.attention.output.dense.bias\n",
      "encoder.layer.4.attention.output.LayerNorm.weight\n",
      "encoder.layer.4.attention.output.LayerNorm.bias\n",
      "encoder.layer.4.intermediate.dense.weight\n",
      "encoder.layer.4.intermediate.dense.bias\n",
      "encoder.layer.4.output.dense.weight\n",
      "encoder.layer.4.output.dense.bias\n",
      "encoder.layer.4.output.LayerNorm.weight\n",
      "encoder.layer.4.output.LayerNorm.bias\n",
      "encoder.layer.5.attention.self.query.weight\n",
      "encoder.layer.5.attention.self.query.bias\n",
      "encoder.layer.5.attention.self.key.weight\n",
      "encoder.layer.5.attention.self.key.bias\n",
      "encoder.layer.5.attention.self.value.weight\n",
      "encoder.layer.5.attention.self.value.bias\n",
      "encoder.layer.5.attention.output.dense.weight\n",
      "encoder.layer.5.attention.output.dense.bias\n",
      "encoder.layer.5.attention.output.LayerNorm.weight\n",
      "encoder.layer.5.attention.output.LayerNorm.bias\n",
      "encoder.layer.5.intermediate.dense.weight\n",
      "encoder.layer.5.intermediate.dense.bias\n",
      "encoder.layer.5.output.dense.weight\n",
      "encoder.layer.5.output.dense.bias\n",
      "encoder.layer.5.output.LayerNorm.weight\n",
      "encoder.layer.5.output.LayerNorm.bias\n",
      "encoder.layer.6.attention.self.query.weight\n",
      "encoder.layer.6.attention.self.query.bias\n",
      "encoder.layer.6.attention.self.key.weight\n",
      "encoder.layer.6.attention.self.key.bias\n",
      "encoder.layer.6.attention.self.value.weight\n",
      "encoder.layer.6.attention.self.value.bias\n",
      "encoder.layer.6.attention.output.dense.weight\n",
      "encoder.layer.6.attention.output.dense.bias\n",
      "encoder.layer.6.attention.output.LayerNorm.weight\n",
      "encoder.layer.6.attention.output.LayerNorm.bias\n",
      "encoder.layer.6.intermediate.dense.weight\n",
      "encoder.layer.6.intermediate.dense.bias\n",
      "encoder.layer.6.output.dense.weight\n",
      "encoder.layer.6.output.dense.bias\n",
      "encoder.layer.6.output.LayerNorm.weight\n",
      "encoder.layer.6.output.LayerNorm.bias\n",
      "encoder.layer.7.attention.self.query.weight\n",
      "encoder.layer.7.attention.self.query.bias\n",
      "encoder.layer.7.attention.self.key.weight\n",
      "encoder.layer.7.attention.self.key.bias\n",
      "encoder.layer.7.attention.self.value.weight\n",
      "encoder.layer.7.attention.self.value.bias\n",
      "encoder.layer.7.attention.output.dense.weight\n",
      "encoder.layer.7.attention.output.dense.bias\n",
      "encoder.layer.7.attention.output.LayerNorm.weight\n",
      "encoder.layer.7.attention.output.LayerNorm.bias\n",
      "encoder.layer.7.intermediate.dense.weight\n",
      "encoder.layer.7.intermediate.dense.bias\n",
      "encoder.layer.7.output.dense.weight\n",
      "encoder.layer.7.output.dense.bias\n",
      "encoder.layer.7.output.LayerNorm.weight\n",
      "encoder.layer.7.output.LayerNorm.bias\n",
      "encoder.layer.8.attention.self.query.weight\n",
      "encoder.layer.8.attention.self.query.bias\n",
      "encoder.layer.8.attention.self.key.weight\n",
      "encoder.layer.8.attention.self.key.bias\n",
      "encoder.layer.8.attention.self.value.weight\n",
      "encoder.layer.8.attention.self.value.bias\n",
      "encoder.layer.8.attention.output.dense.weight\n",
      "encoder.layer.8.attention.output.dense.bias\n",
      "encoder.layer.8.attention.output.LayerNorm.weight\n",
      "encoder.layer.8.attention.output.LayerNorm.bias\n",
      "encoder.layer.8.intermediate.dense.weight\n",
      "encoder.layer.8.intermediate.dense.bias\n",
      "encoder.layer.8.output.dense.weight\n",
      "encoder.layer.8.output.dense.bias\n",
      "encoder.layer.8.output.LayerNorm.weight\n",
      "encoder.layer.8.output.LayerNorm.bias\n",
      "encoder.layer.9.attention.self.query.weight\n",
      "encoder.layer.9.attention.self.query.bias\n",
      "encoder.layer.9.attention.self.key.weight\n",
      "encoder.layer.9.attention.self.key.bias\n",
      "encoder.layer.9.attention.self.value.weight\n",
      "encoder.layer.9.attention.self.value.bias\n",
      "encoder.layer.9.attention.output.dense.weight\n",
      "encoder.layer.9.attention.output.dense.bias\n",
      "encoder.layer.9.attention.output.LayerNorm.weight\n",
      "encoder.layer.9.attention.output.LayerNorm.bias\n",
      "encoder.layer.9.intermediate.dense.weight\n",
      "encoder.layer.9.intermediate.dense.bias\n",
      "encoder.layer.9.output.dense.weight\n",
      "encoder.layer.9.output.dense.bias\n",
      "encoder.layer.9.output.LayerNorm.weight\n",
      "encoder.layer.9.output.LayerNorm.bias\n",
      "encoder.layer.10.attention.self.query.weight\n",
      "encoder.layer.10.attention.self.query.bias\n",
      "encoder.layer.10.attention.self.key.weight\n",
      "encoder.layer.10.attention.self.key.bias\n",
      "encoder.layer.10.attention.self.value.weight\n",
      "encoder.layer.10.attention.self.value.bias\n",
      "encoder.layer.10.attention.output.dense.weight\n",
      "encoder.layer.10.attention.output.dense.bias\n",
      "encoder.layer.10.attention.output.LayerNorm.weight\n",
      "encoder.layer.10.attention.output.LayerNorm.bias\n",
      "encoder.layer.10.intermediate.dense.weight\n",
      "encoder.layer.10.intermediate.dense.bias\n",
      "encoder.layer.10.output.dense.weight\n",
      "encoder.layer.10.output.dense.bias\n",
      "encoder.layer.10.output.LayerNorm.weight\n",
      "encoder.layer.10.output.LayerNorm.bias\n",
      "encoder.layer.11.attention.self.query.weight\n",
      "encoder.layer.11.attention.self.query.bias\n",
      "encoder.layer.11.attention.self.key.weight\n",
      "encoder.layer.11.attention.self.key.bias\n",
      "encoder.layer.11.attention.self.value.weight\n",
      "encoder.layer.11.attention.self.value.bias\n",
      "encoder.layer.11.attention.output.dense.weight\n",
      "encoder.layer.11.attention.output.dense.bias\n",
      "encoder.layer.11.attention.output.LayerNorm.weight\n",
      "encoder.layer.11.attention.output.LayerNorm.bias\n",
      "encoder.layer.11.intermediate.dense.weight\n",
      "encoder.layer.11.intermediate.dense.bias\n",
      "encoder.layer.11.output.dense.weight\n",
      "encoder.layer.11.output.dense.bias\n",
      "encoder.layer.11.output.LayerNorm.weight\n",
      "encoder.layer.11.output.LayerNorm.bias\n",
      "pooler.dense.weight\n",
      "pooler.dense.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in bert_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xrxxkjMSToqA",
    "outputId": "85b9865e-f691-469e-84fa-aab80e9a44fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.tok_embeddings.weight\n",
      "embeddings.norm.weight\n",
      "layers.0.attn.Wqkv.weight\n",
      "layers.0.attn.Wo.weight\n",
      "layers.0.mlp_norm.weight\n",
      "layers.0.mlp.Wi.weight\n",
      "layers.0.mlp.Wo.weight\n",
      "layers.1.attn_norm.weight\n",
      "layers.1.attn.Wqkv.weight\n",
      "layers.1.attn.Wo.weight\n",
      "layers.1.mlp_norm.weight\n",
      "layers.1.mlp.Wi.weight\n",
      "layers.1.mlp.Wo.weight\n",
      "layers.2.attn_norm.weight\n",
      "layers.2.attn.Wqkv.weight\n",
      "layers.2.attn.Wo.weight\n",
      "layers.2.mlp_norm.weight\n",
      "layers.2.mlp.Wi.weight\n",
      "layers.2.mlp.Wo.weight\n",
      "layers.3.attn_norm.weight\n",
      "layers.3.attn.Wqkv.weight\n",
      "layers.3.attn.Wo.weight\n",
      "layers.3.mlp_norm.weight\n",
      "layers.3.mlp.Wi.weight\n",
      "layers.3.mlp.Wo.weight\n",
      "layers.4.attn_norm.weight\n",
      "layers.4.attn.Wqkv.weight\n",
      "layers.4.attn.Wo.weight\n",
      "layers.4.mlp_norm.weight\n",
      "layers.4.mlp.Wi.weight\n",
      "layers.4.mlp.Wo.weight\n",
      "layers.5.attn_norm.weight\n",
      "layers.5.attn.Wqkv.weight\n",
      "layers.5.attn.Wo.weight\n",
      "layers.5.mlp_norm.weight\n",
      "layers.5.mlp.Wi.weight\n",
      "layers.5.mlp.Wo.weight\n",
      "layers.6.attn_norm.weight\n",
      "layers.6.attn.Wqkv.weight\n",
      "layers.6.attn.Wo.weight\n",
      "layers.6.mlp_norm.weight\n",
      "layers.6.mlp.Wi.weight\n",
      "layers.6.mlp.Wo.weight\n",
      "layers.7.attn_norm.weight\n",
      "layers.7.attn.Wqkv.weight\n",
      "layers.7.attn.Wo.weight\n",
      "layers.7.mlp_norm.weight\n",
      "layers.7.mlp.Wi.weight\n",
      "layers.7.mlp.Wo.weight\n",
      "layers.8.attn_norm.weight\n",
      "layers.8.attn.Wqkv.weight\n",
      "layers.8.attn.Wo.weight\n",
      "layers.8.mlp_norm.weight\n",
      "layers.8.mlp.Wi.weight\n",
      "layers.8.mlp.Wo.weight\n",
      "layers.9.attn_norm.weight\n",
      "layers.9.attn.Wqkv.weight\n",
      "layers.9.attn.Wo.weight\n",
      "layers.9.mlp_norm.weight\n",
      "layers.9.mlp.Wi.weight\n",
      "layers.9.mlp.Wo.weight\n",
      "layers.10.attn_norm.weight\n",
      "layers.10.attn.Wqkv.weight\n",
      "layers.10.attn.Wo.weight\n",
      "layers.10.mlp_norm.weight\n",
      "layers.10.mlp.Wi.weight\n",
      "layers.10.mlp.Wo.weight\n",
      "layers.11.attn_norm.weight\n",
      "layers.11.attn.Wqkv.weight\n",
      "layers.11.attn.Wo.weight\n",
      "layers.11.mlp_norm.weight\n",
      "layers.11.mlp.Wi.weight\n",
      "layers.11.mlp.Wo.weight\n",
      "layers.12.attn_norm.weight\n",
      "layers.12.attn.Wqkv.weight\n",
      "layers.12.attn.Wo.weight\n",
      "layers.12.mlp_norm.weight\n",
      "layers.12.mlp.Wi.weight\n",
      "layers.12.mlp.Wo.weight\n",
      "layers.13.attn_norm.weight\n",
      "layers.13.attn.Wqkv.weight\n",
      "layers.13.attn.Wo.weight\n",
      "layers.13.mlp_norm.weight\n",
      "layers.13.mlp.Wi.weight\n",
      "layers.13.mlp.Wo.weight\n",
      "layers.14.attn_norm.weight\n",
      "layers.14.attn.Wqkv.weight\n",
      "layers.14.attn.Wo.weight\n",
      "layers.14.mlp_norm.weight\n",
      "layers.14.mlp.Wi.weight\n",
      "layers.14.mlp.Wo.weight\n",
      "layers.15.attn_norm.weight\n",
      "layers.15.attn.Wqkv.weight\n",
      "layers.15.attn.Wo.weight\n",
      "layers.15.mlp_norm.weight\n",
      "layers.15.mlp.Wi.weight\n",
      "layers.15.mlp.Wo.weight\n",
      "layers.16.attn_norm.weight\n",
      "layers.16.attn.Wqkv.weight\n",
      "layers.16.attn.Wo.weight\n",
      "layers.16.mlp_norm.weight\n",
      "layers.16.mlp.Wi.weight\n",
      "layers.16.mlp.Wo.weight\n",
      "layers.17.attn_norm.weight\n",
      "layers.17.attn.Wqkv.weight\n",
      "layers.17.attn.Wo.weight\n",
      "layers.17.mlp_norm.weight\n",
      "layers.17.mlp.Wi.weight\n",
      "layers.17.mlp.Wo.weight\n",
      "layers.18.attn_norm.weight\n",
      "layers.18.attn.Wqkv.weight\n",
      "layers.18.attn.Wo.weight\n",
      "layers.18.mlp_norm.weight\n",
      "layers.18.mlp.Wi.weight\n",
      "layers.18.mlp.Wo.weight\n",
      "layers.19.attn_norm.weight\n",
      "layers.19.attn.Wqkv.weight\n",
      "layers.19.attn.Wo.weight\n",
      "layers.19.mlp_norm.weight\n",
      "layers.19.mlp.Wi.weight\n",
      "layers.19.mlp.Wo.weight\n",
      "layers.20.attn_norm.weight\n",
      "layers.20.attn.Wqkv.weight\n",
      "layers.20.attn.Wo.weight\n",
      "layers.20.mlp_norm.weight\n",
      "layers.20.mlp.Wi.weight\n",
      "layers.20.mlp.Wo.weight\n",
      "layers.21.attn_norm.weight\n",
      "layers.21.attn.Wqkv.weight\n",
      "layers.21.attn.Wo.weight\n",
      "layers.21.mlp_norm.weight\n",
      "layers.21.mlp.Wi.weight\n",
      "layers.21.mlp.Wo.weight\n",
      "final_norm.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in mbert_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI01Bt1qWP8n"
   },
   "source": [
    "What differences do you see when comparing the layers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4UR8YpwTip1"
   },
   "source": [
    "# Model outputs\n",
    "\n",
    "We have seen the differences in inputs and model architecture, let's now take a look at model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fETmJPVWmWlI"
   },
   "source": [
    "Remember, our text is too long for BERT, so we must truncate it before we can feed the tokenized inputs to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3RG1Lqj8mQxo",
    "outputId": "a88d54ef-9959-4639-f22f-2648b4e50d25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.5935,  0.0940, -0.1687,  ..., -0.3806,  0.0434,  0.3832],\n",
       "         [ 0.4253, -0.4766,  0.7440,  ..., -0.3942,  0.3230,  0.2140],\n",
       "         [ 0.4182, -0.5374,  0.5692,  ..., -0.2323, -0.0629,  1.2690],\n",
       "         ...,\n",
       "         [ 0.5098, -0.5685,  0.2344,  ..., -0.5356,  0.0076,  0.3847],\n",
       "         [ 0.4078, -0.4184,  0.0023,  ..., -0.1491, -0.1271,  0.2958],\n",
       "         [ 1.3882,  0.1162,  1.2960,  ...,  0.3271,  0.2593, -0.2280]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-5.8410e-01,  3.0264e-01,  9.9930e-01, -9.6620e-01,  8.9615e-01,\n",
       "          9.3145e-01,  7.0277e-01, -9.9296e-01, -8.9288e-01,  6.4332e-02,\n",
       "          9.1099e-01,  9.9563e-01, -9.9678e-01, -9.9903e-01,  7.9328e-01,\n",
       "         -8.7767e-01,  9.3608e-01, -5.5321e-01, -9.9983e-01, -4.3656e-01,\n",
       "         -5.8475e-01, -9.9944e-01,  1.0964e-01,  9.4960e-01,  7.4010e-01,\n",
       "          9.5588e-02,  9.5480e-01,  9.9985e-01,  3.6781e-01, -7.1418e-02,\n",
       "          1.5343e-01, -9.6342e-01,  8.0601e-01, -9.9523e-01,  2.3721e-01,\n",
       "          5.7079e-01,  6.7148e-01, -1.1572e-01,  3.4119e-02, -9.6574e-01,\n",
       "          9.3264e-02, -8.1855e-02,  5.0594e-01, -3.3878e-01,  9.0800e-01,\n",
       "         -8.7438e-02,  1.0873e-01, -7.0242e-02, -2.1631e-01,  9.9912e-01,\n",
       "         -6.7767e-01,  8.5516e-01, -9.9331e-01,  8.1020e-01,  9.7947e-01,\n",
       "          9.2903e-02,  9.8613e-01,  6.2485e-02, -9.9823e-01, -2.5996e-01,\n",
       "          9.3471e-01,  2.4518e-01,  7.3368e-01,  4.3069e-01,  7.7860e-02,\n",
       "         -2.7087e-01, -6.1349e-01,  1.9892e-01, -1.9870e-01, -4.1420e-02,\n",
       "         -2.2974e-01,  1.5551e-01,  8.8369e-01, -7.2188e-01, -1.3030e-01,\n",
       "         -7.2228e-01,  8.6983e-02, -9.9951e-01,  7.8917e-01,  9.9984e-01,\n",
       "          7.9675e-01, -9.9898e-01,  9.5664e-01, -3.5850e-01, -7.5224e-01,\n",
       "          6.3721e-01, -9.9897e-01, -9.9861e-01, -4.0646e-02,  2.7731e-01,\n",
       "          7.8841e-01, -9.5581e-01,  4.9356e-01, -8.1524e-01,  9.9988e-01,\n",
       "         -9.2398e-01, -2.0302e-01,  2.3728e-01,  9.0461e-01, -8.6807e-01,\n",
       "         -6.8295e-02,  6.8471e-01,  9.9809e-01, -9.9607e-01,  9.9796e-01,\n",
       "          3.6687e-01, -7.6389e-01, -8.2652e-01,  5.6332e-01,  1.6990e-01,\n",
       "          8.9584e-01, -9.1908e-01, -7.0620e-01,  9.2570e-02,  9.5404e-01,\n",
       "         -8.4261e-01,  9.4571e-01,  6.9897e-01, -1.3166e-01,  9.9991e-01,\n",
       "          8.6573e-04,  9.6499e-01,  9.9181e-01,  7.3345e-01, -7.3252e-01,\n",
       "         -1.6990e-01, -4.5202e-01,  8.8734e-01, -3.9905e-01,  2.1717e-01,\n",
       "          7.1758e-01, -9.5522e-01, -9.9799e-01,  9.9785e-01, -2.3564e-01,\n",
       "          9.9983e-01, -9.9740e-01,  9.8551e-01, -9.9964e-01, -8.8597e-01,\n",
       "         -6.3065e-01, -6.1944e-02, -9.8140e-01, -3.8314e-01,  9.6079e-01,\n",
       "         -3.5562e-02, -8.9131e-01, -9.4146e-01,  6.0549e-01, -7.9863e-01,\n",
       "          3.7850e-01,  2.2629e-01, -6.2396e-01,  6.8147e-01,  9.9628e-01,\n",
       "          9.4864e-01,  9.6761e-01,  2.6589e-01, -9.2230e-01,  7.9300e-01,\n",
       "          9.5846e-01, -9.9813e-01,  8.9732e-01, -9.9189e-01,  9.9563e-01,\n",
       "          8.4069e-01,  7.1321e-01, -9.9685e-01,  9.9961e-01, -5.5877e-01,\n",
       "          1.1755e-01,  6.7096e-01, -1.5607e-02, -9.9877e-01,  3.9619e-01,\n",
       "          4.5943e-01, -1.5558e-01,  9.9727e-01, -9.5714e-01,  9.9774e-01,\n",
       "          1.3252e-02,  4.9255e-01,  3.5772e-01,  9.9831e-01, -9.9158e-01,\n",
       "         -8.0291e-01, -9.4892e-01,  1.9027e-01,  6.8125e-01,  6.1750e-01,\n",
       "          6.7569e-01,  7.8732e-01,  9.9828e-01, -2.9757e-01, -9.9001e-01,\n",
       "         -4.1553e-01,  9.5287e-01, -2.0856e-01,  9.9985e-01, -2.6419e-01,\n",
       "         -9.9937e-01, -8.2160e-01,  8.9943e-01,  8.1914e-01, -1.6182e-01,\n",
       "          8.1462e-01, -5.2396e-01, -2.8664e-01,  9.6293e-01, -6.9406e-01,\n",
       "          9.9832e-01, -3.6147e-03,  6.1132e-01,  3.8594e-01,  8.8349e-01,\n",
       "         -5.3314e-01, -1.6408e-01,  1.2694e-01, -7.5518e-01,  9.9957e-01,\n",
       "         -9.9856e-01, -1.6731e-01, -3.7634e-02, -9.8519e-01, -9.9560e-01,\n",
       "          6.2436e-01,  8.0067e-02, -6.4282e-01, -1.3642e-01,  6.1741e-01,\n",
       "          3.6457e-02,  7.8893e-01,  9.6491e-01, -3.9164e-01, -6.0784e-01,\n",
       "         -9.9932e-01, -9.9750e-01,  2.3789e-02, -9.1867e-01, -4.1951e-02,\n",
       "          6.5721e-01, -1.3801e-01, -7.4794e-01, -9.9856e-01,  7.6599e-01,\n",
       "          7.6082e-01, -9.3422e-01,  2.2875e-02, -7.3960e-01, -9.9835e-01,\n",
       "          2.4609e-01, -6.4626e-01, -9.9438e-01,  9.9815e-01, -6.6258e-01,\n",
       "          9.9429e-01,  7.7789e-01, -9.7316e-01,  7.5115e-01, -9.9838e-01,\n",
       "         -2.1761e-01, -7.2140e-01,  2.4359e-01,  6.5461e-02, -6.3551e-01,\n",
       "          1.5406e-01,  9.7360e-01, -7.6235e-01, -7.6524e-01,  7.9072e-01,\n",
       "         -9.9919e-01,  7.1930e-01, -3.0343e-02,  9.9774e-01,  7.3305e-01,\n",
       "         -1.8897e-01,  8.4528e-01,  8.9185e-01, -9.8000e-01, -9.9931e-01,\n",
       "          8.7562e-01,  4.7079e-01, -9.7402e-01, -1.9808e-01,  9.9971e-01,\n",
       "         -9.9770e-01,  1.2563e-01, -8.2274e-01, -9.3619e-01, -9.9881e-01,\n",
       "          3.3897e-01, -6.8748e-01,  1.8618e-01,  8.5998e-01,  1.4464e-01,\n",
       "          1.5673e-01,  9.7794e-01,  7.4372e-01,  2.8083e-01, -1.1855e-02,\n",
       "          1.1291e-01, -9.1355e-01, -4.6355e-01,  3.6866e-02,  3.8867e-02,\n",
       "         -9.9978e-01,  9.9928e-01, -9.8119e-01,  7.3831e-01,  9.6325e-01,\n",
       "         -9.9413e-01,  5.8742e-01,  2.1178e-01, -9.3854e-01, -1.8941e-02,\n",
       "          9.9969e-01,  8.0575e-01, -1.6765e-02,  1.8186e-01,  9.5256e-01,\n",
       "         -9.4176e-02,  6.5096e-01, -8.6763e-01, -4.4254e-01,  1.5096e-01,\n",
       "         -8.8588e-01,  9.8995e-01,  8.2660e-01, -9.5530e-01,  9.9444e-01,\n",
       "          1.0205e-01, -2.3893e-01, -6.3873e-01,  4.5051e-01,  9.4024e-01,\n",
       "          9.7326e-03, -3.8613e-01, -4.2612e-01, -3.7294e-03, -9.8418e-01,\n",
       "         -1.5258e-01, -9.9191e-01, -1.4834e-01,  8.6001e-01,  8.4497e-01,\n",
       "         -9.3132e-01,  7.7044e-01, -6.7252e-02,  8.3708e-01, -9.9735e-01,\n",
       "          9.9994e-01, -8.7027e-01,  1.6736e-01,  7.1710e-01, -8.7973e-01,\n",
       "         -2.0531e-01,  9.0445e-01,  9.8367e-01,  9.4210e-01, -8.9663e-01,\n",
       "         -8.0824e-01,  4.4318e-01,  8.4767e-01, -9.7307e-01, -9.7092e-02,\n",
       "         -9.9936e-01, -8.5246e-01,  9.6997e-01,  9.9689e-01,  2.8174e-02,\n",
       "         -4.9814e-01, -9.9739e-01,  8.5144e-01, -7.8339e-01, -5.7165e-01,\n",
       "         -1.4819e-01, -7.6239e-01,  5.0902e-01,  9.9785e-01, -5.2632e-01,\n",
       "          7.2047e-01,  3.5955e-02, -8.3544e-01,  7.3413e-01,  7.5778e-01,\n",
       "          9.9952e-01, -9.3652e-01,  5.3679e-01,  9.3030e-01, -5.1552e-02,\n",
       "         -7.6571e-01,  6.3139e-02,  9.9882e-01, -9.2061e-01, -1.1983e-01,\n",
       "         -9.9832e-01, -2.1231e-01, -1.1371e-01, -5.7941e-01, -5.9803e-01,\n",
       "          5.2511e-03, -8.6755e-01,  9.2965e-01,  6.1991e-01,  4.0540e-01,\n",
       "         -4.0696e-01,  6.5047e-01, -4.3759e-01, -5.9640e-02, -3.5947e-01,\n",
       "         -1.4891e-01,  4.2398e-01,  1.2092e-01,  8.4941e-01, -6.1199e-01,\n",
       "          9.9888e-01,  2.0604e-02, -9.9983e-01, -9.9804e-01, -7.5979e-01,\n",
       "         -9.9754e-01, -3.6530e-01, -8.7542e-01,  9.4455e-01,  9.4437e-01,\n",
       "         -9.9871e-01, -9.9864e-01, -8.3780e-01, -8.0400e-01,  7.7003e-01,\n",
       "          4.4651e-02,  8.7671e-02,  5.1311e-01, -1.4248e-02,  9.7379e-02,\n",
       "         -2.5116e-01, -7.2727e-02, -6.0417e-01, -3.3742e-01, -9.9720e-01,\n",
       "          8.6307e-01, -9.9981e-01, -8.8602e-01,  9.9712e-01, -9.9632e-01,\n",
       "         -9.1453e-01, -7.6265e-01, -8.7942e-01, -5.2777e-01,  1.0187e-01,\n",
       "          8.6628e-01, -5.5810e-01, -7.6796e-01, -9.9749e-01,  9.5135e-01,\n",
       "         -7.8158e-01,  1.9349e-01, -8.7656e-01, -8.6784e-01,  9.9857e-01,\n",
       "          9.0132e-01,  1.0297e-01, -1.7389e-01, -9.9703e-01,  9.8191e-01,\n",
       "         -8.2070e-01, -8.9399e-01, -8.3707e-01, -2.2065e-02, -3.9223e-01,\n",
       "         -9.9883e-01,  3.1523e-02,  9.9693e-01,  8.1203e-01,  9.5197e-01,\n",
       "          1.0209e-01, -2.2325e-01, -8.3589e-01,  1.8971e-01, -9.9980e-01,\n",
       "          7.7250e-01,  8.7684e-01, -8.6329e-01, -6.2393e-01,  9.8046e-01,\n",
       "          8.0163e-01, -9.1323e-01, -9.8622e-01,  9.4499e-01,  3.9214e-01,\n",
       "          7.1335e-01, -6.0534e-01, -4.1703e-01,  3.6311e-01, -3.6054e-02,\n",
       "         -9.6733e-01, -6.5509e-01,  9.8941e-01, -9.9883e-01,  9.1965e-01,\n",
       "          9.9470e-01,  9.9835e-01, -2.7217e-01,  3.5714e-01, -9.8183e-01,\n",
       "         -7.3069e-01, -2.3686e-01,  2.8877e-01, -9.9973e-01,  9.9936e-01,\n",
       "         -9.9980e-01,  7.0083e-01, -6.8593e-01,  6.9888e-01,  9.8337e-01,\n",
       "         -3.0083e-01, -9.9965e-01, -9.9940e-01,  7.3178e-01,  7.0974e-02,\n",
       "          9.5615e-01,  2.6224e-01,  6.8535e-02, -7.8356e-01, -2.9336e-01,\n",
       "          9.8399e-01, -7.5783e-01, -4.2944e-01, -9.9851e-01,  9.9885e-01,\n",
       "          4.6250e-01, -9.9067e-01,  9.9266e-01, -9.9760e-01,  8.9149e-01,\n",
       "          8.5303e-01,  6.3637e-01,  6.9369e-01, -9.9889e-01,  9.9985e-01,\n",
       "         -9.9940e-01,  8.2305e-01, -9.9988e-01, -9.9903e-01,  9.9931e-01,\n",
       "         -9.5967e-01, -7.4654e-01, -9.9903e-01, -9.9705e-01,  8.0581e-01,\n",
       "          1.3387e-01, -4.0749e-01,  9.5709e-01, -9.9913e-01, -9.9348e-01,\n",
       "         -4.8434e-01, -8.8434e-01, -7.1351e-01,  9.9202e-01, -4.2931e-01,\n",
       "          9.0131e-01, -2.5380e-01,  6.7407e-01,  1.8382e-01,  9.9532e-01,\n",
       "          7.8862e-01, -6.1045e-01, -8.8052e-01, -9.8013e-01,  9.3874e-01,\n",
       "         -6.9823e-01,  1.5810e-01,  7.2539e-01, -8.0744e-02, -7.3970e-01,\n",
       "          2.1285e-01, -9.9294e-01,  6.0450e-01,  1.9837e-01,  9.5435e-01,\n",
       "          9.2623e-01,  7.3446e-01, -1.5829e-01, -3.9911e-01, -1.6699e-01,\n",
       "         -9.4514e-01,  5.3317e-01, -9.9807e-01,  7.9529e-01, -8.1782e-01,\n",
       "         -2.2800e-02, -4.3114e-01, -2.8864e-01, -6.7582e-01,  9.9818e-01,\n",
       "          9.9429e-01, -8.3768e-01, -3.6043e-02,  9.4775e-01, -7.0868e-01,\n",
       "          7.8949e-01, -9.7776e-01, -2.4616e-02,  9.1519e-01, -2.4871e-01,\n",
       "          9.1743e-01, -1.0177e-01, -1.7112e-02,  8.8072e-01, -9.8735e-01,\n",
       "         -7.7672e-01, -5.5635e-01,  2.1299e-01, -5.1817e-01, -9.1619e-01,\n",
       "          2.6404e-02,  9.8789e-01, -5.0765e-01, -9.9927e-01,  9.5793e-01,\n",
       "         -9.9766e-01, -1.6029e-01,  9.4239e-01, -5.8731e-01,  9.9958e-01,\n",
       "         -8.7988e-01,  5.8187e-02, -2.5853e-02, -9.9920e-01, -9.9926e-01,\n",
       "          6.0247e-02, -7.4651e-02, -9.7408e-01,  9.9883e-01, -5.8897e-02,\n",
       "          8.7604e-01, -9.9933e-01,  2.1828e-01,  9.9540e-01,  1.9401e-01,\n",
       "          4.9669e-01, -8.6318e-01, -6.7310e-01, -9.4338e-01, -3.5492e-01,\n",
       "          1.2480e-01,  7.4864e-01, -8.9541e-01, -8.9164e-01, -7.9287e-01,\n",
       "          9.9974e-01, -9.9302e-01, -5.8616e-01, -8.1951e-01,  7.5901e-01,\n",
       "          9.1131e-01,  3.4101e-01,  1.5840e-01, -7.3799e-01,  9.3804e-01,\n",
       "         -8.8358e-01,  9.9191e-01, -9.8413e-01, -9.6994e-01,  9.9934e-01,\n",
       "          2.8866e-01, -9.8669e-01, -2.3692e-01, -2.4899e-01,  4.3608e-01,\n",
       "          1.4545e-01,  6.0847e-01, -1.2719e-01, -1.2161e-01, -8.4559e-01,\n",
       "          7.7135e-01, -6.7050e-01, -9.6252e-01,  2.8750e-01, -1.5456e-01,\n",
       "         -6.3553e-01,  9.8236e-01,  8.9337e-01,  9.9976e-01, -9.9917e-01,\n",
       "          7.7679e-02,  7.7487e-02,  9.9756e-01,  4.1714e-02, -5.8849e-01,\n",
       "          7.7294e-01,  9.9846e-01, -8.0432e-01,  5.8799e-01, -7.9005e-02,\n",
       "         -1.2900e-03,  2.6761e-01, -3.0022e-01,  9.9773e-01, -9.1677e-01,\n",
       "         -4.1637e-02, -9.1545e-01, -9.9959e-01,  9.9956e-01, -1.2335e-01,\n",
       "          9.6817e-01,  3.1384e-01,  7.8995e-01,  6.9748e-02,  9.6560e-01,\n",
       "         -9.6982e-01, -7.0725e-01, -9.9980e-01,  6.7028e-02, -7.6820e-01,\n",
       "         -9.5286e-01,  7.5772e-02,  8.9075e-01, -9.9857e-01, -9.3012e-01,\n",
       "         -4.0138e-01, -9.9992e-01,  7.9088e-01, -9.8835e-01, -8.6094e-01,\n",
       "         -8.5343e-01,  9.9840e-01,  1.0849e-01, -7.1413e-01,  8.0914e-01,\n",
       "         -6.9948e-01,  8.9800e-01,  8.9527e-01, -4.3697e-01,  4.1742e-01,\n",
       "         -1.1605e-01, -6.9357e-01, -9.9015e-01, -9.3240e-01, -7.9463e-01,\n",
       "          8.0282e-01, -9.4678e-01,  1.1889e-01,  9.8838e-01,  9.2091e-01,\n",
       "         -9.9615e-01, -9.8770e-01,  9.9339e-01,  6.0685e-02,  9.3697e-01,\n",
       "         -1.6057e-01, -9.9938e-01, -9.9962e-01,  1.6786e-01,  2.6170e-01,\n",
       "          9.7812e-01, -2.4267e-01,  6.6394e-01,  1.5277e-01,  7.6269e-02,\n",
       "          3.2090e-01, -6.7380e-01, -4.6392e-01, -4.6153e-01, -1.0765e-01,\n",
       "          9.9985e-01, -5.5829e-01,  9.5898e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "bert_outputs = bert_model(**bert_inputs)\n",
    "bert_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rAxaG9KmxRf"
   },
   "source": [
    "BERT has two outputs, do you remember what they are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vuPMy9UJXz8",
    "outputId": "4e227975-e541-46f5-eadf-84bb54aaf1c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first BERT output:  torch.Size([1, 512, 768])\n",
      "Shape of second BERT output:  torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print('Shape of first BERT output: ', bert_outputs[0].shape)\n",
    "print('Shape of second BERT output: ', bert_outputs[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QUsJwWZm7P4"
   },
   "source": [
    "With ModernBERT, thanks to the long context capabilities, we do not need to truncate our long text before feeding it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFIpBPQvnD_F",
    "outputId": "feab5281-4043-4a0c-ef1e-71a647e5d8a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput(last_hidden_state=tensor([[[ 0.1680, -0.3384, -0.7694,  ..., -0.4745, -0.1105, -0.7589],\n",
       "         [-0.8740, -0.3295,  0.6591,  ..., -1.9694, -2.3188,  0.4383],\n",
       "         [ 0.3498, -1.4112,  0.0969,  ...,  0.4195,  0.0887, -0.6138],\n",
       "         ...,\n",
       "         [ 1.8556, -0.1904, -1.1067,  ..., -1.8617,  0.1445, -0.0550],\n",
       "         [ 0.2851, -0.9601, -0.9530,  ..., -1.6096,  0.1400,  0.2290],\n",
       "         [ 0.1789, -0.0395,  0.0412,  ...,  0.0570,  0.1783,  0.1189]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell takes a minute to run because our text is so long\n",
    "mbert_inputs = mbert_tokenizer(text, return_tensors=\"pt\")\n",
    "mbert_outputs = mbert_model(**mbert_inputs)\n",
    "mbert_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lrrln-jsnMwL"
   },
   "source": [
    "Interesting! ModernBERT only has one output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLIupZHsnDsP",
    "outputId": "fcd1a259-5238-47f1-9bb6-5b5c4bc9afb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of ModernBERT output:  torch.Size([1, 2671, 768])\n"
     ]
    }
   ],
   "source": [
    "print('Shape of ModernBERT output: ', mbert_outputs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6MM_BQwnh7r"
   },
   "source": [
    "Compare the first output from BERT and the only output from ModernBERT, what is the difference? What does the 2nd dimension represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0KkmWeaKmG62",
    "outputId": "10f974bf-d648-477f-e78d-e6ca424aaafc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last hidden state shape\n",
      "torch.Size([1, 512, 768])\n",
      "Last hidden state\n",
      "tensor([[[ 0.5935,  0.0940, -0.1687,  ..., -0.3806,  0.0434,  0.3832],\n",
      "         [ 0.4253, -0.4766,  0.7440,  ..., -0.3942,  0.3230,  0.2140],\n",
      "         [ 0.4182, -0.5374,  0.5692,  ..., -0.2323, -0.0629,  1.2690],\n",
      "         ...,\n",
      "         [ 0.5098, -0.5685,  0.2344,  ..., -0.5356,  0.0076,  0.3847],\n",
      "         [ 0.4078, -0.4184,  0.0023,  ..., -0.1491, -0.1271,  0.2958],\n",
      "         [ 1.3882,  0.1162,  1.2960,  ...,  0.3271,  0.2593, -0.2280]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "bert_lhs_shape = bert_outputs.last_hidden_state.shape\n",
    "bert_lhs_data = bert_outputs.last_hidden_state\n",
    "\n",
    "print(\"Last hidden state shape\")\n",
    "print(bert_lhs_shape)\n",
    "\n",
    "print(\"Last hidden state\")\n",
    "print(bert_lhs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x_YUd8ApB0gL",
    "outputId": "4c35e904-c622-4a9d-e5a6-39f0caa72b3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last hidden state shape\n",
      "torch.Size([1, 2671, 768])\n",
      "Last hidden state\n",
      "tensor([[[ 0.1680, -0.3384, -0.7694,  ..., -0.4745, -0.1105, -0.7589],\n",
      "         [-0.8740, -0.3295,  0.6591,  ..., -1.9694, -2.3188,  0.4383],\n",
      "         [ 0.3498, -1.4112,  0.0969,  ...,  0.4195,  0.0887, -0.6138],\n",
      "         ...,\n",
      "         [ 1.8556, -0.1904, -1.1067,  ..., -1.8617,  0.1445, -0.0550],\n",
      "         [ 0.2851, -0.9601, -0.9530,  ..., -1.6096,  0.1400,  0.2290],\n",
      "         [ 0.1789, -0.0395,  0.0412,  ...,  0.0570,  0.1783,  0.1189]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mbert_lhs_shape = mbert_outputs.last_hidden_state.shape\n",
    "mbert_lhs_data = mbert_outputs.last_hidden_state\n",
    "\n",
    "print(\"Last hidden state shape\")\n",
    "print(mbert_lhs_shape)\n",
    "\n",
    "print(\"Last hidden state\")\n",
    "print(mbert_lhs_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oUEihv3IJFw"
   },
   "source": [
    "As expected, the output tensors from the two models are different.\n",
    "\n",
    "What advantage does long context capability have over short context model? Why might one prefer one model over another for their task?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ljUlpnCqi3q"
   },
   "source": [
    "# Inference Speed\n",
    "\n",
    "To make an apple-to-apple comparison of inference speed between the two models, let's truncate the long text to 512 tokens for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52I15Kx2rZ79",
    "outputId": "ed9093e5-9ffb-4f3b-9651-f5d894120b2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = dataset[\"positive\"]\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "JokDBnYcr6YB"
   },
   "outputs": [],
   "source": [
    "bert_inputs = bert_tokenizer(texts,\n",
    "                             max_length=512,\n",
    "                             padding=True,\n",
    "                             truncation=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "mbert_inputs = mbert_tokenizer(texts,\n",
    "                               max_length=512,\n",
    "                               padding=True,\n",
    "                               truncation=True,\n",
    "                               return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nQ3esYqMoHgS",
    "outputId": "79b76f02-b975-4e91-bf55-7b1daf3edfa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.21 s, sys: 413 ms, total: 8.62 s\n",
      "Wall time: 9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bert_outputs = bert_model(**bert_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wj3hhROgoH0j",
    "outputId": "794502c5-fa38-4150-cce0-74bc25dd43ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 2.54 s, total: 15 s\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mbert_outputs = mbert_model(**mbert_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68y_at_gtIAr"
   },
   "source": [
    "Even with the enhanced capabilities of ModernBERT, the inference time is comparable between the two models on CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epRzaNjMstgH"
   },
   "source": [
    "We can also run inference on a GPU and compare the speed on a GPU.\n",
    "\n",
    "Select a GPU runtime and run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wntSrbSKs57A"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets -q -U\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1uEXD_3UtCZo"
   },
   "outputs": [],
   "source": [
    "bert_checkpoint = \"bert-base-cased\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_checkpoint)\n",
    "bert_model = BertModel.from_pretrained(bert_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yQRNuWKutGaf"
   },
   "outputs": [],
   "source": [
    "mbert_checkpoint = \"answerdotai/ModernBERT-base\"\n",
    "mbert_tokenizer = AutoTokenizer.from_pretrained(mbert_checkpoint)\n",
    "mbert_model = AutoModel.from_pretrained(mbert_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HwauOPc7tUbQ"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sentence-transformers/mldr\", \"en-triplet\", split=\"train\").take(5)\n",
    "texts = dataset[\"positive\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPDnsVPZtw2i"
   },
   "source": [
    "Again, we truncate the long context texts to the same length to compare the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lHrcNv93taL9"
   },
   "outputs": [],
   "source": [
    "bert_inputs = bert_tokenizer(texts,\n",
    "                             max_length=512,\n",
    "                             padding=True,\n",
    "                             truncation=True,\n",
    "                             return_tensors='pt')\n",
    "\n",
    "mbert_inputs = mbert_tokenizer(texts,\n",
    "                               max_length=512,\n",
    "                               padding=True,\n",
    "                               truncation=True,\n",
    "                               return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pkD5ELGJtaWg",
    "outputId": "c004eef9-8e4a-4749-e11a-dc246cad80f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 s, sys: 5.32 s, total: 16.7 s\n",
      "Wall time: 29.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "bert_outputs = bert_model(**bert_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCkiUcP5tdmw",
    "outputId": "b4eb0527-76c9-4b05-fac7-6521d7b5d62c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 7.88 s, total: 21.3 s\n",
      "Wall time: 30 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mbert_outputs = mbert_model(**mbert_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3AcR7TFuBOM"
   },
   "source": [
    "GPU certainly makes things much faster and the inference speed between the two models are comparable to each other!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvkhSOd6uQwq"
   },
   "source": [
    "Next, take a look at the [ModernBERT documentation](https://huggingface.co/docs/transformers/main/en/model_doc/modernbert) to see if you can finetune a ModernBERT model for a downstream task!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
