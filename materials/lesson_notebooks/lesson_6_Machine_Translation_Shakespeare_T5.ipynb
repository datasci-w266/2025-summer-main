{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wy37jVQVufTY"
   },
   "source": [
    "## Lesson Notebook 6: Machine Translation With Shakespeare and T5\n",
    "\n",
    "In this notebook we will look at one example related to machine translation:\n",
    "\n",
    "   * Train a transformer from scratch on translation of Shakespeare to Modern English\n",
    "\n",
    "\n",
    "\n",
    "<a id = 'returnToTop'></a>\n",
    "\n",
    "## Notebook Contents\n",
    "  * 1. [Setup](#setup)\n",
    "  * 2. [Shakespeare-to-Modern English translation with a from-scratch transformer](#shakespeare)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-summer-main/blob/master/materials/lesson_notebooks/lesson_6_Machine_Translation_Shakespeare_T5.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkM-Z34gIvyo"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'setup'></a>\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "\n",
    "We'll start with the usual setup. We need to begin with the sentencepiece code in order to tokenize the text for some of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bZSrl0npwP2X"
   },
   "outputs": [],
   "source": [
    "!pip install -q sentencepiece\n",
    "!pip install -q transformers\n",
    "!pip install -q tokenizers\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_eb6kzr0fShh"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import T5TokenizerFast, T5Config, T5ForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WJuh_1Kpa0P"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'shakespeare'></a>\n",
    "\n",
    "## 2. Shakespeare-to-Modern English Translation with a Seq2Seq Transformer\n",
    "\n",
    "What if we want to create a translation model from scratch? These days, you'll usually be working with pre-trained transformers. But there are some circumstances in which it makes sense to build a new model, especially if you're working with a very rare language.\n",
    "\n",
    "To explore how we might do so, just as a learning exercise, we'll train a brand-new sequence-to-sequence transformer model for the task of translating text from Shakespearean English to Modern English.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFFVderUrOWX"
   },
   "source": [
    "### 2.1 Downloading the data\n",
    "\n",
    "The data includes aligned sentences from a number of plays by William Shakespeare.  The data was copied from this repo --[https://github.com/cocoxu/Shakespeare](https://github.com/cocoxu/Shakespeare) -- and consolidated into one file for easier handling.\n",
    "\n",
    "You will to grab a copy from our git repo and import it to your Google drive.  From there you'll be able to easily load it in to a Colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t7PzFLfmrOeI",
    "outputId": "f586e3bd-32d0-4a7b-84d2-a58de9fd458d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#This cell will authenticate you and mount your Drive in the Colab.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jmBjDeKfrcLB"
   },
   "outputs": [],
   "source": [
    "#Modify this path to the appropriate location in your Drive\n",
    "text_file = 'drive/MyDrive/ISchool/MIDS/266/data/train_plays-org-mod.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0RYSPnYrfpb"
   },
   "source": [
    "### 2.2 Parsing the data\n",
    "\n",
    "Each line contains a Shakespearean sentence and its corresponding modern English translation.\n",
    "The Shakesperean sentence is the *source sequence* and modern English one is the *target sequence*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ihgSri99rfwK"
   },
   "outputs": [],
   "source": [
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    old, mod = line.split(\"\\t\")\n",
    "    old = old.lower()\n",
    "    mod = mod.lower()\n",
    "    text_pairs.append((old, mod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uS1iLd31rtNG",
    "outputId": "600960f1-7493-468e-d60e-5b7c28d56ba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('where be your gibes now?', 'where are your jokes now?')\n",
      "(\"think yourself a baby, that you have ta'en these tenders for true pay, which are not sterling.\", 'think that you are a baby, that you have taken these offers for true love, which are not true offers.')\n",
      "('well, i am glad that all things sort so well.', 'well, iâ€™m glad that everything has been sorted out.')\n",
      "('let him greet england with our sharp defiance.', 'tell him to greet the king of england with our sharp defiance.')\n",
      "(\"here comes the fool, i' faith.\", 'look, here comes the fool.')\n"
     ]
    }
   ],
   "source": [
    "#look at some examples\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19hGQ6p5ruLV",
    "outputId": "c3ddbd7d-bf88-4bbd-9e85-059e5ecaffca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19088 total pairs\n",
      "16798 training pairs\n",
      "1145 validation pairs\n",
      "1145 test pairs\n"
     ]
    }
   ],
   "source": [
    "#Let's create some splits\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.06 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fgxp5i3rf1W"
   },
   "source": [
    "Note that if we only use 6% for validation and test (about 1k each), we have roughly almost 17,000 sentence pairs for training from scratch. This is still a small amount of data to train a language model from scratch, so we shouldn't have very high expectations, but it will be interesting to try.\n",
    "\n",
    "### 2.3 Define vocabulary and tokenizer\n",
    "\n",
    "We'll want a new tokenizer for our data, so that we can use a smaller vocabulary than typical pre-trained models. We're going to use a T5 model as the basis of our model architecture, but we'll make a much smaller version of it to train from scratch only on our dataset. We'll need to use a similar tokenizer to go with it.\n",
    "\n",
    "T5 models use one vocabulary for both encoder and decoder, so we'll combine the text from our Shakespearean and Modern English (they do share a lot of words in common). The easiest thing to do is to load an existing T5 tokenizer, then retrain a new version of it with our own text and chosen vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ByoyLdKmrf6R"
   },
   "outputs": [],
   "source": [
    "# The size of our vocabulary covers both languages\n",
    "VOCAB_SIZE = 15000\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "\n",
    "\n",
    "def get_word_piece_tokenizer(text_samples, vocab_size):\n",
    "\n",
    "    base_tokenizer = T5TokenizerFast.from_pretrained('t5-base')\n",
    "    new_tokenizer = base_tokenizer.train_new_from_iterator(\n",
    "        text_samples,\n",
    "        vocab_size=VOCAB_SIZE\n",
    "    )\n",
    "\n",
    "    return new_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "2387c01668b1404cbd3a33071262151a",
      "99f24b60ddf1498593ed368d6ac6dfe6",
      "1e15b9d40dae4a0ea620202a86a976b7",
      "afc402868be141869e953fca3fc631c1",
      "181da8235b784b239a8b0b96aef83b49",
      "41e03a7c391e493aa588308c6584112e",
      "85db9621e94543d78e9358e5160ceb0a",
      "7996f8d9865c44e1a8daae4568551902",
      "3f18701ad97041e5b5bfdafa11849a2a",
      "c0605a70db8f41c2bfb1f755bdee7dca",
      "574e446b3b364c26af6826abc32932cb",
      "9b3111ca1e384ad5a2c60661cd81a107",
      "5c5aef26d168477c855f3be224d96d57",
      "73c532b2986f47bda854f8665bd0a74a",
      "36a7cfb02a0c4d98ba366d155fb68f4f",
      "be1c5649dd3e43e090320058efb95fe4",
      "97d0aa9f428844699360b7408c577e2f",
      "4619a27ecc9845faae37d75e6edc93da",
      "df4c524e89764773a0e351bb46251fa6",
      "0769ed1e97fd4fb0afdcb8d9fc17c012",
      "a00af8460361490f98d6fa2d813046da",
      "9b469348f82841a2b81ea10c364a354d",
      "0644137750954bd0b68ef0c70af12e0b",
      "3ea74243319a4b7bb49a05eeaf761a00",
      "f2b58f9879d040a0953f00e23955c8a1",
      "ac48216f54d54e14bac84dcae9833aa8",
      "2df4164516cc469081b4d1d8c0d7cafd",
      "c662f850c142486395fc07dec4c304c5",
      "b5130390c2f245219c29e23fb7419e59",
      "514fc843780042289791ae48f638b3b2",
      "93fc18755c1941c0884790d3d68c066f",
      "7ebe5bce7ea1473a9f99060ce918f135",
      "9896b4cf5a034f6f817ab2cbaa07a38c"
     ]
    },
    "id": "ioWfgUnrrgFw",
    "outputId": "e3972a9a-6b91-4117-ffbc-c425ba443e1e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2387c01668b1404cbd3a33071262151a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3111ca1e384ad5a2c60661cd81a107",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0644137750954bd0b68ef0c70af12e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shakespeare_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "modern_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "\n",
    "t5_tokenizer = get_word_piece_tokenizer(shakespeare_samples + modern_samples, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrDM3UwxsK5o",
    "outputId": "2681be23-90d6-4570-854e-74cc0f125bb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Tokens:  i of my?i that a ins is your! for be not with have he this it\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocab Tokens: \", t5_tokenizer.decode(range(110, 130)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-OzTEEPYsM7Q",
    "outputId": "1a652ed3-355b-45bb-8d86-a3ec583bfdda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakesperean English sentence:  ['i say again, give out that anne my queen is sick and like to die.']\n",
      "Tokens:  {'input_ids': [[110, 187, 351, 103, 209, 192, 115, 4546, 112, 586, 119, 655, 109, 169, 107, 329, 105, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "Recovered text after detokenizing:  ['i say again, give out that anne my queen is sick and like to die.</s>']\n",
      "\n",
      "Modern English sentence:  ['i repeat, spread the rumor that anne, my wife, is sick and likely to die.']\n",
      "Tokens:  {'input_ids': [[110, 4301, 103, 3106, 106, 2832, 115, 4546, 103, 112, 396, 103, 119, 655, 109, 1951, 107, 329, 105, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "Recovered text after detokenizing:  ['i repeat, spread the rumor that anne, my wife, is sick and likely to die.</s>']\n"
     ]
    }
   ],
   "source": [
    "old_input_ex = [text_pairs[1][0]]\n",
    "old_tokens_ex = t5_tokenizer.batch_encode_plus(old_input_ex)\n",
    "print(\"Shakesperean English sentence: \", old_input_ex)\n",
    "print(\"Tokens: \", old_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    t5_tokenizer.batch_decode(old_tokens_ex['input_ids']),\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "mod_input_ex = [text_pairs[1][1]]\n",
    "mod_tokens_ex = t5_tokenizer.batch_encode_plus(mod_input_ex)\n",
    "print(\"Modern English sentence: \", mod_input_ex)\n",
    "print(\"Tokens: \", mod_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    t5_tokenizer.batch_decode(mod_tokens_ex['input_ids']),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzdXVX16sRTQ"
   },
   "source": [
    "### 2.4 Format Datasets\n",
    "\n",
    "Let's turn our data into a Huggingface dataset, so that we can work with it similarly to earlier lesson notebooks. We'll need to write a preprocess function to convert the input and output texts into sequences of vocab IDs, using the tokenizers we just made. Then we'll map the preprocess function to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "omwrKUlfP0Ii"
   },
   "outputs": [],
   "source": [
    "def make_dataset(pairs):\n",
    "    org_texts, mod_texts = zip(*pairs)\n",
    "    org_texts = list(org_texts)\n",
    "    mod_texts = list(mod_texts)\n",
    "\n",
    "    dataset = Dataset.from_dict({\"shakespeare\": org_texts, \"modern\": mod_texts})\n",
    "    return dataset.shuffle()\n",
    "\n",
    "#make the training data\n",
    "train_ds = make_dataset(train_pairs)\n",
    "\n",
    "#make the validation data\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "vcVxffbIsRZN"
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(batch_text_pairs):\n",
    "    shakespeare_encoded = t5_tokenizer.batch_encode_plus(\n",
    "        batch_text_pairs[\"shakespeare\"],\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    modern_encoded = t5_tokenizer.batch_encode_plus(\n",
    "        batch_text_pairs[\"modern\"],\n",
    "        max_length=MAX_SEQUENCE_LENGTH,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    return {'input_ids': shakespeare_encoded['input_ids'],\n",
    "            'labels': modern_encoded['input_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "c50be28a4783473697383f22a3731b55",
      "0160a37477a44d78abd73c5f2c618c18",
      "1dc9717768a243bb93514b3b2981adc9",
      "2274e8fbb2424fad9020d17c2e17152b",
      "78dc639f3f964b639e1aa969f798713d",
      "a8b8af35c3c74217b795cba9ceb064f2",
      "bafa97ec3ff4472da29aed8e7779e65f",
      "14d558ac670246129210c8394feb5beb",
      "f24d89f726784871b37c3a928e58bcbe",
      "20b80047d8454dc797b65afe4c1857e1",
      "3a0154bd556647819e16792262bbdf36",
      "30b5506db0e14fb4a2a2ac3f40a8e6ce",
      "6830c388531e48b29c4c6080e3d0c34f",
      "61b981e4efa84136912c7af156c2f20d",
      "c31680d391f144e18c917ce69bce564b",
      "7af8e572772a418fb4fe381c2a17fd09",
      "b8d34ce313fa470b93b0ed905f5824ec",
      "eb23c6bc5b0a4ee8916087b60f67f7ff",
      "82168c6387484390a9bc2b8351d71cbf",
      "410008c266d8468699b6247ce36f855d",
      "c185640a4580440ea7e8d8db8916a51c",
      "701f9911aea04059993d9044e09bb79e"
     ]
    },
    "id": "3gs0zGj5sdCE",
    "outputId": "aff3c8a8-a751-40be-ada2-d9e83a3dca9e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50be28a4783473697383f22a3731b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b5506db0e14fb4a2a2ac3f40a8e6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = train_ds.map(preprocess_batch, batched=True)\n",
    "val_ds = val_ds.map(preprocess_batch, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6V6Rck8AsRes"
   },
   "source": [
    "### 2.5 Define the model\n",
    "\n",
    "Huggingface allows us to use an existing type of model architecture, but to load a randomly initialized version with no pretrained weights. To do so, we'll load a model from a config, instead of from a pretrained checkpoint.\n",
    "\n",
    "Since this will be a new untrained model, we can specify some of the dimensions that we want in the config, to fit our specific project. Let's make a very small version of a T5 model, with our small vocabulary, and half the default sized embedding and intermediate dimensions. We'll only use a single transformer layer in both the encoder and decoder.\n",
    "\n",
    "(In assignment 3, you will explore these parameters more, including adding more transformer layers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "8apZZSedrcQk"
   },
   "outputs": [],
   "source": [
    "# Define some hyperparameter values for our transformer model\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 1\n",
    "\n",
    "# Also define some training parameters we'll use next\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25  # Should be at least 25 to converge; takes 5-6 mins to train\n",
    "\n",
    "\n",
    "t5_config = T5Config(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=EMBED_DIM,\n",
    "    d_ff=INTERMEDIATE_DIM,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    decoder_start_token_id=t5_tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "a4Y9x3gprd_P"
   },
   "outputs": [],
   "source": [
    "t5_model = T5ForConditionalGeneration(config=t5_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cya3sb86sRp5"
   },
   "source": [
    "### 2.6 Train the model\n",
    "\n",
    "To train a Huggingface model, like we did with BERT, we'll create a Trainer class and associated TrainerArguments. We need to use the ones for Seq2Seq models this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "xWXXUNabsRvG"
   },
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    \"shakespeare_translation_model\",\n",
    "    eval_strategy='epoch',\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    report_to='none'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "1TC-qdfTs1hj"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    t5_model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqqH8OjZ7v4P"
   },
   "source": [
    "Training this small model on our dataset will take about 5 minutes on a Colab T4 GPU. Note that we're training for quite a few epochs to get the model to start to pick up the task, since the model has not been pre-trained in any way. We might stop early in the live session if we're running low on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 933
    },
    "id": "zKE-nKB-s7ZF",
    "outputId": "48fe9722-a617-4c00-df46-84c8cfea2df8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6575' max='6575' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6575/6575 05:42, Epoch 25/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.603108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.637300</td>\n",
       "      <td>2.449992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.637300</td>\n",
       "      <td>2.357891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.326000</td>\n",
       "      <td>2.288018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.326000</td>\n",
       "      <td>2.229813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.179700</td>\n",
       "      <td>2.180087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.179700</td>\n",
       "      <td>2.133529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.100500</td>\n",
       "      <td>2.094637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.100500</td>\n",
       "      <td>2.060402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.012900</td>\n",
       "      <td>2.031612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.012900</td>\n",
       "      <td>2.006961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.945700</td>\n",
       "      <td>1.985206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.945700</td>\n",
       "      <td>1.964316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.911600</td>\n",
       "      <td>1.949765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.911600</td>\n",
       "      <td>1.935331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.866300</td>\n",
       "      <td>1.921580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.866300</td>\n",
       "      <td>1.911036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.838300</td>\n",
       "      <td>1.901124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.838300</td>\n",
       "      <td>1.893057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.814200</td>\n",
       "      <td>1.886191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.800900</td>\n",
       "      <td>1.881523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.800900</td>\n",
       "      <td>1.876675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.782200</td>\n",
       "      <td>1.874096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.782200</td>\n",
       "      <td>1.872263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.782300</td>\n",
       "      <td>1.871981</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6575, training_loss=1.9973726291438927, metrics={'train_runtime': 343.7513, 'train_samples_per_second': 1221.668, 'train_steps_per_second': 19.127, 'total_flos': 370074184704000.0, 'train_loss': 1.9973726291438927, 'epoch': 25.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0CQjMvNtAhp"
   },
   "source": [
    "### 2.7 Generate and examine some test sentences\n",
    "\n",
    "Finally, let's write a function to generate some translations of new inputs. We'll use the model's .generate() method and the tokenizer's .batch_decode() method. Huggingface text generation models has multiple options built into their .generate() method, including beam search or top-k/p sampling, constraints on repeat ngrams, min and max length constraints for the output, etc. We'll start with simple defaults here, that keep the generation loop relatively fast. You'll explore these options more in Assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "HTm4SQSPtAm5"
   },
   "outputs": [],
   "source": [
    "def generate_output(input_sentences):\n",
    "    inputs_encoded = t5_tokenizer(input_sentences, return_tensors='pt')\n",
    "    output_ids = t5_model.generate(\n",
    "        inputs_encoded['input_ids'].cuda(),\n",
    "        num_beams=1,\n",
    "        no_repeat_ngram_size=4\n",
    "    )\n",
    "\n",
    "    generated_sentences = t5_tokenizer.batch_decode(output_ids,\n",
    "                                                    skip_special_tokens=True,\n",
    "                                                    clean_up_tokenization_spaces=False)\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N02j5d73tXRL",
    "outputId": "150ef613-531d-4ae9-99e3-6c7df43fbf7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Example 0 **\n",
      "your majesty shall mock at me.\n",
      "your your your your at at at at your your your.\n",
      "\n",
      "** Example 1 **\n",
      "she was the wife of caius marcellus.\n",
      "she was the was was was was of a good she was was was the of the of a of\n",
      "\n",
      "** Example 2 **\n",
      "the king doth wake tonight and takes his rouse, keeps wassail, and the swaggering upspring reels, and as he drains his draughts of rhenish down, the kettle-drum and trumpet thus bray out the triumph of his pledge.\n",
      "the king, and the king, he was his his his his and the king of his his his\n",
      "\n",
      "** Example 3 **\n",
      "the ladies, her attendants of her chamber saw her abed, and in the morning early they found the bed untreasured of their mistress.\n",
      "they have the king, and her her her her and they have her her her, they have the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_org_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(4):\n",
    "    input_sentence = random.choice(test_org_texts)\n",
    "    translated = generate_output([input_sentence])\n",
    "    translated = translated[0]\n",
    "    print(f\"** Example {i} **\")\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BXUv1WetHGf"
   },
   "source": [
    "This doesn't look very good yet, but that's to be expected. It's very difficult to get an NLP model to do well on a complex task without pre-training it on huge amounts of raw text. In Assignment 3, we'll try a few more options to see if we can get this model to do at least a little better on this task.\n",
    "\n",
    "Remember that training a model from scratch is not something you would normally do, but it can be a useful educational exercise to better understand the starting point of these models and what it takes for them to learn language processing tasks.\n",
    "\n",
    "\n",
    "**QUESTION 2**: What things could we do to improve the output?\n",
    "* add more sentence pairs\n",
    "* ensure a good distribution over all the sentence lengths\n",
    "* add another transformer layer to encoder and decoder\n",
    "* change the generation hyperparameters\n",
    "* ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS_O3bXhtQvf"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'answers'></a>\n",
    "\n",
    "## ANSWERS\n",
    "\n",
    "1.  The T5 model doesn't have the token type ids that BERT uses to identify different segments.\n",
    "\n",
    "2.  The first two suggestions -- more sentence pairs and better balance on length are a good start.  More and better data typically lead to improved performance.  We might also look into separtely \"pre-training\" our encoder and decoder with their own language models.  We could then use those as pre-trained models as a foundation on which we train our connected encoder and decoder.  We could also look in to using back translation to augment our existing sentence pairs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
