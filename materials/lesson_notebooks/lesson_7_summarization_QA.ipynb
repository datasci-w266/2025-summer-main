{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcH1I6BsI1CY"
   },
   "source": [
    "# Lesson notebook 7 - Summarization and Question Answering\n",
    "\n",
    "\n",
    "\n",
    "### Extractive summarization example\n",
    "\n",
    "One of the challenges faced by current neural systems is the size of the input they can manage.  As a result most  of these systems end up truncating the input in some fashion.  Can you get a good summary if you only read in the first 500 words of a document?  One solution to this is to use an older approach called extractive summarization.  In this approach the content of the input document(s) is broken into sentences which are scored for their relevance to either the document or to a query.  We'll demonstrate it's use on a wikipedia article.\n",
    "\n",
    "\n",
    "### Abstractive  summarization example\n",
    "\n",
    "We'll use T5 again to summarize some input text.  We do this because the text in -> text out interface as well as the multi-task fine tuning makes it a great vehicle for demonstration.\n",
    "\n",
    "\n",
    "### Span-based question answering example\n",
    "\n",
    "There are a variety of approaches to question answering.  Here we demonstrate one particular approach to the problem -- span detection -- where we feed a context paragraph and the question to the system and want the machine to identify the answer span within the context paragraph.\n",
    "\n",
    "<a id = 'returnToTop'></a>\n",
    "\n",
    "## Notebook Contents\n",
    "  * 1. [Setup](#setup)\n",
    "  * 2. [SumBasic Extractive Summarization](#extractiveSummarization)\n",
    "  * 3. [Abstractive Summarization with T5](#abstractiveSummarization)\n",
    "  * 4. [Extractive Question Answering with T5](#extractiveQA)\n",
    "  * [Answers](#answers)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-summer-main/blob/master/materials/lesson_notebooks/lesson_7_summarization_QA.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NYOyJBZHbLgv"
   },
   "source": [
    "[Return to Top](#returnToTop)\n",
    "<a id = 'setup'></a>\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "Let's set up our environment so we can grab the wikipedia page on Natural Language Processing.  You can modify the string to find the Wikipedia page of your choice.  We'll need NLTK to build our extractive summarizer.\n",
    "\n",
    "We'll also need the HuggingFace Transformers library for our abstractive summarization and question answering examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El7x22c9bLgy"
   },
   "source": [
    "Now let's get a document to summarize.  We'll use Wikipedia since it contains a large number of longer documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKbUf5Fq0Tf6",
    "outputId": "1d540c23-a5dc-47b3-ebf8-699d96872758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9_4igssSbLgz"
   },
   "outputs": [],
   "source": [
    "!pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qCp2UU4fbLg0"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UBxhOWtCaMO8",
    "outputId": "15a79b02-3f60-4e6b-ca9a-71b1e7d89ec0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unTmpeZBbLg0"
   },
   "source": [
    "[Return to Top](#returnToTop)\n",
    "<a id = 'extractiveSummarization'></a>\n",
    "\n",
    "## 2. SumBasic Extractive Summarization\n",
    "\n",
    "Let's run our extractive summarization example.  We'll use NLTK and a simple algorithm that relies on the frequency of words to identify sentences to extract and place in the summary.\n",
    "\n",
    "The advantage of these older counting approaches is that they can handle documents of arbitrary length and can easily run without a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pORm2-l9VxB0"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import sent_tokenize, regexp_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RGnjEX4cbLg2"
   },
   "source": [
    "Extractive summarization allows us to specify the size of the summary we want.  We will do it as a percentage of the size of the input.  Let's first grab a document to work with.  We'll grab the [Wikipedia article on natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing) since it is long.  Under the hood the system is breaking the document into sentences and scoring those sentences by their relevance to the document according to the SumBasic algorithm.  As a result the summary is a set of sentences copied directly from the original.  Some algorithms presented the extracted sentences in score order while others present in the order in which they appeared in the original document.  Why do you think that might matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OHvwr7Qw0TXm"
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# Get wiki content.\n",
    "wikisearch = wikipedia.page(\"Natural Language Processing\")\n",
    "wikicontent = wikisearch.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d38LCO03Nbb"
   },
   "source": [
    "First let's implement the SumBasic algorithm using some NLTK functions.  It's a very straightforward approach using probabilities to assign scores to each sentence and word and then pick the highest scoring sentences.  Those highest scoring sentences are extracted from the original and then printed as part of the summary.  The original paper (MSR-TR-2005-101) can be [found here](https://www.cs.bgu.ac.il/~elhadad/nlp09/sumbasic.pdf) as well as a [followup article by the same authors here](https://www.cis.upenn.edu/~nenkova/papers/ipm.pdf).  The idea is to try and score sentences for inclusion in the summary based primarily on word frequency.\n",
    "\n",
    "The basic algorithm is:\n",
    "\n",
    "1.   Compute the score of each word in the document by dividng the frequency of the word by the total number of words.\n",
    "2.   Compute the score of each sentence by computing the average score of its words\n",
    "3.   Select the highest scoring sentence that contains the highest scoring word and add to summary\n",
    "4.   For each word in the selected sentence, update its score by squaring the word score.  This makes words already in the summary score lower and sentences without those words score higher.\n",
    "5.   If summary not long enough, return to step 2 and recalculate sentence scores.\n",
    "\n",
    "The SumBasic algorithm is a computationally cheap way of creating an extractive summary of an arbitrarily long document.  Let's see what it looks like.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "V7rDfULL9jye"
   },
   "outputs": [],
   "source": [
    "#score the sentences and print the highest scoring sentence with the highest scoring word\n",
    "#keep repeating (with word score recalulation) until length is reached\n",
    "\n",
    "def sumbasic(lem_sentences, lem_words, size):\n",
    "\n",
    "    freq = FreqDist(lem_words)\n",
    "    total = sum(freq.values())\n",
    "    probs = {k: v/total for k, v in freq.items()}\n",
    "\n",
    "    len_summary = int(size * len(lem_sentences))    #calculate number of sentences to put in the summary\n",
    "\n",
    "    summary = []\n",
    "\n",
    "    for _ in range(len_summary):\n",
    "\n",
    "        scores = {k: [] for k in lem_sentences}\n",
    "        importance = {k: 0 for k in scores}\n",
    "        for key, value in lem_sentences.items():               #recalulate the sentence scores\n",
    "            for word in value:\n",
    "                scores[key].append(probs[word])\n",
    "            importance[key] = sum(scores[key]) / len(scores[key])\n",
    "\n",
    "        most_importance_sentence = max(scores, key=scores.get)  #pull out the most important sentence\n",
    "        summary.append(most_importance_sentence)\n",
    "\n",
    "        for word in lem_sentences[most_importance_sentence]:    #recalculate word scores\n",
    "            probs[word] = probs[word] * probs[word]\n",
    "\n",
    "    for sentence in lem_sentences:\n",
    "        if sentence in summary:\n",
    "            pprint(sentence, compact=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByUGQ8N0G6q5"
   },
   "source": [
    "Now let's run the SumBasic function now with the Wikipedia page on NLP and lets ask for a summary that is 5% of the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yLYOnQWx3Gq3",
    "outputId": "87ca0404-d70d-4eb0-ae28-7255313e0679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('== Approaches: Symbolic, statistical, neural networks ==\\n'\n",
      " 'Symbolic approach, i.e., the hand-coding of a set of rules for manipulating '\n",
      " 'symbols, coupled with a dictionary lookup, was historically the first '\n",
      " 'approach used both by AI in general and by NLP in particular: such as by '\n",
      " 'writing grammars or devising heuristic rules for stemming.')\n",
      "('language models, produced by either statistical or neural networks methods, '\n",
      " 'are more robust to both unfamiliar (e.g.')\n",
      "('Rule-based systems are commonly used:\\n'\n",
      " '\\n'\n",
      " 'when the amount of training data is insufficient to successfully apply '\n",
      " 'machine learning methods, e.g., for the machine translation of low-resource '\n",
      " 'languages such as provided by the Apertium system,\\n'\n",
      " 'for preprocessing in NLP pipelines, e.g., tokenization, or\\n'\n",
      " 'for postprocessing and transforming the output of NLP pipelines, e.g., for '\n",
      " 'knowledge extraction from syntactic parses.')\n",
      "('Some of these tasks have direct real-world applications, while others more '\n",
      " 'commonly serve as subtasks that are used to aid in solving larger tasks.')\n",
      "('In natural speech there are hardly any pauses between successive words, and '\n",
      " 'thus speech segmentation is a necessary subtask of speech recognition (see '\n",
      " 'below).')\n",
      "('Word segmentation (Tokenization)\\n'\n",
      " 'Tokenization is a process used in text analysis that divides text into '\n",
      " 'individual words or word fragments.')\n",
      "('Sentence boundaries are often marked by periods or other punctuation marks, '\n",
      " 'but these same characters can serve other purposes (e.g., marking '\n",
      " 'abbreviations).')\n",
      "('Semantic parsing\\n'\n",
      " 'Given a piece of text (typically a sentence), produce a formal '\n",
      " 'representation of its semantics, either as a graph (e.g., in AMR parsing) or '\n",
      " 'in accordance with a logical formalism (e.g., in DRT parsing).')\n",
      "('NLP-powered Document AI enables non-technical teams to quickly access '\n",
      " 'information hidden in documents, for example, lawyers, business analysts and '\n",
      " 'accountants.')\n"
     ]
    }
   ],
   "source": [
    "#get the wiki article and break it first into sentences using NLTK's sent_tokenize\n",
    "all_sentences = sent_tokenize(wikicontent)\n",
    "\n",
    "#Let's walk through each of these sentences so we can divide into tokens (e.g. words)\n",
    "word_tokens = []\n",
    "sentence_tokens = {sentence: [] for sentence in all_sentences}\n",
    "\n",
    "for one_sentence in all_sentences:\n",
    "    for token in regexp_tokenize(one_sentence.lower(), '\\w+'):  #divide the sentences into tokens based on the regex for whitespace\n",
    "        if token not in string.punctuation:                     #ignore punctuation\n",
    "            if token not in stopwords.words('english'):         #ignore stopwords\n",
    "                word_tokens.append(token)\n",
    "                sentence_tokens[one_sentence].append(token)\n",
    "\n",
    "#A lemmatizer takes conjugated verbs and returns their infinitive form (e.g. conjugating -> conjugate)\n",
    "#It does the same thing with nouns taking the plural form and returning the singular form.\n",
    "#We're doing this because we want to count up occurences of word roots to get a tighter distribution\n",
    "lem = WordNetLemmatizer()\n",
    "lem_words = [lem.lemmatize(word) for word in word_tokens]\n",
    "lem_sentences = {sentence: [lem.lemmatize(word) for word in sentence_tokens[sentence]] for sentence in sentence_tokens}\n",
    "\n",
    "#Now we have a list of lemmatized words and a list of sentences containing lemmatized words\n",
    "#we pass them to the sumbasic fiunction along with a size parameter\n",
    "#We'll also pass a summary size as a percentage of the sentences in the original document\n",
    "sumbasic(lem_sentences, lem_words, 0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-uhdR0oz_ge"
   },
   "source": [
    "[Return to Top](#returnToTop)\n",
    "<a id = 'abstractiveSummarization'></a>\n",
    "\n",
    "## 3. Abstractive summarization with T5\n",
    "\n",
    "Let's set up our environment to run the Hugging Face version of T5 and feed it a small snippet of text to see what kind of summary it produces.  Note that we could not feed the entire Wikipedia article we used above into T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "a5T3hjU60VRe"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "qMUH5Xsj0VLN"
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nzpCG4QbLg7"
   },
   "source": [
    "Here's the text that we'll summarize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "y7N0FrSK0VAj"
   },
   "outputs": [],
   "source": [
    "WARTICLE_TO_SUMMARIZE = (\"A neutron star is the collapsed core of a massive supergiant star, which had a total mass of \\\n",
    "            between 10 and 25 solar masses, possibly more if the star was especially metal-rich. Except for black holes, \\\n",
    "            and some hypothetical objects (e.g. white holes, quark stars, and strange stars), neutron stars are the smallest \\\n",
    "            and densest currently known class of stellar objects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 611,
     "referenced_widgets": [
      "56c938bbe9024b968d924135094808ae",
      "1b7540c260514dcf84b28d5c891c4780",
      "e7c682c87ef64117ba2e22dbaedb0276",
      "a90e67d9ab2d4d0193bd003b9bb51b1d",
      "c428d70b94904b72bcc54e69a0832128",
      "a77836b47da343e5bf53bfa9ee989eb8",
      "aec33ed823254f4c816b7553e5051fb6",
      "fad50373f12f48ef97b16a39f611c250",
      "623f909a766244078baedcefaad0d0c5",
      "9ad95dff7741400a8bf3826707954894",
      "c336c7199c5b4118823b148b51f7ff1e",
      "5f58292761354a4eadc62680caf5e646",
      "fe7677050021485e82abd784eb89522e",
      "ed4422028b8b4fea86f2c0d65caa8709",
      "e20bad1b930f4ed8a4bfd0902fd11244",
      "2d7111e127bb456789cd5030be059b0b",
      "48c32bb5831a49aeaa89b449019ff554",
      "6ea0fb0d837443a58b8c6d3d85908f75",
      "7b6e61a080244c95b5add82b28be03e3",
      "d4f1acbabd7e40b6a1563e80476c2916",
      "d69cfc65a17045a4bbbc8418120ee92c",
      "05ac24b2a89b4c928562a5e83b7738c1",
      "7c35672c2df84ef8b78e144e36a0d36a",
      "352a858e9ca74d05a69fce279127d957",
      "d79808d54c1147d5bf27db3cd347dc30",
      "e543f95712d648a48305c18a0de24087",
      "bd04b985344c4dd7b838e06d63c9b646",
      "d25958c65e1c4d629df3e0b88ec2d324",
      "669494dd4c204e5d8158d82b6248c184",
      "fae8ec0fd9224543a41dade721c70694",
      "0054126d6cd2400ea23718a393e342bf",
      "10192052a67f4ff0b46a8329c5f19981",
      "afd0c822f7664987940546f581e6f869",
      "0fcea3bb171b41b98e29f3515881b647",
      "55914addb75649d2b6a200566e9fb0fc",
      "66e7673edc4d4e18bba6aa7943d98d50",
      "727c89f2fc3044888c31e59564058b3e",
      "8388ce27fa574022ae74b372be833dfa",
      "16da0c61baa24fbca95689fd2aa30a95",
      "910d32ea59c240adad77f4aa6f870d6e",
      "6223f22d73764bbeaa64f708c54af3c8",
      "48a3bdf9838346389aeabcd9bb3990ae",
      "e480c927144f4cc5a68f7ee781da9cef",
      "93a30bb687e54f30abb1afd89e5561d2"
     ]
    },
    "id": "Sf5QPJGm0U2a",
    "outputId": "926a0141-fd27-43de-f207-a8ef290fa0e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c938bbe9024b968d924135094808ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f58292761354a4eadc62680caf5e646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c35672c2df84ef8b78e144e36a0d36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fcea3bb171b41b98e29f3515881b647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  24674304  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  109628544 \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  137949312 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222903552 (850.31 MB)\n",
      "Trainable params: 222903552 (850.31 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-base') #also t5-small and t5-large\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "t5_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_WqdYm7bLg9"
   },
   "source": [
    "Don't forget to add the prompt to the begining of the article so T5 knows what we are asking it to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "jWdPetvc3tm_"
   },
   "outputs": [],
   "source": [
    "t5_input_text = \"summarize: \" + WARTICLE_TO_SUMMARIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "839JTWPX3tbp"
   },
   "outputs": [],
   "source": [
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjY3c3kIbLg9"
   },
   "source": [
    "Here's the output.  The sentence is quite fluid.  How faithful to you think it is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xirurxbm3tMc",
    "outputId": "4fdcc339-958e-4b32-ba8a-20592598a2a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a neutron star is the collapsed core of a massive supergiant star . neutron stars are the smallest and densest currently known class']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                    num_beams=3,\n",
    "                                    no_repeat_ngram_size=3,\n",
    "                                    min_length=15,\n",
    "                                    max_length=35)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJ_mCsHo0GrS"
   },
   "source": [
    "[Return to Top](#returnToTop)\n",
    "<a id = 'extractiveQA'></a>\n",
    "\n",
    "## 4. Extractive question answering with T5\n",
    "\n",
    "Now let's look at an extractive question answering example.  We'll need to feed the model a context paragraph and a question.  The T5 model was pre-trained on the SQUAD dataset so it knows how to identify and extract the answer span. Note that we already have the prompt in the respective texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xE5ANINx0XM6"
   },
   "outputs": [],
   "source": [
    "t5_context_text = \"\"\"context: Hyperbaric (high-pressure) medicine uses special oxygen\n",
    "chambers to increase the partial pressure of O 2 around the patient and, when needed,\n",
    "the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness\n",
    "(the ’bends’) are sometimes treated using these devices. Increased O 2 concentration\n",
    "in the lungs helps to displace carbon monoxide from the heme group of hemoglobin.\n",
    "Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing\n",
    "its partial pressure helps kill them. Decompression sickness occurs in divers who\n",
    "decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen\n",
    "and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible\n",
    "is part of the treatment.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "dUkH1xrl0XQE"
   },
   "outputs": [],
   "source": [
    "t5_question_text = \"\"\"question: What does increased oxygen concentrations in the patient’s\n",
    "lungs displace? \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "d7KSOnfm0XJO"
   },
   "outputs": [],
   "source": [
    "t5_qa_input_text = t5_question_text + t5_context_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCi0yvlqbLhA"
   },
   "source": [
    "Now let's run T5 and see how well it answers our question.  What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X8XW3u6T0XGB",
    "outputId": "46a1467b-a2e1-4a10-e3fe-741236876599"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/tf_utils.py:836: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carbon monoxide']\n"
     ]
    }
   ],
   "source": [
    "t5_inputs = t5_tokenizer([t5_qa_input_text], return_tensors='tf')\n",
    "\n",
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'])\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
