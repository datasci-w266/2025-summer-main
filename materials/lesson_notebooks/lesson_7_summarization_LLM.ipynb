{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9eZI4y7U88b"
   },
   "source": [
    "## LLaMa 3.1\n",
    "\n",
    "LLaMa 3.1 is a year old model from Meta. Check out [the model card](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct) for further details. It is open-sourced.  To use it, you need to log in to your Hugging Face account and get permission.  We're using the 8 billion parameter version but quantized so it has a much smaller memory footprint.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-summer-main/blob/master/materials/lesson_notebooks/lesson_7_summarization_LLM.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDr0c9pgi5Qa",
    "outputId": "ef7d6e5e-c0a1-42b5-f869-0780291c5ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7IMPokHgtffJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWwuz03eAwoI"
   },
   "source": [
    "Here's some text from the introduction to [The Prompt Report: A Systematic Survey of Prompting Techniques](https://arxiv.org/pdf/2406.06608).  Let's have the model summarize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oo3IBfof2a49",
    "outputId": "62a33533-ad75-4f19-e5cf-40e61c49e397"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1899"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ARTICLE = \"Scope of Study We create a broad directory of prompting techniques, which can be quickly understood and easily implemented for rapid experimentation by developers and researchers. To this end, we limit our study to focus on discrete prefix prompts (Shin et al., 2020a) rather than cloze prompts (Petroni et al., 2019; Cui et al., 2021), because modern LLM architectures (especially decoder-only models), which use prefix prompts, are widely used and have robust support for both consumers and researchers. Additionally, we refined our focus to hard (discrete) prompts rather than soft (continuous) prompts and leave out papers that make use of techniques using gradient-based updates (i.e. fine-tuning). Finally, we only study task-agnostic techniques. These decisions keep the work approachable to less technical readers and maintain a manageable scope. \"\n",
    "\n",
    "ARTICLE += \"Sections Overview We conducted a machine-assisted systematic review grounded in the PRISMA process (Page et al., 2021) (Section 2.1) to identify 58 different text-based prompting techniques, from which we create a taxonomy with a robust terminology of prompting terms (Section 1.2) While much literature on prompting focuses on English-only settings, we also discuss multilingual techniques (Section 3.1). Given the rapid growth in multimodal prompting, where prompts may include media such as images, we also expand our scope to multimodal techniques (Section 3.2). Many multilingual and multimodal prompting techniques are direct extensions of English text-only prompting techniques. \"\n",
    "\n",
    "ARTICLE += \"As prompting techniques grow more complex, they have begun to incorporate external tools, such as Internet browsing and calculators. We use the term ‘agents‘ to describe these types of prompting techniques (Section 4.1). It is important to understand how to evaluate the outputs of agents and prompting techniques to ensure accuracy and avoid hallucinations.\"\n",
    "\n",
    "len(ARTICLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "h2JdB4kcjHF9"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz_4_94-BbPG"
   },
   "source": [
    "Let's load the LLama 3.1 model using the Pipeline abstraction from Hugging Face and before we have it summarize, let's ask it a question and see how well it answers.  Do you think the answer is accurate?  How might we evaluate the generated answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722,
     "referenced_widgets": [
      "9ad8b5508b8d4fee895dff558f14f14f",
      "0efada3ab24a47aca2c0612c48013b16",
      "e21f20a67bdc49ada354c2e91b929b86",
      "ce2749535b1c4bdfb2a22ddf6cf4ad5d",
      "fa3a2de154ad4c1b819cd560ee05b978",
      "28ab6d0be66347e49972302fec38b6be",
      "00b3fe2406754db28371746a8caf0ff4",
      "908bd3fcabc64b9d92a6bcde243a5052",
      "1435cb2349ae48cf8c6c5fb1c9a184d7",
      "cab8d10a665b45908f7dd647939808f4",
      "4af172a98c3046a083cccc5e7e62ad5b",
      "c1215281b2fd4128af95b04c90e25d45",
      "2366bd71ff184912949c815fff39d73c",
      "5ccf22adf567447d9ea47d547693f9bc",
      "33e300d3207c4c03987bd3ec0fa537a9",
      "33b3d607f5674b8996447047ebfc906a",
      "f6ed22426ecc478f9328534c27a70754",
      "080890096cf94730b01bcb093b6c8e74",
      "f18af3625d0c4b08bdf6575aebab9ba1",
      "ed3361248a054bd8a1268b8878fc00e8",
      "a15de4d44a5b43acb4ac4bbd4718e5e4",
      "35b1cfcff9c64c71a46d0061d0071f4e",
      "3abff877848e4320a8d449da6a181145",
      "3dfdc74514bd498080b638fba6d81cfe",
      "05525b277c614de3bd347bb1c1aba072",
      "9dd245c3fa5641508011bbd321125f8c",
      "5637d16fef19409a9e6b08e2f5b2ab45",
      "ffdc3e3ca93f40f6acc6dabb0f7a3b5c",
      "f1a6faf32c2944f385e18c8cea1c4573",
      "eb087d8b23604dc9b6177900d40a8278",
      "bd59f96e9aed46449808771f39b034d4",
      "89027741c1914a61b85170d64ebd5901",
      "be2873d7a2294c28b3a6edbf10d1182f",
      "cb4694e6f3fc4c5bb47fcf40d26c611e",
      "a44b69a879c24394b9bd6cd0645e9396",
      "ea4b4046357f4f5ab920da3aacb1d6ed",
      "3e4b9b6bf2b747dd82c5c90670540512",
      "549877ddad544b4abf55d9fa35e95a8e",
      "a2a2e44dd25b4ad0a31a8e0fcd5eeac4",
      "60546d4a44b646eca50c5bdf28877f5b",
      "bb937084037b40d696836298d69a1d38",
      "1e6f65b8957048d49f4fef9eebdbabe1",
      "c3777e3e510c43f1939e5c6b7d98d567",
      "422d39a0c2714c50874e73fcdd680786",
      "6df5715abf5949d391d9e6a4f33477b0",
      "11faca6f0fb8489299bf0d7ea01e46b7",
      "64f1fbe247e844be9fed481e82f38728",
      "1840ce7a6b854933822b023e92e7ff0a",
      "82bb12b9142447c4a58fd41b817ee4d0",
      "680a9c25b47647488825c23ff157d268",
      "912fbdc5fa11479286dda736cfc09f98",
      "3cbb4f763c96433c8e0ddd193164d86b",
      "3cb5302beb8043e7bbe4a0db76adc77d",
      "410225cb4bfd479d8ea9ae0b79c0bf90",
      "bf66ca7cdd9b4dbca919c891d343d9cc",
      "47f9baac5de54acc95cd0c01b18f5761",
      "543fb4059a804a94bcc084d90d19d192",
      "fe8a7e8457c44faaa3c2a4f3124e107c",
      "dbc74aedbe924ea38af03b8eba0b62b0",
      "a84d609969ad4411b265f909a99da7da",
      "11756f5a72a0438c9066e5f58b886c50",
      "4ce96fdc86634e54ba05ca965c020a2f",
      "44c6a6c3575340cfa633dd257806bcdb",
      "37be2c0ef8774596924f66a3d6005fc4",
      "d3cf3322bfbd4875a3b948a0941800d3",
      "9622338697834ebfbe738676c07abb7d",
      "996af987d81e491c827f8fc66c06d7fa",
      "2519f94bbcfd4495a4e0595fbd356412",
      "659268b52a874ef3bf06c061dc728887",
      "17b51156fa564269884435ba20fb3148",
      "2e81fa29c07e4c9088c56706c85d7843",
      "072bf71cacad4a96b8b9021db1957ed0",
      "97b8b22b16b640ad8b13d45e2155e57a",
      "71c2cc477aa047f5b51cfb1c442e4118",
      "a868ed38fb194c0a93e7e660223eed45",
      "28df171d43e848afaa2657360cc71899",
      "c17bae0e211c4fb1a1d015769028884e",
      "eb415626dec7428f91ce1d4e13347ebe",
      "dcd877bdc0854b02908d68f818784644",
      "c6a129dcfe5049cfabe4154f3ef51d66",
      "0471aad4655748b8b7c814ca59490d27",
      "50b8b817bd7a416e85fff65c9ee8d54c",
      "d2dc0cb520d848b69ea18caa62f2c015",
      "1a83c3f4a0a349da9d5ec03e4cfe7b54",
      "9bf41c89d5b34d6c8098aa588a5fdbb3",
      "bbf109bf2df44a4a8811b9b9f44389df",
      "18d71cb530bb443b9f06ea4fe907acb8",
      "195a3276b3fa4eb8874c34dc16415696",
      "f2c619a76c7b429c83059f43803d36d7",
      "bba06046f151480fa6b05165dde4c601",
      "d2d8904555294b20bb49767927c7d382",
      "21174a8f80ba457c99ab24d40a39eb8f",
      "88f66e6f57c14a32ae8f76ff06c46723",
      "5182a57269b84a06ab31eda59d7acce6",
      "10e6eb0fcfac489e9d20c3bd63a154d1",
      "72fbb048079b4bbf994df23943840c92",
      "31e50f6da81b4d4ab2fd30adbd331be0",
      "ee3b729de39f4328b633b7d1697428a0",
      "f9a005e21472498e9170f995907634fc",
      "6421ee0bc4664439a019f3cdc68d8ca4",
      "88dbf4c425ba40a0bc34374622c47fa8",
      "366a12ff4aae485584f2f8c7c2462b37",
      "357b1e40a8674c6a98882ca56752522b",
      "2828d65dee6c4acbabd602dae21710cf",
      "c5643394b0824d5abe27a790b526a0b4",
      "73190e906ffb4a2fa98bc3e20f981849",
      "13e18657fb244eea85d82ad3b675794c",
      "1a37008459214ddcb048e813925f3caa",
      "7a40399cc9824dfaa4bea9d7e7187e18",
      "88b32b7da77e4a47bb30b25ea5a0da9f",
      "6bd96927cf4b4667a41bb0d10e4e535d",
      "e9fd099ebb4b41d38aa8354ccea16111",
      "9b527e4c821d4ee79a5a2c5167dccedf",
      "32e4bc04f7aa4dabb48f12774c119373",
      "1935a0ff04384c72b5d58a734dc2c498",
      "6c29bbf8dc7b44629ea1f7246b1dba00",
      "3cce5c01717f46038f72e6453865252d",
      "9f567ff7ef754bd6849f4a8c860252b9",
      "0f483cda72dc443a8b69b4b877848e34",
      "2b9689d8a4204a1dba2fa8fa66b13433",
      "7061b6a9156e45f698c0341d550c52fc",
      "63c9ac2dedaa46f190408090758b6e6e",
      "40a09ff1ea584642af8845c462b828bd",
      "1c3bf85f1e304dfd814476002be19f62",
      "92c8a4a989e14289ab3993df530769b1",
      "9b566778356747468527931c75dd377e",
      "8043d9f11b994e2c851cebbd76cbd5d7",
      "b5802473829545e6b0a49e9c0b43c003",
      "35a3eb45b060470480b22994f6c54fd0",
      "b60614ec798946d58942c35a9dd20039",
      "e6e74668ea164cc1a1b64f010ad374e7",
      "db8d8f5e80024e80aa3a638a95c0b7aa"
     ]
    },
    "id": "CPYJUPmBU-no",
    "outputId": "2d134c92-db0f-4e7e-e2e3-cb750fbb0775"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad8b5508b8d4fee895dff558f14f14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1215281b2fd4128af95b04c90e25d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abff877848e4320a8d449da6a181145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4694e6f3fc4c5bb47fcf40d26c611e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df5715abf5949d391d9e6a4f33477b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47f9baac5de54acc95cd0c01b18f5761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996af987d81e491c827f8fc66c06d7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb415626dec7428f91ce1d4e13347ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c619a76c7b429c83059f43803d36d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6421ee0bc4664439a019f3cdc68d8ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd96927cf4b4667a41bb0d10e4e535d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c9ac2dedaa46f190408090758b6e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Large Language Models (LLMs) like myself represent knowledge '\n",
      "            'through complex neural networks that process and store vast '\n",
      "            'amounts of information in a distributed manner. At the heart of '\n",
      "            'this process lies a type of knowledge representation known as a '\n",
      "            '\"knowledge graph,\" where interconnected nodes and edges capture '\n",
      "            \"relationships between concepts, entities, and ideas. When I'm \"\n",
      "            'trained on a dataset, I build a graph-like structure where each '\n",
      "            'node represents a word or concept, and the edges between them '\n",
      "            'signify the semantic connections between these nodes, such as '\n",
      "            'synonyms, antonyms, or co-occurrences. This knowledge graph is '\n",
      "            'then used to retrieve and generate text based on patterns and '\n",
      "            'associations learned from the training data. By leveraging this '\n",
      "            'knowledge graph, I can draw inferences, answer questions, and '\n",
      "            'even generate novel text based on the relationships and concepts '\n",
      "            'stored within it.',\n",
      " 'role': 'assistant'}\n"
     ]
    }
   ],
   "source": [
    "#import transformers\n",
    "#import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16, \"quantization_config\": quantization_config},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a science communicator who makes technology accessible to everyone!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please write a five sentence explanation of how LLMs do knowledge representation.\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "pprint(outputs[0][\"generated_text\"][-1], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFjMNa0w2AOn"
   },
   "source": [
    "Now let's try it for abstractive summarization.  Note that it takes a lot longer to generate answers because this model has 8 billion.  The next cell can take up to 2 minutes to complete.\n",
    "\n",
    "How good is the output from Llama3.1?  How can we measure the performance? What's are all of the elements we need to say run ROUGE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUqNgfpIVATV",
    "outputId": "23ef5e76-8dd8-4ab4-99d2-9eed2db9d09c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Imagine you have a super smart computer program that can understand and '\n",
      " 'respond to questions or tasks. To help people use these programs better, '\n",
      " 'researchers are studying different ways to give them instructions, called '\n",
      " '\"prompts.\" They\\'re focusing on simple, specific instructions, like \"What\\'s '\n",
      " 'the capital of France?\" rather than more complex instructions that might '\n",
      " \"need to be adjusted as they're used. They're also looking at how to use \"\n",
      " 'these prompts with different languages and with pictures or other types of '\n",
      " \"media. To make it easier for people to understand, they're breaking down the \"\n",
      " 'different techniques into categories and explaining them in a clear way.')\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert on natural language processing.  Please summarize the following content for a fifth grader. Your summary should be no longer than five sentences.\"},\n",
    "            {\"role\": \"user\", \"content\": ARTICLE},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "#lets set some values to have more control over the output\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "pprint(outputs[0][\"generated_text\"][len(prompt):], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZADKJf1-B_B"
   },
   "source": [
    "Try it yourself.  You can fill in the system and the user portion of the prompt.  See what kinds of questions it can answer and see how well it summarizes content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mceHEu4A95Iv",
    "outputId": "3fedfc61-aa71-43c3-88e1-3f8f596fcaa8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('It seems you just repeated the phrase. Could you provide more context or '\n",
      " 'clarify what you mean by \"Your Value Here\"?')\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Your Value Here\"},\n",
    "            {\"role\": \"user\", \"content\": \"Your Value Here\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "#lets set some values to have more control over the output\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "pprint(outputs[0][\"generated_text\"][len(prompt):], compact=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
