{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wy37jVQVufTY"
   },
   "source": [
    "## Lesson Notebook 6: Machine Translation\n",
    "\n",
    "In this notebook we will look at several examples related to machine translation:\n",
    "\n",
    "   * Simple translation examples with T5\n",
    "\n",
    "   * Translation example with M2M100 - many more languages\n",
    "\n",
    "   * MT metrics examples\n",
    "\n",
    "   * Subword models and tokenizers\n",
    "\n",
    "\n",
    "\n",
    "<a id = 'returnToTop'></a>\n",
    "\n",
    "## Notebook Contents\n",
    "  * 1. [Setup](#setup)\n",
    "  * 2. [Simple Translation Model](#simpleTranslation)\n",
    "  * 3. [M2M100 Translation Example](#m2mTranslation)\n",
    "  * 4. [Machine Translation Metrics](#translationMetrics)  \n",
    "  * 5. [Subword Models](#subwordModels)\n",
    "  * [Answers](#answers)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-summer-main/blob/master/materials/lesson_notebooks/lesson_6_Machine_Translation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkM-Z34gIvyo"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'setup'></a>\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "\n",
    "We'll start with the usual setup. We need to begin with the sentencepiece code in order to tokenize the text for some of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bZSrl0npwP2X"
   },
   "outputs": [],
   "source": [
    "!pip install -q sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kp16mZ73x-x2",
    "outputId": "4618177b-76a1-410d-8152-cbb5813d6aaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
      "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
      "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
      "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
      "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow -U --quiet\n",
    "!pip install keras -U --quiet\n",
    "!pip install tensorflow-datasets -U --quiet\n",
    "!pip install tensorflow-text -U --quiet\n",
    "!pip install transformers -U --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzHlP57n-oW8",
    "outputId": "9d09a7b1-89ab-4c2d-9149-f1df7cdf868f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyNZZ984Z5pE",
    "outputId": "788097ba-3b5f-42c5-802b-7899b8cfb59d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for keras-hub (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "keras-nlp 0.18.1 requires keras-hub==0.18.1, but you have keras-hub 0.22.0.dev0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q git+https://github.com/keras-team/keras-nlp.git --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMt9qlNlKVsZ",
    "outputId": "4d24c5e7-0ce9-4837-d6e4-2961e5ac9fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun  8 23:44:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   41C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#Am I running a GPU and what type is it?\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49yFXHMzEckg"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'simpleTranslation'></a>\n",
    "\n",
    "\n",
    "## 2. Simple Translation Example\n",
    "\n",
    "These T5 models are trained to translate in one direction only.  For example, they can translate from English to French but not from French to English.\n",
    "\n",
    "Let's test this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZuHgxaqAutA2"
   },
   "outputs": [],
   "source": [
    "#import T5 and show\n",
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RuDEYpkxus14"
   },
   "outputs": [],
   "source": [
    "SENTENCE_TO_TRANSLATE = ( \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \\\n",
    "            amid dry conditions.\")\n",
    "\n",
    "BACK_TRANSLATE_TEST = (\"PG&E a déclaré qu'elle avait prévu les panne de courant.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522,
     "referenced_widgets": [
      "91f840be327b436cbc90027de9d18a85",
      "41a77b5f519d4cacbfa0d22c5e076159",
      "c98537abfe914c41b07b06e4f3336c2f",
      "c2e2c605441f414fa1511b7de19e230b",
      "3addd2bbffb24667a96b5f67be93e51d",
      "57c7d524c48a4b70835a2d19e9dba274",
      "41316708246a4efca817226e56132373",
      "9199f0250c3d418b82fe5547df157a82",
      "1d85b427634f4ee782def4a95d9aff58",
      "ca5931cc2977429887917c820d6a96ba",
      "a1a5262ab96644edb12d5895148bd5a0",
      "ddc150c689244952b856c60d612b51bb",
      "68b7cf4b1a3744f69ba27bab9d8817d3",
      "51861c9ba1c440a2b19f91f69cc7faa3",
      "3794d2b4c0e14e689883d02509ae70d1",
      "f63f9a75435f4cd3b9d1fed1477781d2",
      "177be5d2c7a64c2e953f33c13853ff65",
      "bfca285b41dc43f78e07acdcd90ccd69",
      "39d6c903bef54e9f839cf5bd8b556c66",
      "cba9bddcb3bf46108ca1f679808ca24c",
      "065b7c471bf543fd93243289fe9d90eb",
      "870cc967a5eb401eae1228c8a21363b3",
      "5870b3d1d78e4a899a4ad60855987678",
      "039b7bb391f148cf9299c71d6e70322a",
      "e673dac9a2d54988b60d7086b22bc7b3",
      "4186e2e068f04f6db90c05374150a525",
      "15c4ae7e2fb244909bfd6d8244512241",
      "07a79c1d762d48bfba6458eb5984531e",
      "0aeaba311985414da810ebb11c10fc0d",
      "b808faeecbab4b06b809f6eea59ef1e5",
      "3fe4ee05c68a4df4ab3b04712f4b9203",
      "64ecd938aeb84f5a93543c059ead63a7",
      "3d9492c98d5e4663a11b4db214c8e2ba",
      "da49d1f30cc84bf8a8be5a3d58763ed5",
      "24f46b0066284c6ca9712abc17f8cc20",
      "758e97b2b1d646e6a0d3a21fcf14330a",
      "295056d69abe4c93a4b1de61f2c20232",
      "aa45cf6617f94dbbaa19b90b1fcea73d",
      "48ef9a31b7534ca0892fef2c38bbf453",
      "2dfb21fc79174f55928a9114bfa4aaaa",
      "0ee7b93d7d2a4cf585d6b6f0c0053372",
      "c828ae7cd4a44e75beaa29e4b70b2063",
      "4a3ad1444c8343b9b2245950ad647259",
      "fb802a4199524b9bacbf892a8f7c28d4"
     ]
    },
    "id": "cPgIuk5gusoh",
    "outputId": "556b8794-bf24-48fe-c74e-ec4af6724c7b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f840be327b436cbc90027de9d18a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddc150c689244952b856c60d612b51bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5870b3d1d78e4a899a4ad60855987678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da49d1f30cc84bf8a8be5a3d58763ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  24674304  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  109628544 \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  137949312 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222903552 (850.31 MB)\n",
      "Trainable params: 222903552 (850.31 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-base') #also t5-small and t5-large\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "\n",
    "t5_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybohJuVpfAsc"
   },
   "source": [
    "Add the prompt to the sentence we want to translate so the model knows what we want it to do with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hgJctlSrusTB"
   },
   "outputs": [],
   "source": [
    "t5_input_text = \"translate english to french: \" + SENTENCE_TO_TRANSLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "AVPtcA0yyxDg"
   },
   "outputs": [],
   "source": [
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww6UCZHifPW4"
   },
   "source": [
    "**QUESTION 1**: What do the inputs look like?  We've already seen BERT inputs. What's happening with T5? What's the same as what we saw with BERT and what's different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8BiEBWAp1VU2",
    "outputId": "65bf143f-b763-4749-d765-51e6b1c2e646"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 29), dtype=int32, numpy=\n",
       "array([[13959, 22269,    12, 20609,    10,     3,  7861,   184,   427,\n",
       "         4568,    34,  5018,     8,  1001,   670,     7,    16,  1773,\n",
       "           12,  7555,     7,    21,   306, 13551, 18905,  2192,  1124,\n",
       "            5,     1]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 29), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fkfo7XPGyw4T",
    "outputId": "857b8f18-3f18-49b6-91d8-1f5841e22224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"PG&E a déclaré qu'elle avait prévu les panne de courant\"]\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                   max_length=20)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHg0cLezfzYS"
   },
   "source": [
    "Not bad. Now let's try the reverse even though we know the model wasn't trained to translate in that direction.  What do you think it will do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1zlg8iOiywth"
   },
   "outputs": [],
   "source": [
    "t5_back_text = \"translate french to english: \" + BACK_TRANSLATE_TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6SyETk-IywfV"
   },
   "outputs": [],
   "source": [
    "t5_binputs = t5_tokenizer([t5_back_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpC6MVef4XaO"
   },
   "source": [
    "The decoder still runs and emits language, specifically French, as requested.  These models will pretty much always produce some output but you need to make sure that you're asking it to do something it can and that it is doing the right thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zvcgylb72wbY",
    "outputId": "e0a6070f-f02b-4466-c75e-37d25c7875a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"PG&E a déclaré qu'elle avait prévu les panne de courant\"]\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_binputs['input_ids'],\n",
    "                                   max_length=20)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj7FlVxsufTr"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'm2mTranslation'></a>\n",
    "\n",
    "\n",
    "## 3. M2M100 Translation Example\n",
    "\n",
    "M2M100 is a large model that was pre-trained on many languages simultaneoulsy.  You do need to give it some clues about what you are expecting when it translates.  Typically this takes the form of specifying the input and the output languages.  Let's look at the tokenizer first. How can it handle 100 different languages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427,
     "referenced_widgets": [
      "bff4a11013a34211b408768845a39d09",
      "6fa33267c65543638cd81e6cdf80a25f",
      "cd114ccc08034be1b72d421bca4df474",
      "1f72dd39e1d541c0b67a02adcd8ab34f",
      "86f52baa3f7f4bbebcfc3032d3f7f436",
      "42bc5f22e8a147f39621f31421ff66f0",
      "a6a3d666245f41c9be65d5a777f75051",
      "0f3bb874ce1441149447c211f6522f41",
      "c4ec58a4981440da94c44bf466cfd3ba",
      "7ea9d6e026db45689ab9f5a755a667c0",
      "c828972220054d58bbcfe66869e04099",
      "bfbd0487815f4c5089b817272673a419",
      "e5c3af767cbf487aa29f98e9bf68ace8",
      "1ad976c37f1f42b78071bd856a0c14a2",
      "e446160d35a64addb46a9db66ed941ae",
      "52e72ac0ac6846feb8454956bfd3b571",
      "da4e235089064e09b27f9cb8d5dabee3",
      "5c817d6edb1940e7a12b5eee445d1583",
      "4a51ec8918354f068713f701c8fd94b7",
      "377707f9d5b64a38937eeff10f424fe1",
      "f387b0cd129547c19871871b145176ea",
      "645d52456e92473299a6aa19676527af",
      "a2e18394738d40abb4d7b50d3d3b8ea5",
      "bc4329174f8444cdb59cedb5febe318c",
      "3f131396dfb04da5a2c3265e4bb15a81",
      "5a5d275c96ec421082e9e2cf3cf60af1",
      "c593dcb8cc5f4737b43d726322078ccb",
      "3192cee684564d9cabfdac8aa3bce17c",
      "0d804949191140abbae06cb3ba934aaa",
      "5625df70873b4ff6a0a2156177dbb60a",
      "851e380251324f26b17d70d714b15961",
      "674b6133790c42b092b914bc585fcbd4",
      "6792805c082c4abd88970bf649963afc",
      "07d946d04ed348c4a3a16c2e9a167bd4",
      "764da6194f744699ab5b1f58925dea91",
      "e54f3e6bbc8f48fa937d94042b206e5c",
      "6d9cf22aedac4fc2b1c93c24d080c362",
      "3c61c4e296ce4318adadc19cac9754c9",
      "804abcd4d4914ae09d6099a04729bfe2",
      "9f64249d85a3428bb989b15936d37b2e",
      "9de1fa2faf904a81a08af84a4a1a7335",
      "46c56d4edbdd4077a8acf97fabb62d1f",
      "b54a0c06b89b4a7ab0b86b4a0928227e",
      "658de52bc3a44974aa57ac75c7a12184",
      "a4782de049a64f8f93736401adaa4e82",
      "a50eaef8a49e4ff6a272c503702edee0",
      "82f34a142e8c44b8af200aa4a444da89",
      "b1b926237e154e84a2d35a33fc95114b",
      "ca98ec2d8fad48e595c12b0b3cb91b99",
      "94de9d8e99f3440a896417543074ca8b",
      "05471b3bd27743028eadfbdd0043264c",
      "67576a423cde4448b07b0d225d2bd3c6",
      "d8c8f42acef345c191d03fd70d64ee64",
      "f1c1b3e04d394cde9590d0d38f6eb10c",
      "b266e495147f4ece998902765a47439c"
     ]
    },
    "id": "sG_ACHmCS1uj",
    "outputId": "69a65ada-bd47-4ccb-adad-6fcc9911109c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff4a11013a34211b408768845a39d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/298 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbd0487815f4c5089b817272673a419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e18394738d40abb4d7b50d3d3b8ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d946d04ed348c4a3a16c2e9a167bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4782de049a64f8f93736401adaa4e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/908 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['▁Don',\n",
       " \"'\",\n",
       " 't',\n",
       " '▁you',\n",
       " '▁love',\n",
       " '▁',\n",
       " '🤗',\n",
       " '▁Transform',\n",
       " 'ers',\n",
       " '?',\n",
       " '▁We',\n",
       " '▁sure',\n",
       " '▁do',\n",
       " '.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import M2M100Config, M2M100Tokenizer\n",
    "\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"en\", tgt_lang=\"fr\")\n",
    "\n",
    "tokenizer.tokenize(\"Don't you love 🤗 Transformers? We sure do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ph96AzU-hGpr"
   },
   "source": [
    "Now let's try to translate.  We have two original sentences, one in English and one in Chinese that have roughly the same meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "4e3f2cb4eff54ab2b23bde9107611b12",
      "9a611306b3524906806f4d77142121f9",
      "f42178858ce9487ea8eef68e8382d9df",
      "f0b697d3053d41419777b700a5166ee5",
      "14764d73c6cf48b382e097cf5654b1d5",
      "f63d4d4a610841e7951b613c42abd68c",
      "31dcace7752d4d468e1ef6bae487b17d",
      "31ee43f33880407aa4fcc80c86fd2b47",
      "4902f00bc0d74c59b6fc7c7172442e09",
      "8c12737fe4df41098b543ea1bccf2ddd",
      "fff7ec2d6af143dfa4ac364d90725604",
      "7aebf8ec6f3f484f99ee2ce3881f2d40",
      "99ceec314a6646ea885a68a6696524e0",
      "2e633c337b124e53a0f66c1f185f15c9",
      "30cc262b65144a3b9e19e14b904c9698",
      "e10272b0efd64463ad22a9a351819a00",
      "b56e2c18df264c5385323d7850bc30f5",
      "c520530db2b2429f844ac89f744c61a9",
      "f3aa7c92a22c43e3837d73334a2fc52a",
      "fb4cac0a2e364bb486daecf424f54d08",
      "1d7d696d7c634ad09e5e449a1dfbbbdc",
      "decdb6a855a4473e8d701db00aa639f3",
      "e5786829e71842bb9f1d650a65ae3b0b",
      "9157f62357c14c2a94d5b23c169d1576",
      "92d6046c87134129bb6ad077f7154bf9",
      "6395e824020343b4b2b7c048dac0f13b",
      "346cb0a63e3c4167bb6cb6c097194f30",
      "d19a4971021f49549374df87f36f4e62",
      "c967bd0c03dc45a2a4800af627393602",
      "f03fb092f11c4bc697875692440855f3",
      "ee6da52066214f6db9268d7a80787f37",
      "09fc82fc1b514f5f9869e5abb1c08751",
      "6aeb77c440b049cf83cd802f12c461fe"
     ]
    },
    "id": "YcW5k7v9S1n_",
    "outputId": "1bf0db04-a57a-423f-f9e6-d4b6554d6fa3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3f2cb4eff54ab2b23bde9107611b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aebf8ec6f3f484f99ee2ce3881f2d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5786829e71842bb9f1d650a65ae3b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/233 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "\n",
    "en_text = \"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\"\n",
    "chinese_text = \"不要插手巫師的事務, 因為他們是微妙的, 很快就會發怒.\"\n",
    "\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\", src_lang=\"zh\")\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "JSdcWC-jS1iT"
   },
   "outputs": [],
   "source": [
    "encoded_zh = tokenizer(chinese_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rODCh_UQTL8Q"
   },
   "source": [
    "Let's start by taking the Chinese sentence and translate it back in to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Zl_OFXDZS1cW",
    "outputId": "53137286-64f4-4109-8cd3-f1c649f70b4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do not interfere with the matters of the witches, because they are delicate and will soon be angry.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = model.generate(**encoded_zh, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMZZ1A0bTiVv"
   },
   "source": [
    "Interesting and subtly different from our English original. Now we'll try translating the English to Chinese and then we'll take that Chinese output and translate it back into English.  This should give us an idea of how well the model works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "2aNZM7mc5oaF"
   },
   "outputs": [],
   "source": [
    "encoded_en = tokenizer(en_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "oTjxP4_f54dL",
    "outputId": "7692626b-08d2-4dff-c93c-3e764ac387d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['不要介入魔術師的事情,因為他們是微妙和快樂的憤怒。']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = model.generate(**encoded_en, forced_bos_token_id=tokenizer.get_lang_id(\"zh\"))\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PPPAaVgiCov"
   },
   "source": [
    "Now we'll store that Chinese output in a variable so we can translate back to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "o2NhYIx86YGI"
   },
   "outputs": [],
   "source": [
    "chinese_back_text = '不要介入魔術師的事情,因為他們是微妙和快樂的憤怒。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "dK3mhfip6mP5"
   },
   "outputs": [],
   "source": [
    "encoded_zhb = tokenizer(chinese_back_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t38gWo3U61i-",
    "outputId": "2861d413-dba1-49f6-e10b-13318462a52b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do not interfere with the things of the witches, because they are delicate and pleasant anger.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_tokens = model.generate(**encoded_zhb, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ry7b0T5mUD4f"
   },
   "source": [
    "Now you can see how far it has drifted as we have translated back and forth. With some care this approach can be used to generate novel content that can augment a training set (as long as the drift isn't too bad). This is what we call back translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yV10KZGq71y3"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'translationMetrics'></a>\n",
    "\n",
    "## 4. Machine Translation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rX6WLqInGqab"
   },
   "source": [
    "HuggingFace provides a library called evaluate that includes a large number of metrics.  We'll use two of them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BpfwtMkS3y4",
    "outputId": "ce1af30e-de7b-4caa-df8f-6edb8a263633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q evaluate\n",
    "import evaluate\n",
    "\n",
    "from datasets import DownloadConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StEFxpgHG8aJ"
   },
   "source": [
    "### 4.1 BLEU example\n",
    "\n",
    "The [BLEU metric](https://huggingface.co/spaces/evaluate-metric/bleu) has been around for awhile. Let's run an example of the scoring using the function provided by the evaluate library from HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Ht0IsazDIGsl"
   },
   "outputs": [],
   "source": [
    "#let's manually create some candidates and references\n",
    "#individual sentece example - this is best to experiment with.\n",
    "bleu_candidate = [\"the earth trembled in Japan again on Monday the 4th of September\"\n",
    "]\n",
    "\n",
    "bleu_reference = [[\"earthquakes hit Japan again on Monday September 4\"]\n",
    "]\n",
    "\n",
    "#multiple pairs of inputs and reference outputs\n",
    "bleu_candidates = [\"the earth trembled in Japan again on Monday the 4th of September\",\n",
    "                   \"earthquakes struck Japan again on Monday the 4th of September\"\n",
    "]\n",
    "bleu_references = [\n",
    "                   [\"earthquakes hit Japan again on Monday September 4\"],\n",
    "                   [\"On September 4th , a Monday , Japan had another earthquake\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7F4aj9TVEuX"
   },
   "source": [
    "Let's first try our individual candidate and reference.  They're both sort of saying the same thing.  Does the BLEU score reflect that similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151,
     "referenced_widgets": [
      "f9e343099df34e8b86e3b04ef3d9c4ca",
      "fc3c79f7945540a390f1260115f90449",
      "63e6be39d9c547759f81e1b076e435ef",
      "2a53c2d120854d7eb227b92df5367dfb",
      "a1cab619d0fb41a6a67b78a59ca73972",
      "50c9c5f5f0ff4317922773a3ff23f88c",
      "1f092318fad7423ab1df3bc7ca1c8afa",
      "ae606e9a68dc417b92d64b8b8fc04c05",
      "93cde1d1ef4c4c90af8d2450095fbc09",
      "1e5052a51f2e4ea7b7c0c11836233e35",
      "237b3a93c7464efca282e6f0660baf7e",
      "b968cd7ad846440d8176e8fa2cccad3f",
      "6e1dc72b34394583baa6d20190baea96",
      "cba0cb16b6054a86b183b831dcafefe4",
      "3eb42c09195744218e1a28bd1b179224",
      "1389780199844ec5991b9a386a0af7d4",
      "e10cb46365bc43919838494adcc97640",
      "d9e530db5e2648beaa5a174637112f4f",
      "012b9892715e49e9aa24ad3761615b79",
      "326bdc725a404c4abc2166dff1b251f2",
      "a57e1b9d1e154baca16232517750ffd9",
      "d4de3462df5a4503bdf1f1f79cef1353",
      "664e1e6b17014187bbe21a8f9ae051bf",
      "2d05ad31b5184e6c878b50db561cf59c",
      "a30b8c392678444994aace95dea6f074",
      "ee555886c5dd4920b0db3ba45d97635a",
      "7fabad5fbb5d4bc0b64a3e69542975b3",
      "68e05007ddbb4e9bb1fbd098c4293e00",
      "feedab45694a477790e87e7daf6aca9c",
      "211ae182aa1c4334a15935ebd46a5bd2",
      "b71408a1917746eeae8366b043bb6356",
      "3c8f37e16b1a4f8aae173eef2d42ba2a",
      "2b6680a756234d808b701fffa1555da3"
     ]
    },
    "id": "lac3w0JfBC3z",
    "outputId": "ad7a6c3f-04be-4d01-e602-6759330fdacb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e343099df34e8b86e3b04ef3d9c4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b968cd7ad846440d8176e8fa2cccad3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664e1e6b17014187bbe21a8f9ae051bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.22416933501922287, 'precisions': [0.4166666666666667, 0.2727272727272727, 0.2, 0.1111111111111111], 'brevity_penalty': 1.0, 'length_ratio': 1.5, 'translation_length': 12, 'reference_length': 8}\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=bleu_candidate, references=bleu_reference)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fgWde76LVYY-"
   },
   "source": [
    "BLEU is typically used in aggregate with multiple candidates as well as multiple reference examples for each sentence pair.  Here we run the candidates and references so you can see how it's done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VV9p8cCBrxw",
    "outputId": "6d5ea9d9-8e29-4bfc-9da1-1944bed47ba0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.14367696612929734, 'precisions': [0.4090909090909091, 0.15, 0.1111111111111111, 0.0625], 'brevity_penalty': 1.0, 'length_ratio': 1.1578947368421053, 'translation_length': 22, 'reference_length': 19}\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(predictions=bleu_candidates, references=bleu_references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7O1GyFUXJzJF"
   },
   "source": [
    "### 4.2 BERTScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRa04YrzBwOk"
   },
   "source": [
    "The BLEU score matches the actual word strings in your candidate translation to the word strings in the reference translation.  But what if your candidate says the same thing as the reference but simply uses different words to do so?  In that case your BLEU score may be zero because no words match but at a meaning level your candidate is actually a partial match.\n",
    "\n",
    "There's another way to measure, called BERTScore, that leverages the contextualized vectors geerated by a BERT model to makes the comparison between the candidate and the reference.  You can read about [BERTScore here](https://openreview.net/pdf?id=SkeHuCVFDr).\n",
    "\n",
    "BERTScore makes pairwise comparisons between the vectors for all of the words in the reference and the candidate using cosine similarity.  The result is a similarity score between the two sentences that takes in to account synonyms and also alternate orderings of words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovJGqaKFKOKn",
    "outputId": "5ede55ed-396a-4383-cac7-fd8a0934638f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lG6TYFi8G5hX"
   },
   "source": [
    "To see how it works, let's give the algorithm some data we create.  Let's make examples that use diffrent words but mean sort of the same thing.  We would expect these to produce a high score.  You can change the inputs below to see how it affects the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "de95e407b4f1401b8d1053bcde2fcb05",
      "887030e9a39e4c99ba496a73e0534fbc",
      "6c95cb8945514ab78f3a3d57fb608b38",
      "98a9ff947ed449f2a933570414de5d51",
      "458ad8c51e2a41ce91cb706b3dd0cea7",
      "3d7fa1a800704438bdff05f8b4446d24",
      "5d8bcccdd4334293a97e4de7ca627a11",
      "a8e80e4cf5024124a1cc693c11f9a96c",
      "930f45a7630c403aa460372ef424646d",
      "cc5352cbb1804db293e380a458a4697f",
      "79c91d2f342241bab16af40dad279608"
     ]
    },
    "id": "2WuMNg3LImS2",
    "outputId": "a7d12eed-4185-48d0-ff7b-a34afbb2583b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de95e407b4f1401b8d1053bcde2fcb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283,
     "referenced_widgets": [
      "ae0ff894f9de4e3b9478cea1200e5a52",
      "d043bda3069e4a7d961ff3deec17147c",
      "bff96e74573b473783c268651b067f19",
      "e4aaf34ac6644eecafac0e413d587670",
      "520aef836e31463589889062b62348a1",
      "053365b814ec4d42a431db0b830581a8",
      "da2ec27d64804d868a068c693ba22861",
      "fdc7d95e5636405da92851cf20f88d72",
      "471fe6a0f494496c9443e8f18d4f0f01",
      "b3dabe001210409a85d2ad0a5cb4ddae",
      "e2ba5d761ace4067b25403f20bfb2645",
      "7e72a477700d4d9b84a3926fa970084c",
      "8d79629c71a5462a9e0eb7eb547d7acc",
      "7b142609bd4b479299780f9ff261b8ff",
      "af99867a2d6649eda18384551cf09552",
      "8dc8225e1b774ce5a639d54e4b8ca68b",
      "470ca3fe10ae4ea8b07a2ea87bbe779c",
      "b39e14e67a8b4bbc9d291e00a87ece61",
      "feb008acb7754d59b9fcb3b27d241d5c",
      "246a55b597fa4d3dbf531484e38d0281",
      "5382e54330274ec5ae0c539a43f95139",
      "30b27dc3f5b2402c894bad17ad8d8e5f",
      "85cc9e61b94547dd8163ad7c80993beb",
      "f12ba4b03f44415bbf88359b0c9d6852",
      "cab2f9dbbc724befb382499f775a6677",
      "ab6209c2c5434a748eb9848608a248b3",
      "c6b82b84b27a47279e2e4b49722c0564",
      "de62b2eba657423fb53cddbb71f18abc",
      "00bdedc49f2b4a4da63353927de29144",
      "2236b6d5eb7247478ecfaa0c484b9796",
      "14a63266fa9f4ca8b41e75eab8452073",
      "2b5c0f2774d74e51ab9af65c0b6e0360",
      "cb54eb2140a7487c804372d1c7b7d35b",
      "17bd5b3b39b040f381f0100def31547d",
      "54acd5a3adc8437ea8512d257514c8d8",
      "26f33ab74b2c4e7c87e305bc72e77e6e",
      "7410e8f5be6c41648af3684f5c016328",
      "ac85cb6bbe594e668f06b1dacca55457",
      "34ebc914922e45e0beb940f5365ff22f",
      "e7048e078a81455a8e8de9ba04159efd",
      "37f178bbac274d1f98df20c2b6900975",
      "9d44030935734011b0323bcca1cdb03b",
      "a6fa43b3c08340bba859d1a81161724a",
      "aedcd9c9135345f59afb3524cd6b24b5",
      "1e81c4183744460680f3d01c44d020ec",
      "cf39fe51928d439ea2d847de46b1f1a6",
      "45d4022ed78d4b51aff55fb84f55b60c",
      "7f68d030c8af40758e7c1d534cb6d475",
      "3e532c394ccf421dad7142e6ddc2431c",
      "e85eb8a966a5463a8390839ab06b1db8",
      "54dadecafbb1413680739054b4fe52ff",
      "50327f63b11e440db304f5a7d19e6392",
      "84d9195df50e45c7bb460b9fa8dae297",
      "efe5cee759954125bfe2c66ca9b15a41",
      "a2037115e97c4829a38f12950c744cb7",
      "8a5f830a4bb74db79757d17583cd2cad",
      "c7b53ac7ddf54ac2a7bc4e8205a75de2",
      "17477f5567a241f5ac4a143677bbec51",
      "3b66681bbdda495d9bfede30c493a32d",
      "4c71f9da86da4500a8ad2ae452cebc5d",
      "d9092737bb6d4838b09344505c68b457",
      "dec884dfa11242b89e4b913d36546bbf",
      "ae69abbddbdd4e7bb0fe53513920f31a",
      "892782b5904e4fd2ba1aa6808d2b27f2",
      "8ce1bf327d7c484fba488fa23829caa4",
      "c148aca819c1434ead789763ec7bcf70"
     ]
    },
    "id": "KUDHqJyAJyT6",
    "outputId": "907027bf-5205-4919-839a-0f88caa6cff5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0ff894f9de4e3b9478cea1200e5a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e72a477700d4d9b84a3926fa970084c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cc9e61b94547dd8163ad7c80993beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bd5b3b39b040f381f0100def31547d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e81c4183744460680f3d01c44d020ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5f830a4bb74db79757d17583cd2cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': [0.9971846342086792, 0.9397112727165222], 'recall': [0.9971846342086792, 0.8711985349655151], 'f1': [0.9971846342086792, 0.904158890247345], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.52.4)'}\n"
     ]
    }
   ],
   "source": [
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hi there\", \"obie wan kenobi\"]\n",
    "results = bertscore.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "#results = bertscore.compute(predictions=predictions, references=references, lang=\"en\", model_type=\"distilbert-base-uncased\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ox8bq6QKMwME"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9T_clC29MxQW"
   },
   "source": [
    "### 4.3 Sentence Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KqRwJDLI4qJ"
   },
   "source": [
    "There's another way that you can compare sequences of text to see how similar they are.  We can use a model that generates a single vector to represent that sequence of text and then compare those vectors to see how \"similiar\" the two sentences are.  There's a library called [Sentence Transformers](https://sbert.net/docs/sentence_transformer/pretrained_models.html) that includes a variety of pretrained models that convert sequences of text into vectors.  You can choose among the models to find one that works well for your particular circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "c2c6e1ee73f24239b78ef7dae661a111",
      "2c1046a7e8f64ed981f044168912d323",
      "ac6bcd118f8a431a860d090c4a0e2f00",
      "00d2dd099c8e407893d86a47b9d13f1d",
      "5c2636c1a5a741e6aedd00178f23be4f",
      "417d2d8e617441358f80d719efb8f1ad",
      "a186b75675074dfbb1cb84e9b721fd98",
      "43edf4859fa4483bb3d9017fceaf064a",
      "d083ca95615c4b4eaf4c91354e22cf67",
      "6420ba6f25b5423eb8490e9bb6ba4ed8",
      "e750c72c8eea4bb4b852a35d42e65835",
      "513452bc55d64e8b84d6b9dda9e1b840",
      "b576b8877fd24ad2a68f8a5b775303ba",
      "f7899d659b3c4879b9c36f7fb2b96ccb",
      "a4233481b7974f4cbc13921ad42d67d0",
      "eb860921caf4492da07b06b57a272c4c",
      "0330de286550448c821557afb5bd24fa",
      "1f923cab35b745c2b0e3279ac171f8b4",
      "fb9fbc296cd24280af24b440b1b9ef79",
      "92b7eb621a6b4830939dca579e29200a",
      "69a16cfc9b77467b9429297da1512f98",
      "2e4f3393efcb4ff7ad224aa35ee38845",
      "81ed8550da514e14a2f20bbf5cbddeec",
      "21868de5d17d44b08c861f90b5c71526",
      "17729f4b145a4055ba0683df3252aa45",
      "b8110c8b1b834db5bdf91c1b6465dcae",
      "08a71e9841a84703b0eae3a67439a7f3",
      "53267c47805d4ce5b398bbe631d1cdbb",
      "53917b75f9184de0aa739c7aacb8835c",
      "8e3b216ef7b64fe298aa11525ad9ce63",
      "0ceef3394a8148e98f945c06a3d78577",
      "67f24c8d7109471487a06f2932d5239e",
      "4161f72f3ca2449482bfd301da384a46",
      "862fa82cdbaf4356966d572b1331962f",
      "e014f57584f24b2499a8175262483efb",
      "c576ce395ba64ca6a0ffb6b0eadc1010",
      "b42f73539e7c434a99656a5bf24717d4",
      "724c887a803640ac9d42efb15644113b",
      "c048bd198e1b4054adf45369c6d81f69",
      "bad5a08bf80642d38886b553b5af09e1",
      "5404b54a3adb417987a8f15299dbdfbf",
      "6c90d4b187964e36a14ec23244380d2b",
      "49ef1e7af1194bb9930cb5d35fec7082",
      "54f91f5588a64db4a57c097bb02daf94",
      "42751728f00345db9714ebbb4a035e29",
      "d168bc374e1f463b99ed2faa5c451caa",
      "83a8c5e43e014af1bef51878c08aface",
      "5197661ef9f342ce879c28a1badc1fe9",
      "a9ac7c9ecfcf4e1daeaa1169b4509b70",
      "aaf240b4d45f4711960aef5fd321b960",
      "18da28f2da904d3999086ad433ffd2af",
      "227b8f5d47cf4793a52f91635c9a5c1f",
      "f0638ad7bd39497a86edf0663a054e06",
      "fb0a78bff73146889f5b79579c902b0b",
      "1c7ea5f4525d46a6bc0a4c2c6fbddfad",
      "068fcae697cb42d9942accafdde1352b",
      "544a24c9f49549f49c4b5f0c5597eff4",
      "e10d4c2b2d86431d83816a709b08b270",
      "022080e6646542ba8458e56278f7db8d",
      "649dcf6f3232403d965aa0f5b0cad521",
      "761f211b3be142498742309bd4f98ccf",
      "d765bfcee0044de585591ab24de2e94e",
      "a241b80d8e0e4929aa39771615f37a24",
      "3cc54ad1b6ed4a0e96e2d90141225128",
      "f0b04bea4c3048f1944d071be64c7839",
      "8d78604d353c4698bc3e6049cd887195",
      "df3ee834557947ea9f6e65e27f902054",
      "5e16ea6f03514178a5c986655f5ef9d3",
      "85d5c4a602b648bf90c6854c6f232a6c",
      "347195d775d442269e458b3dffbba938",
      "27b0b4ed7b634b6080525c12743b05dc",
      "d4b68e3ca1eb4da2a85f6b226d028f7a",
      "a13c014995474e94b5b3cacf528cdc98",
      "025bdd6585254e3b9769b75a199db714",
      "1f7085a3eb7344cb8dd43ead9f2815ac",
      "85042ab4d18a477c958181eb00976255",
      "63684451dd1f48b493d66314f6d522a6",
      "c552ddea03174c78bd30e2c3a1db78bf",
      "2d586ee0d00e43ec9b14374447e6c1b6",
      "617c3b4ceef64d378734739172e2c3aa",
      "bb4570d9bebe4048821d51a376e49ed7",
      "4b1fa6bcf603404dba8d4364d6de4f33",
      "b2be47146fd248d69bef6b2bfc3211a7",
      "59072d348a884b14ae0f96c349e7a436",
      "e8c85760ddb64e70812caf425cc47dc7",
      "adc4c12736ae4a73b3f969ff00777be3",
      "1e35f1fc90974606ad80198e627315ea",
      "7c02beb8403d40b8805028eda80d90c9",
      "e19a7bf20ad7480db930219b53042607",
      "c957eb6a8be64a29a041eee5c80d6fa4",
      "6ffd9a2f5a5446bdabda55a83977fa56",
      "00e2b52cd8b9434c9bbdcabbe70e0b60",
      "0b0d6b4cda2c4b2b99a562d20cc92d82",
      "4db7b49596a34ee7884dcf669eae0e09",
      "9c61024b21404602ba6b49ba7b2329fe",
      "f3bf7ab5e0b04a5d89e72a6dc6b927e0",
      "4f18e2fd75484787b37e9cd2872d14df",
      "35689b8704bc48e19f9ac8a67ffc8d6a",
      "5ce6606e0bf74575ab411a60f805ea0c",
      "cb1604beecc4470a9183f3a03a008f8f",
      "16f1d11de2dc4d9ba4ff7218abcb05b1",
      "cb989f9204264c93ba45ccc3045b43d8",
      "81eecb3788be4a93a8afcad695f41c6c",
      "3f374fb0bd4549f0b2bfe669c8257525",
      "a87b26b94ebf49f1b18a4ac702589fba",
      "719a5f792b8d456bbd05f4b7771aaa0c",
      "912fb38827684da6866124f503bbe5fa",
      "2ee40470ae4c48759314f2912565dd5c",
      "d75e0ce2f8444d808ffb216b45ce843e",
      "01e97d67984640aaa0806c272859aaea",
      "554b4698d0c44a22a2a4db9a11530f93",
      "eacbb38337b74d6ebf941ae8c8bd05c6",
      "143beaf8eb8542099437af541af5a7a6",
      "9ebca7ecabfb4688b418404c68185be7",
      "059c0b8b9b7e445d8b2806eb44156516",
      "122b9d2e78564e3ea87dce7de2defbed",
      "2cf53dc014d144bd9eda5ebf6152d397",
      "906fdc5336244ec1b83317166df8b9a0",
      "ea76232f9019450bb34468d3577300b3",
      "c58a3c4c749d4eafb43587dd5d188ef3",
      "a1bbabb5015c416bbc99f5d31da8077b"
     ]
    },
    "id": "YaBPT42tNItW",
    "outputId": "2a88e8d3-ddb1-4ce3-d8ad-3c59c364be3e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c6e1ee73f24239b78ef7dae661a111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "513452bc55d64e8b84d6b9dda9e1b840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ed8550da514e14a2f20bbf5cbddeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862fa82cdbaf4356966d572b1331962f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42751728f00345db9714ebbb4a035e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068fcae697cb42d9942accafdde1352b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3ee834557947ea9f6e65e27f902054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c552ddea03174c78bd30e2c3a1db78bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19a7bf20ad7480db930219b53042607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1604beecc4470a9183f3a03a008f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554b4698d0c44a22a2a4db9a11530f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8mtAEpBNWIJ"
   },
   "source": [
    "Let's compare the vectors of several sentences. Two are topically similiar so should generate a higher but not very hgih score, while the third is unrelated and should therefore have a low score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t4GxtyhKMwxM",
    "outputId": "2f90ecb9-1ab1-4e38-edc7-7da9579fa33c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.6817, 0.0492],\n",
      "        [0.6817, 1.0000, 0.0421],\n",
      "        [0.0492, 0.0421, 1.0000]])\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode([\n",
    "    \"The weather is lovely today.\",\n",
    "    \"It's so sunny outside!\",\n",
    "    \"He drove to the stadium.\",\n",
    "])\n",
    "similarities = model.similarity(embeddings, embeddings)\n",
    "print(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tBMNCYYIGQP"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'subwordModels'></a>\n",
    "\n",
    "## 5. Subword Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2mwUhWVbF3XZ"
   },
   "source": [
    "Different pretrained models use different subword models.  Each subword model identifies a different set of \"tokens\" based on an efficient representation of words and parts of words in the pre-training corpus.  The model has an embedding for each one of the subwords in its vocabulary.\n",
    "\n",
    "We do not typically interact directly with the subword models but rather do so indirectly through the Tokenizer object.\n",
    "\n",
    "Let's try the BERT base cased tokenizer.  It uses a wordpiece subword model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163,
     "referenced_widgets": [
      "1d3f2e79b66f444fa4e4b313d9e07f3b",
      "c9851bb8563842799c07dc4888160fcb",
      "f6eaa5e9dcbf4f0fb19196a03a216aa3",
      "d90a33f89eb34ccfb7620d690ceeb5b3",
      "0f627ca117154a0dafd6bf1795dca229",
      "9cde899e63df45cc839bd5ac783a91a6",
      "d1df4f4c808a46f8bee6f0a029ba56d2",
      "b091ae982bfd49f7b0b6bca662286ae6",
      "b6bf11aa17bd46e59ea896ca83dfbb8f",
      "01edd02bc4264279bb2a369cfcd92863",
      "c631c098fcca419180b66ff5ebb29a1d",
      "d11421d1e0514481b17f636308d39fbc",
      "0c338733d7d243df92200b2318489953",
      "6979f42116c74b2d96f46ce28a21db8d",
      "86c0c121c44544fea1eb86ea881d1194",
      "a97c4e14396a456db1a28d0cb6188392",
      "f4ec42add805403b9b847800aaf7d34d",
      "cfcd0c0f67d3429cb76924cd9f0ca96f",
      "6abde6884b754ed1a0d6f273333cfda3",
      "2ba0458db3c149238ca73313f9ced57b",
      "14e442676d174d26b34eaab4f83ae12e",
      "1a1a6d79540340d9afa68dbb08163d4a",
      "711975e6f13e46b5a8e8268c09868929",
      "260b2dec4f9349758f9dc215070fcb7d",
      "b98c1e77938c48178174a537ae693eda",
      "b6299e04284e47a4ae703264a4e875c4",
      "fa4e5986d88f430b9abecb9ec23d47da",
      "68b93cb3242547a1acbde78b4cf96f9b",
      "6f3a58ca7fbf4faeb996ff0fc386744d",
      "a7e55b2fcac64c7bb1893e2e718822f7",
      "55fae5b8d1b24087b87790507df9f762",
      "c529bae6af5e4394944dd8268bce8436",
      "7b106d5ee7a24fa2ae8c3773b8ca67af",
      "45cebcdb0e664386a55232d94b755aa7",
      "7def5d9e917243698f0d66769aa1e26e",
      "18cc65537ae54c46ab4b586c96fca3fc",
      "9a346f06661642da8626db25f09f4db1",
      "f249b45c0e984195be41875a57bd309f",
      "441712c6aa624f33b1c59a99b87f4ee3",
      "e780c786097c4342b0be8e9523d581d1",
      "3f43d761f6924cdfbc6771febc672b2b",
      "a5a6d13dee924b64bf276e7d4f91e8b2",
      "ca9e0ffe8344457490d444853845249b",
      "810ce4e7dc4c40a8a9fbf2a352436b9a"
     ]
    },
    "id": "KOF2b8XGIGHt",
    "outputId": "fb299ee9-7bca-4ec5-a546-6ee5471e6ed0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3f2e79b66f444fa4e4b313d9e07f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11421d1e0514481b17f636308d39fbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711975e6f13e46b5a8e8268c09868929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cebcdb0e664386a55232d94b755aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size is 28996\n"
     ]
    }
   ],
   "source": [
    "#wordpiece\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "print(f'The vocabulary size is {tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5BdFwPGpIdrZ",
    "outputId": "a2fe0e5c-67b6-45fa-940f-7ebe0297806b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'you',\n",
       " 'love',\n",
       " '[UNK]',\n",
       " 'Transformers',\n",
       " '?',\n",
       " 'We',\n",
       " 'sure',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Don't you love 🤗 Transformers? We sure do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QrOt7kczgxF"
   },
   "source": [
    "This is the same tokenizer code but instead it is loaded with the multilingual model version. Note that it contains many more tokens than BERT base because it has to be able to deal with multiple kinds of symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163,
     "referenced_widgets": [
      "1dd1999dddc34518beca2a00329ee59d",
      "877575e678bb45cc9f7a02ff51a12915",
      "9462105174e34bfd9e2c1d0c16174e5b",
      "f91e5bfc3e494d9a86c413460a6c52b3",
      "f22b35ad09f047378b8df62f9044f399",
      "8fb41ffb9823451789aa8697e12a643c",
      "9323f17f01ac45ea8d469139cb63ddbe",
      "8827970f3cc84b73bcbb641ed4bff852",
      "1e674fe0549f4fd9b70c175f5fd80413",
      "601ec5a1db164c13b2f7ed847676ab3e",
      "5002b1d984e1490b81ab1c7d26e03afa",
      "7fb2ccc4bec84b5eb6aaa6a8dd9c965c",
      "f81ca8a86d7f448a88eabe1a50bccc24",
      "d99f1138e10244f88df2a3b7796ce023",
      "d29ac018455c49f2815ca1f6bb483b13",
      "a34ae84206dc4378835392df58e162ac",
      "bcaf81dd14924d04b203330745e5760c",
      "1a18d4e3f2c0419fb362b8f55b0ddf9f",
      "ae2dc1eb21f3459e8544719d208b2122",
      "249bbe3e1576425981e699610b390c9b",
      "5b66db74086040ac82f98a6cdac82e22",
      "c3c6550afa014f35ba4d14be117d97a2",
      "9fafc58e4f484f378e1efa0bdd5a67f4",
      "e178cbd2cff2412aae5b913ef0614865",
      "252e535f48e449e7ba8f2dbc372554da",
      "8d1d6da135874443b1e0acbe3ce9b834",
      "797fa49e4a43450d86bc850a1899eaee",
      "2ac060dff31c45fa973d620808d81c1d",
      "5ed6bac0751e452893d1b503b34b241e",
      "b3dd9eeef7624f0ca70ced31911e46ef",
      "6023a56a7e594286a8762689e67072ed",
      "0c66d42b84a64617b9193b055c4a75cb",
      "3fc5fb2ce03e441aadf08baa31663716",
      "fce3e55f5a3a45169c62ed37cabfc9a9",
      "51ac3fb3b1c14ee4bbd4db4799f0880d",
      "f10aee4a8b4e431fa02f0ad5909797e0",
      "f9f36b7797854fc3baf27e56be128e2f",
      "a12a25c8a53a438b95d338b3b4896890",
      "2993a87e9005485b8a35cf547515741d",
      "faab8ea555f24dd48e1e2c98c7ac5343",
      "d161b7602d174c5c9d4706182a96e755",
      "84e3676eddc04bb4bbe4b02d15d0a317",
      "08414950c8874037b7d7bd59b652c35b",
      "b06b1fec29cf4e6d83811627829c70a3"
     ]
    },
    "id": "-cJGyulLjigO",
    "outputId": "e8285c28-22e0-47b9-c921-15f34a778221"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd1999dddc34518beca2a00329ee59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb2ccc4bec84b5eb6aaa6a8dd9c965c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fafc58e4f484f378e1efa0bdd5a67f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce3e55f5a3a45169c62ed37cabfc9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size is 105879\n"
     ]
    }
   ],
   "source": [
    "#wordpiece\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "print(f'The vocabulary size is {tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0TwEchTqIloh",
    "outputId": "3b88702d-6cd0-4563-e7fe-318d1a0fae57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['你',\n",
       " '不',\n",
       " '喜',\n",
       " '欢',\n",
       " '[UNK]',\n",
       " '变',\n",
       " '形',\n",
       " '金',\n",
       " '刚',\n",
       " '吗',\n",
       " '？',\n",
       " '我',\n",
       " '们',\n",
       " '肯',\n",
       " '定',\n",
       " '会',\n",
       " '。']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"你不喜欢🤗变形金刚吗？ 我们肯定会。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7_tURiwSo2z"
   },
   "source": [
    "Let's put that first English sentence through the multilingual tokenizer.  It produces the same subwords for English even though it can also handle other languages as shown by it's much lager vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m_U47UUdShSU",
    "outputId": "bed20249-4efa-4815-80a3-cd5016ec34cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'you',\n",
       " 'love',\n",
       " '[UNK]',\n",
       " 'transformers',\n",
       " '?',\n",
       " 'we',\n",
       " 'sure',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Don't you love 🤗 Transformers? We sure do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClLjXFopJJk7"
   },
   "source": [
    "T5 uses the sentencepiece subword model.  Here we'll use the tokenizer for the multilingual version of T5 called mt5.  Notice the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163,
     "referenced_widgets": [
      "a114689776324e5dbf40d92ddf2368df",
      "3d603bf22aaf4642a7e647848a8d8595",
      "d07db28cbd8c445c968eca4c02b9c2f6",
      "964331366c6b471784dd5d4bbdb79ca2",
      "a519d1a75d164910a84546cae37f74ec",
      "694ebe73cf2047ca96081848d67f8d95",
      "f160c25022664db4886daff8b9f86a83",
      "0e29d3933fbc4d2f8db260b54731bfc0",
      "45d159ed0e7641d380b0888991d04e42",
      "77d244fed29e49f599c7a89b8a1e96e3",
      "6388297feda64a6ba7dfe6770bb9097b",
      "bb9e3464c15b45efba7bd04b663b325d",
      "c62900601d954b58b2ca24b67245be1a",
      "a701109aa5724832a42cd6e08eda5e64",
      "7997d2815b06412baa0c338a1c2a2159",
      "fc7f29c47d0540bc83ddcc6716d7e4a3",
      "72c84d41675c4277ad846089f81f2d47",
      "461bb5e9f8c74ba49e4828a98737727a",
      "a321afbd258148fd8946e8facda65ae8",
      "ea66bae439534656b7f198cd1fc75e4c",
      "b824a35ed20e4304a2aed1c88c6e00ed",
      "d263f43cb37d4f8dbd2f54d103ab0119",
      "b4016b21830c41c59523408e23c5bfd6",
      "10c98ea122044dd6948d427cd724b070",
      "93028dc73f034282bb78436e4bd6c425",
      "353e028c5bfb47a3827f8d669177c2a4",
      "c51f4fafcbf5400c9089dad744b2f4dd",
      "ada00d0e9e81433890b16dfb49acb807",
      "71cc251c8ed54759ac5b05d8b855edf2",
      "84d901946fb1436ea46bd369dd8bd503",
      "70ff9ba91d5349d18f4e2c1546a7f8e3",
      "0ed2f9fe258548c3868e26fc026b216b",
      "80f51f8bc47d40778e5fa1fec172d58d",
      "4732cae4327a4744855478200065ca55",
      "94454565fd58489689c74cb4ad259525",
      "266aa5d6443c40cfa58e9804c161ac6d",
      "efe4aec1b7154887b8e3a1d0b9c96f3b",
      "3d9fd474a5f743bb825055b15bdcc877",
      "34936ab3be2a4fd191cb8ac6b38a9388",
      "fb34be92bab649bba3f1c187f8760c9b",
      "0dd9d2d89e37424bb9063d7a7f1a451b",
      "b99b9b2a32864d9eac8e01e4b3465dca",
      "6fe78c03e7254e79bdee053f2e23e0e6",
      "3e383d00c8f74f16b73980ca5062ac3e"
     ]
    },
    "id": "-4PgauEWIF3R",
    "outputId": "9618a929-3565-467e-ae3d-af348018deed"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a114689776324e5dbf40d92ddf2368df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9e3464c15b45efba7bd04b663b325d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4016b21830c41c59523408e23c5bfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4732cae4327a4744855478200065ca55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size is 250100\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "print(f'The vocabulary size is {tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EzWJuzCVJV-l",
    "outputId": "f5f1882c-eade-4343-80b6-73dd438e1ddb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Don',\n",
       " \"'\",\n",
       " 't',\n",
       " '▁you',\n",
       " '▁love',\n",
       " '▁',\n",
       " '🤗',\n",
       " '▁',\n",
       " 'Transformers',\n",
       " '?',\n",
       " '▁We',\n",
       " '▁sure',\n",
       " '▁do',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Don't you love 🤗 Transformers? We sure do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNLYqA07OTa5"
   },
   "source": [
    "The sentencepiece subword model includes a marker to indicate if a subword is at the begining of a word and thus, in English, is preceeded by a space.  This means that with sentence piece it is possible to accurately reconstruct the sentence because we explicitly identify the word boundaries.\n",
    "\n",
    "\n",
    "Finally, let's look at **GPT2** which uses the BytePair Encoding subword model.  Its output will be completely different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195,
     "referenced_widgets": [
      "078550d594584752a3bb52fc0b6e1d4d",
      "f9f75c06bff146fb8e99ce544f5c2659",
      "58dac49adc0f474280e9b4912997598d",
      "9ea89cf767a947c99e5f2f6e568bbf88",
      "a31353534f664842aa35bf1d189e780a",
      "b6e026183c814857944bc25a4b6c2e9f",
      "6ba46a8549544fb69ba2bb5c0459989d",
      "dfdc2ea16db447debe0c8a727c8b4417",
      "3a478f6558a74176bdce0cd3a61dd4bb",
      "95c6c015414945e980d74d139ebfa067",
      "2962caa0db6146daa95f26f6c14a9135",
      "c2af2d915e6a425180d998d04b5edc61",
      "e51c5cfccfee43a5ac2edfe8ea3a4265",
      "4871eb3753e84911b1b1d69841908343",
      "37abc356cacc4798aeabacb990983d76",
      "a1d11363a0e4461daad42d135951a54a",
      "ffc9514c273d4826995507bc2df17fb5",
      "5a501613f50244a4a5b4bfbc63e1a2f4",
      "45925fbad7ca4735969e5ea7786f63a1",
      "e7c7e83d63e6469b938e3086551c2491",
      "74b6ba534358416fb722e4a79e200498",
      "e4c5b2aa4e294df6b00baa4e4e4e089b",
      "2a58dae4590b44ca8de7d8c3124fe2b6",
      "e8d5875089b947ee8f4474c916d3e1d7",
      "ab95a6dcf38b49bb8f53b5f61181938b",
      "61035012b56e474eb8376c237d4d2d2d",
      "5f8ecaec82994effbc7395ad71b1d41a",
      "e4f4647b98274ed5ae515d91df11daab",
      "611ee3f7b0fc40439460134731b5ec26",
      "16c953d677a74600b093ca833daabb26",
      "18a5963d6e75438a8b51f22891de3248",
      "0a5d735460354cbfadb8c50f6d2fdd23",
      "c3edeca9cfd2493b9854114f76db4cbf",
      "827d35e7171c4534a0f4bda521a8cf37",
      "222094f0e5e74ff8a6f5d1cfa9dac60c",
      "99f0b99533b948c68989affc20e1aa0a",
      "7dcb222f7dff4608ad43eb19a286379f",
      "fe8351a38f8b4e778329fff07ee7e9c8",
      "bddc5ffc85ab4d10992fa5b9c5a4701b",
      "6cc709e9f4d8442a8b7bb6565e1177da",
      "8971d5dd47b94d7f846ecc2a34a9408a",
      "26cd6111bdbe48c29fd8bf856695b863",
      "fd22a154d05347c3a5bf8abc0b8ca0de",
      "af28ae35eccb4affb246a3cb41eb384d",
      "180ac2412a8f40b4b456b39ff6aa74c7",
      "5027d77cc8ca48959aed6152a57cbbe6",
      "c23038f82fad4ddbbade2828c5c92e07",
      "db5cd6cdbd8a4f2eb7526141596b0f6e",
      "37d35f0be3634d8ba9195260f71a61dc",
      "90a72ec14f7949999410422bee8d8071",
      "78a026bfb0e341dbb58b832c75efb5d5",
      "c45dfa27654b48cca542496537e6fa1b",
      "99befa3c65834bba90b6f78d20624e88",
      "a3a36789f990490da4bed764e7e6f3f5",
      "ad754ab43e4945b79cdd792d1d856d30"
     ]
    },
    "id": "yzJHQZECIFmx",
    "outputId": "3efd9ff8-6ce3-40dd-ae63-13f1b13b7509"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078550d594584752a3bb52fc0b6e1d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2af2d915e6a425180d998d04b5edc61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a58dae4590b44ca8de7d8c3124fe2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827d35e7171c4534a0f4bda521a8cf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180ac2412a8f40b4b456b39ff6aa74c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size is 50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(f'The vocabulary size is {tokenizer.vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taDZOyw7S1Fn",
    "outputId": "1fe87c93-2143-49c8-80f5-cd3d6feeb2d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Don',\n",
       " \"'t\",\n",
       " 'Ġyou',\n",
       " 'Ġlove',\n",
       " 'ĠðŁ',\n",
       " '¤',\n",
       " 'Ĺ',\n",
       " 'ĠTransformers',\n",
       " '?',\n",
       " 'ĠWe',\n",
       " 'Ġsure',\n",
       " 'Ġdo',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"Don't you love 🤗 Transformers? We sure do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9p7Vi63LTm2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "L4Rp0LwtLVM1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-g7iBYT2U64I"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'answers'></a>\n",
    "\n",
    "## ANSWERS\n",
    "\n",
    "1.  The T5 model doesn't have the token type ids that BERT uses to identify different segments.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
