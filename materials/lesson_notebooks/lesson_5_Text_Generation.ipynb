{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CJVpUxEKnCn"
   },
   "source": [
    "# Lesson Notebook 5: Text Generation\n",
    "\n",
    "In this notebook we will look at 3 different examples:\n",
    "\n",
    "1. Building a Seq2Seq model for machine translation using RNNs with and without Attention\n",
    "\n",
    "2. Playing with T5 for summarization and translation\n",
    "\n",
    "3. Exercise with prompts and language generation using the various models. Note that in this section it is necessary to stop and disconnect the notebook and then restart to run the specific model.  This is because the T4 GPU has 15 gigabytes of RAM and models like Qwen 3 cannot be loaded with others because of their size.\n",
    "\n",
    "The sequence to sequence architecture is inspired by the Keras Tutorial https://keras.io/examples/nlp/lstm_seq2seq/.\n",
    "\n",
    "\n",
    "<a id = 'returnToTop'></a>\n",
    "\n",
    "## Notebook Contents\n",
    "  * 1. [Setup](#setup)\n",
    "  * 2. [Seq2Seq Model](#encoderDecoder)\n",
    "    * 2.1 [Data Acquisition](#dataAcquisition)\n",
    "    * 2.2 [Seq2Seq without Attention](#s2sNoAttention)\n",
    "    * 2.3 [Seq2Seq with Attention](#s2sAttention)\n",
    "  * 3. [T5](#t5Example)\n",
    "    * 3.1 [Tokenization](#tokenization)\n",
    "    * 3.2 [Model Structure & Output](#modelOutput)\n",
    "  * 4. [Prompt Engineering and Generative Large Language Models](#prompts)\n",
    "    * 4.1 [Cloze Prompts](#clozeExample)\n",
    "    * 4.2 [Prefix Prompts](#prefixExample)\n",
    "    * 4.3 [Instruction Tuned Prompts](#llama3)\n",
    "    * 4.4 [Chat GPT](#chatgpt)\n",
    "    * 4.5 [Class Exercise](#classExercise)\n",
    "  * 5. [Answers](#answers)      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/datasci-w266/2025-summer-main/blob/master/materials/lesson_notebooks/lesson_5_Text_Generation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tsSyq1WjYsO"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'setup'></a>\n",
    "\n",
    "## 1. Setup\n",
    "\n",
    "We first need to do the usual setup. We will also use some nltk and sklearn components in order to tokenize the text.\n",
    "\n",
    "This notebook requires the tensorflow dataset and other prerequisites that you must download and then store locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKdHlKOSKkgM"
   },
   "outputs": [],
   "source": [
    "#@title Installs\n",
    "\n",
    "!pip install pydot --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install sentencepiece --quiet\n",
    "!pip install nltk --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgTghneFvZqd"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import sklearn as sk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration\n",
    "from transformers import GPT2Tokenizer, TFOPTForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhvbHp3Gq7CM",
    "outputId": "c81a2abc-7c73-4b18-d9a0-e455474fa747"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SM3-mYDnnxyT"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'encoderDecoder'></a>\n",
    "\n",
    "\n",
    "## 2. Building a Seq2Seq model for Translation using RNNs with and without Attention\n",
    "\n",
    "### 2.1 Downloading and pre-processing Data\n",
    "\n",
    "\n",
    "Let's get the data. Just like the Keras tutorial, we will use http://www.manythings.org as the source for the parallel corpus, but we will use German.  Machine translation requires sentence pairs for training, that is individual sentences in German and the corresponding sentence in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EJgys8FQva6V",
    "outputId": "b6d4441d-4ddb-4f67-ea41-221ddd693a6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Archive:  deu-eng.zip',\n",
       " '  inflating: deu.txt                 ',\n",
       " '  inflating: _about.txt              ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!curl -O http://www.manythings.org/anki/deu-eng.zip\n",
    "!!unzip deu-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fGnUBkZQteK"
   },
   "source": [
    "Next, we need to set a few parameters.  Note these numbers are much smaller than we would set in a real world system.  For example, vocabulary sizes of 2000 and 3000 are unrealistic unless we were dealing with a highly specialized domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWyF80SsnP-M"
   },
   "outputs": [],
   "source": [
    "embed_dim = 100  # Embedding dimensions for vectors and LSTMs.\n",
    "num_samples = 10000  # Number of examples to consider.\n",
    "\n",
    "# Path to the data txt file on disk.\n",
    "data_path = \"deu.txt\"\n",
    "\n",
    "# Vocabulary sizes that we'll use:\n",
    "english_vocab_size = 2000\n",
    "german_vocab_size = 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgRfFYZsRGmn"
   },
   "source": [
    "Next, we need to format the input. In particular we would like to use nltk to help with the tokenization. We will then use sklearn's CountVectorizer to create a vocabulary from the most frequent words in each language.\n",
    "\n",
    "(Before, we used pre-trained word embeddings from Word2Vec that came with a defined vocabulary. This time, we'll start from scratch, and need to extract the vocabulary from the training text.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSTzz6VAtlqv",
    "outputId": "4cb754de-de5b-4d7d-871e-eb618a5598a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum source input length:  6\n",
      "Maximum target output length:  10\n"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "\n",
    "max_input_length = -1\n",
    "max_output_length = -1\n",
    "\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "\n",
    "    tokenized_source_text = nltk.word_tokenize(input_text, language='english')\n",
    "    tokenized_target_text = nltk.word_tokenize(target_text, language='german')\n",
    "\n",
    "    if len(tokenized_source_text) > max_input_length:\n",
    "      max_input_length = len(tokenized_source_text)\n",
    "\n",
    "    if len(tokenized_target_text) > max_output_length:\n",
    "      max_output_length = len(tokenized_target_text)\n",
    "\n",
    "\n",
    "    source_text = (' '.join(tokenized_source_text)).lower()\n",
    "    target_text = (' '.join(tokenized_target_text)).lower()\n",
    "\n",
    "    input_texts.append(source_text)\n",
    "    target_texts.append(target_text)\n",
    "\n",
    "vectorizer_english = CountVectorizer(max_features=english_vocab_size)\n",
    "vectorizer_english.fit(input_texts)\n",
    "vocab_english = vectorizer_english.get_feature_names_out()\n",
    "\n",
    "vectorizer_german = CountVectorizer(max_features=german_vocab_size)\n",
    "vectorizer_german.fit(target_texts)\n",
    "vocab_german = vectorizer_german.get_feature_names_out()\n",
    "\n",
    "print('Maximum source input length: ', max_input_length)\n",
    "print('Maximum target output length: ', max_output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQwEsJh7R8Yd",
    "outputId": "25e67029-a6bb-40e6-a2f3-d1ca177ad0d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go .', 'hi .']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Uz463u9GR_wO",
    "outputId": "61acda64-c00f-43e8-d141-d48ddd8a8c6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geh .', 'hallo !']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIAaknDaRasy"
   },
   "source": [
    "Looks simple but correct.\n",
    "\n",
    "So the source and target sequences have max lengths 6 and 11, respectively. As we will add start and end tokens (\\<s> and \\</s>) to our decoder side we will set the respective max lengths to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDdzDfvQ9EO1"
   },
   "outputs": [],
   "source": [
    "max_encoder_seq_length = 6\n",
    "max_decoder_seq_length = 13 #11 + start + end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtF9ihIeRuTO"
   },
   "source": [
    "Next, we create the dictionaries translating between integer ids and tokens for both source (English) and target (German)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8JiJ9pL3AnU"
   },
   "outputs": [],
   "source": [
    "source_id_vocab_dict = {}\n",
    "source_vocab_id_dict = {}\n",
    "\n",
    "for sid, svocab in enumerate(vocab_english):\n",
    "  source_id_vocab_dict[sid] = svocab\n",
    "  source_vocab_id_dict[svocab] = sid\n",
    "\n",
    "source_id_vocab_dict[english_vocab_size] = \"<unk>\"\n",
    "source_id_vocab_dict[english_vocab_size + 1] = \"<pad>\"\n",
    "\n",
    "source_vocab_id_dict[\"<unk>\"] = english_vocab_size\n",
    "source_vocab_id_dict[\"<pad>\"] = english_vocab_size + 1\n",
    "\n",
    "target_id_vocab_dict = {}\n",
    "target_vocab_id_dict = {}\n",
    "\n",
    "for tid, tvocab in enumerate(vocab_german):\n",
    "  target_id_vocab_dict[tid] = tvocab\n",
    "  target_vocab_id_dict[tvocab] = tid\n",
    "\n",
    "# Add unknown token plus start and end tokens to target language\n",
    "\n",
    "target_id_vocab_dict[german_vocab_size] = \"<unk>\"\n",
    "target_id_vocab_dict[german_vocab_size + 1] = \"<start>\"\n",
    "target_id_vocab_dict[german_vocab_size + 2] = \"<end>\"\n",
    "target_id_vocab_dict[german_vocab_size + 3] = \"<pad>\"\n",
    "\n",
    "target_vocab_id_dict[\"<unk>\"] = german_vocab_size\n",
    "target_vocab_id_dict[\"<start>\"] = german_vocab_size + 1\n",
    "target_vocab_id_dict[\"<end>\"] = german_vocab_size + 2\n",
    "target_vocab_id_dict[\"<pad>\"] = german_vocab_size + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKY10hN1SLui"
   },
   "source": [
    "Lastly, we need to create the training and test data that will feed into our two models. It is convenient to define a small function for that that also takes care off padding and adding start/end tokens on the decoder side.\n",
    "\n",
    "Notice that we need to create three sequences of vocab ids: inputs to the encoder (starting language), inputs to the decoder (output language, for the preceding tokens in the output sequence) and labels for the decoder (the correct next word to predict at each time step in the output, which is shifted one over from the inputs to the decoder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DWf2bb905SW9"
   },
   "outputs": [],
   "source": [
    "def convert_text_to_data(texts,\n",
    "                         vocab_id_dict,\n",
    "                         max_length=20,\n",
    "                         type=None,\n",
    "                         train_test_vector=None,\n",
    "                         samples=100000):\n",
    "\n",
    "  if type == None:\n",
    "    raise ValueError('\\'type\\' is not defined. Please choose from: input_source, input_target, output_target.')\n",
    "\n",
    "  train_data = []\n",
    "  test_data = []\n",
    "\n",
    "  for text_num, text in enumerate(texts[:samples]):\n",
    "\n",
    "    sentence_ids = []\n",
    "\n",
    "    for token in text.split():\n",
    "\n",
    "      if token in vocab_id_dict.keys():\n",
    "        sentence_ids.append(vocab_id_dict[token])\n",
    "      else:\n",
    "        sentence_ids.append(vocab_id_dict[\"<unk>\"])\n",
    "\n",
    "    vocab_size = len(vocab_id_dict.keys())\n",
    "\n",
    "    # Depending on encoder/decoder and input/output, add start/end tokens.\n",
    "    # Then add padding.\n",
    "\n",
    "    if type == 'input_source':\n",
    "      ids = (sentence_ids + [vocab_size - 1] * max_length)[:max_length]\n",
    "\n",
    "    elif type == 'input_target':\n",
    "      ids = ([vocab_size -3] + sentence_ids + [vocab_size - 2] + [vocab_size - 1] * max_length)[:max_length]\n",
    "\n",
    "    elif type == 'output_target':\n",
    "      ids = (sentence_ids + [vocab_size - 2] + [vocab_size -1] * max_length)[:max_length]\n",
    "\n",
    "    if train_test_vector is not None and not train_test_vector[text_num]:\n",
    "      test_data.append(ids)\n",
    "    else:\n",
    "      train_data.append(ids)\n",
    "\n",
    "\n",
    "  return np.array(train_data), np.array(test_data)\n",
    "\n",
    "\n",
    "train_test_split_vector = (np.random.uniform(size=10000) > 0.2)\n",
    "\n",
    "train_source_input_data, test_source_input_data = convert_text_to_data(input_texts,\n",
    "                                                                       source_vocab_id_dict,\n",
    "                                                                       type='input_source',\n",
    "                                                                       max_length=max_encoder_seq_length,\n",
    "                                                                       train_test_vector=train_test_split_vector)\n",
    "\n",
    "train_target_input_data, test_target_input_data = convert_text_to_data(target_texts,\n",
    "                                                                       target_vocab_id_dict,\n",
    "                                                                       type='input_target',\n",
    "                                                                       max_length=max_decoder_seq_length,\n",
    "                                                                       train_test_vector=train_test_split_vector)\n",
    "\n",
    "train_target_output_data, test_target_output_data = convert_text_to_data(target_texts,\n",
    "                                                                         target_vocab_id_dict,\n",
    "                                                                         type='output_target',\n",
    "                                                                         max_length=max_decoder_seq_length,\n",
    "                                                                         train_test_vector=train_test_split_vector)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwsJeSJVWbnI"
   },
   "source": [
    "Let us look at a few examples. They appear coorect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YB5WqPpYV_EQ",
    "outputId": "faaa11bb-3f6b-4223-cff5-12d018e8d9a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 697, 2000, 2001, 2001, 2001, 2001],\n",
       "       [ 790, 2000, 2001, 2001, 2001, 2001]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_source_input_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N103xCPpWBx7",
    "outputId": "d4189581-c596-4e70-c892-4c7169c9476a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3001,  785, 3000, 3002, 3003, 3003, 3003, 3003, 3003, 3003, 3003,\n",
       "        3003, 3003],\n",
       "       [3001, 1048, 3000, 3002, 3003, 3003, 3003, 3003, 3003, 3003, 3003,\n",
       "        3003, 3003]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_input_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7Wcc1bFWB5c",
    "outputId": "8c19e7ab-588b-4513-9e5f-395cb51c1f3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 785, 3000, 3002, 3003, 3003, 3003, 3003, 3003, 3003, 3003, 3003,\n",
       "        3003, 3003],\n",
       "       [1048, 3000, 3002, 3003, 3003, 3003, 3003, 3003, 3003, 3003, 3003,\n",
       "        3003, 3003]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_output_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMTL8pWhSxjc"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 's2sNoAttention'></a>\n",
    "\n",
    "### 2.2 The Seq2seq model without Attention\n",
    "\n",
    "We need to build both the encoder and the decoder and we'll use LSTMs.  We'll set up the system first without an attention layer between the encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UK9eP5bC6jCL"
   },
   "outputs": [],
   "source": [
    "def create_translation_model_no_att(encode_vocab_size, decode_vocab_size, embed_dim):\n",
    "\n",
    "    source_input_no_att = tf.keras.layers.Input(shape=(max_encoder_seq_length,),\n",
    "                                                dtype='int64',\n",
    "                                                name='source_input_no_att')\n",
    "    target_input_no_att = tf.keras.layers.Input(shape=(max_decoder_seq_length,),\n",
    "                                                dtype='int64',\n",
    "                                                name='target_input_no_att')\n",
    "\n",
    "    source_embedding_layer_no_att = tf.keras.layers.Embedding(input_dim=encode_vocab_size,\n",
    "                                                              output_dim=embed_dim,\n",
    "                                                              name='source_embedding_layer_no_att')\n",
    "\n",
    "    target_embedding_layer_no_att  = tf.keras.layers.Embedding(input_dim=decode_vocab_size,\n",
    "                                                               output_dim=embed_dim,\n",
    "                                                               name='target_embedding_layer_no_att')\n",
    "\n",
    "    source_embeddings_no_att = source_embedding_layer_no_att(source_input_no_att)\n",
    "    target_embeddings_no_att = target_embedding_layer_no_att(target_input_no_att)\n",
    "\n",
    "    encoder_lstm_layer_no_att = tf.keras.layers.LSTM(embed_dim, return_sequences=True, return_state=True, name='encoder_lstm_layer_no_att')\n",
    "    encoder_out_no_att, encoder_state_h_no_att, encoder_state_c_no_att = encoder_lstm_layer_no_att(source_embeddings_no_att)\n",
    "\n",
    "    decoder_lstm_layer_no_att = tf.keras.layers.LSTM(embed_dim, return_sequences=True, return_state=False, name='decoder_lstm_layer_no_att')\n",
    "    decoder_lstm_out_no_att = decoder_lstm_layer_no_att(target_embeddings_no_att, [encoder_state_h_no_att, encoder_state_c_no_att])\n",
    "\n",
    "    target_classification_no_att = tf.keras.layers.Dense(decode_vocab_size,\n",
    "                                                         activation='softmax',\n",
    "                                                         name='classification_no_att')(decoder_lstm_out_no_att)\n",
    "\n",
    "    translation_model_no_att = tf.keras.models.Model(inputs=[source_input_no_att, target_input_no_att], outputs=[target_classification_no_att])\n",
    "\n",
    "    translation_model_no_att.compile(optimizer=\"Adam\",\n",
    "                                     loss='sparse_categorical_crossentropy',\n",
    "                                     metrics=['accuracy'])\n",
    "\n",
    "    return translation_model_no_att\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61fGG0H0jYsd"
   },
   "source": [
    "Now we can call the function we created to instantiate that model and confirm that it is set up the way we like using model.sumary()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AQafY_4y6jFl",
    "outputId": "e1b1bdcc-ba9d-4f5e-9917-295fd68ce016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " source_input_no_att (Input  [(None, 6)]                  0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " target_input_no_att (Input  [(None, 13)]                 0         []                            \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " source_embedding_layer_no_  (None, 6, 100)               200200    ['source_input_no_att[0][0]'] \n",
      " att (Embedding)                                                                                  \n",
      "                                                                                                  \n",
      " target_embedding_layer_no_  (None, 13, 100)              300400    ['target_input_no_att[0][0]'] \n",
      " att (Embedding)                                                                                  \n",
      "                                                                                                  \n",
      " encoder_lstm_layer_no_att   [(None, 6, 100),             80400     ['source_embedding_layer_no_at\n",
      " (LSTM)                       (None, 100),                          t[0][0]']                     \n",
      "                              (None, 100)]                                                        \n",
      "                                                                                                  \n",
      " decoder_lstm_layer_no_att   (None, 13, 100)              80400     ['target_embedding_layer_no_at\n",
      " (LSTM)                                                             t[0][0]',                     \n",
      "                                                                     'encoder_lstm_layer_no_att[0]\n",
      "                                                                    [1]',                         \n",
      "                                                                     'encoder_lstm_layer_no_att[0]\n",
      "                                                                    [2]']                         \n",
      "                                                                                                  \n",
      " classification_no_att (Den  (None, 13, 3004)             303404    ['decoder_lstm_layer_no_att[0]\n",
      " se)                                                                [0]']                         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 964804 (3.68 MB)\n",
      "Trainable params: 964804 (3.68 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encode_vocab_size = len(source_id_vocab_dict.keys())\n",
    "decode_vocab_size = len(target_id_vocab_dict.keys())\n",
    "\n",
    "translation_model_no_att = create_translation_model_no_att(encode_vocab_size, decode_vocab_size, embed_dim)\n",
    "\n",
    "translation_model_no_att.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w911c-QMYky3"
   },
   "source": [
    "It never hurts to look at the shapes of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E-NVTE_pU3-H",
    "outputId": "71025eec-5228-41bb-906b-e4025d6998a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252/252 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8047, 13, 3004)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model_no_att.predict(x=[train_source_input_data, train_target_input_data]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9EgkYYmjYsf"
   },
   "source": [
    "Now that everything checks out, we can train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h35KVb-G6jNs",
    "outputId": "d9aa9472-3d1e-4ab9-9cc6-d565c1bd69e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "252/252 [==============================] - 14s 38ms/step - loss: 2.5446 - accuracy: 0.6443 - val_loss: 1.6909 - val_accuracy: 0.7506\n",
      "Epoch 2/40\n",
      "252/252 [==============================] - 3s 11ms/step - loss: 1.5515 - accuracy: 0.7658 - val_loss: 1.4891 - val_accuracy: 0.7748\n",
      "Epoch 3/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 1.4052 - accuracy: 0.7754 - val_loss: 1.3935 - val_accuracy: 0.7809\n",
      "Epoch 4/40\n",
      "252/252 [==============================] - 3s 13ms/step - loss: 1.3076 - accuracy: 0.7870 - val_loss: 1.3194 - val_accuracy: 0.7944\n",
      "Epoch 5/40\n",
      "252/252 [==============================] - 3s 10ms/step - loss: 1.2257 - accuracy: 0.8001 - val_loss: 1.2681 - val_accuracy: 0.8013\n",
      "Epoch 6/40\n",
      "252/252 [==============================] - 5s 20ms/step - loss: 1.1613 - accuracy: 0.8059 - val_loss: 1.2213 - val_accuracy: 0.8060\n",
      "Epoch 7/40\n",
      "252/252 [==============================] - 4s 18ms/step - loss: 1.0988 - accuracy: 0.8134 - val_loss: 1.1813 - val_accuracy: 0.8118\n",
      "Epoch 8/40\n",
      "252/252 [==============================] - 5s 19ms/step - loss: 1.0425 - accuracy: 0.8205 - val_loss: 1.1501 - val_accuracy: 0.8178\n",
      "Epoch 9/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.9920 - accuracy: 0.8267 - val_loss: 1.1206 - val_accuracy: 0.8217\n",
      "Epoch 10/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.9468 - accuracy: 0.8326 - val_loss: 1.0978 - val_accuracy: 0.8254\n",
      "Epoch 11/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.9055 - accuracy: 0.8368 - val_loss: 1.0795 - val_accuracy: 0.8276\n",
      "Epoch 12/40\n",
      "252/252 [==============================] - 2s 10ms/step - loss: 0.8665 - accuracy: 0.8413 - val_loss: 1.0607 - val_accuracy: 0.8313\n",
      "Epoch 13/40\n",
      "252/252 [==============================] - 2s 10ms/step - loss: 0.8301 - accuracy: 0.8461 - val_loss: 1.0462 - val_accuracy: 0.8340\n",
      "Epoch 14/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.7950 - accuracy: 0.8505 - val_loss: 1.0301 - val_accuracy: 0.8370\n",
      "Epoch 15/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.7617 - accuracy: 0.8543 - val_loss: 1.0205 - val_accuracy: 0.8380\n",
      "Epoch 16/40\n",
      "252/252 [==============================] - 3s 11ms/step - loss: 0.7308 - accuracy: 0.8579 - val_loss: 1.0126 - val_accuracy: 0.8399\n",
      "Epoch 17/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.7010 - accuracy: 0.8616 - val_loss: 0.9975 - val_accuracy: 0.8430\n",
      "Epoch 18/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.6718 - accuracy: 0.8653 - val_loss: 0.9923 - val_accuracy: 0.8432\n",
      "Epoch 19/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.6441 - accuracy: 0.8690 - val_loss: 0.9866 - val_accuracy: 0.8436\n",
      "Epoch 20/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.6173 - accuracy: 0.8729 - val_loss: 0.9790 - val_accuracy: 0.8455\n",
      "Epoch 21/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.5920 - accuracy: 0.8765 - val_loss: 0.9776 - val_accuracy: 0.8462\n",
      "Epoch 22/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.5664 - accuracy: 0.8803 - val_loss: 0.9679 - val_accuracy: 0.8480\n",
      "Epoch 23/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.5423 - accuracy: 0.8846 - val_loss: 0.9619 - val_accuracy: 0.8497\n",
      "Epoch 24/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.5183 - accuracy: 0.8875 - val_loss: 0.9588 - val_accuracy: 0.8497\n",
      "Epoch 25/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.4970 - accuracy: 0.8911 - val_loss: 0.9587 - val_accuracy: 0.8502\n",
      "Epoch 26/40\n",
      "252/252 [==============================] - 3s 11ms/step - loss: 0.4751 - accuracy: 0.8950 - val_loss: 0.9588 - val_accuracy: 0.8510\n",
      "Epoch 27/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.4549 - accuracy: 0.8974 - val_loss: 0.9512 - val_accuracy: 0.8525\n",
      "Epoch 28/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.4349 - accuracy: 0.9013 - val_loss: 0.9506 - val_accuracy: 0.8538\n",
      "Epoch 29/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.4165 - accuracy: 0.9045 - val_loss: 0.9504 - val_accuracy: 0.8538\n",
      "Epoch 30/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.3987 - accuracy: 0.9085 - val_loss: 0.9545 - val_accuracy: 0.8529\n",
      "Epoch 31/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.3813 - accuracy: 0.9116 - val_loss: 0.9486 - val_accuracy: 0.8554\n",
      "Epoch 32/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.3639 - accuracy: 0.9153 - val_loss: 0.9528 - val_accuracy: 0.8558\n",
      "Epoch 33/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.3488 - accuracy: 0.9179 - val_loss: 0.9545 - val_accuracy: 0.8546\n",
      "Epoch 34/40\n",
      "252/252 [==============================] - 3s 10ms/step - loss: 0.3337 - accuracy: 0.9208 - val_loss: 0.9514 - val_accuracy: 0.8568\n",
      "Epoch 35/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.3187 - accuracy: 0.9239 - val_loss: 0.9557 - val_accuracy: 0.8579\n",
      "Epoch 36/40\n",
      "252/252 [==============================] - 3s 11ms/step - loss: 0.3045 - accuracy: 0.9271 - val_loss: 0.9588 - val_accuracy: 0.8571\n",
      "Epoch 37/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.2911 - accuracy: 0.9300 - val_loss: 0.9648 - val_accuracy: 0.8581\n",
      "Epoch 38/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.2784 - accuracy: 0.9335 - val_loss: 0.9593 - val_accuracy: 0.8593\n",
      "Epoch 39/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.2667 - accuracy: 0.9360 - val_loss: 0.9663 - val_accuracy: 0.8586\n",
      "Epoch 40/40\n",
      "252/252 [==============================] - 3s 11ms/step - loss: 0.2547 - accuracy: 0.9383 - val_loss: 0.9694 - val_accuracy: 0.8598\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7991fb4e5690>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model_no_att.fit(x=[train_source_input_data, train_target_input_data],\n",
    "                             y=train_target_output_data,\n",
    "                             validation_data=([test_source_input_data, test_target_input_data],\n",
    "                                              test_target_output_data),\n",
    "                             epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSmVtP2oXGAc"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 's2sAttention'></a>\n",
    "\n",
    "### 2.3 The Seq2seq model with Attention\n",
    "\n",
    "All we need to do is add an attention layer that ceates a context vector for each decoder position. We can use the attention layer provided by Keras in *tf.keras.layers.Attention()*.  We will then simply concatenate these corresponding context vectors with the output of the LSTM layer in order to predict the translation tokens one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPu2HDtfGKvK"
   },
   "outputs": [],
   "source": [
    "def create_translation_model_with_att(encode_vocab_size, decode_vocab_size, embed_dim):\n",
    "\n",
    "    source_input_with_att = tf.keras.layers.Input(shape=(max_encoder_seq_length,),\n",
    "                                                  dtype='int64',\n",
    "                                                  name='source_input_with_att')\n",
    "    target_input_with_att = tf.keras.layers.Input(shape=(max_decoder_seq_length,),\n",
    "                                                  dtype='int64',\n",
    "                                                  name='target_input_with_att')\n",
    "\n",
    "    source_embedding_layer_with_att = tf.keras.layers.Embedding(input_dim=encode_vocab_size,\n",
    "                                                                output_dim=embed_dim,\n",
    "                                                                name='source_embedding_layer_with_att')\n",
    "\n",
    "    target_embedding_layer_with_att  = tf.keras.layers.Embedding(input_dim=decode_vocab_size,\n",
    "                                                                 output_dim=embed_dim,\n",
    "                                                                 name='target_embedding_layer_with_att')\n",
    "\n",
    "    source_embeddings_with_att = source_embedding_layer_with_att(source_input_with_att)\n",
    "    target_embeddings_with_att = target_embedding_layer_with_att(target_input_with_att)\n",
    "\n",
    "    encoder_lstm_layer_with_att = tf.keras.layers.LSTM(embed_dim, return_sequences=True, return_state=True, name='encoder_lstm_layer_with_att')\n",
    "    encoder_out_with_att, encoder_state_h_with_att, encoder_state_c_with_att = encoder_lstm_layer_with_att(source_embeddings_with_att)\n",
    "\n",
    "    decoder_lstm_layer_with_att = tf.keras.layers.LSTM(embed_dim, return_sequences=True, return_state=False, name='decoder_lstm_layer_with_att')\n",
    "    decoder_lstm_out_with_att = decoder_lstm_layer_with_att(target_embeddings_with_att, [encoder_state_h_with_att, encoder_state_c_with_att])\n",
    "\n",
    "    attention_context_vectors = tf.keras.layers.Attention(name='attention_layer')([decoder_lstm_out_with_att, encoder_out_with_att])\n",
    "\n",
    "    concat_decode_out_with_att = tf.keras.layers.Concatenate(axis=-1, name='concat_layer_with_att')([decoder_lstm_out_with_att, attention_context_vectors])\n",
    "\n",
    "    target_classification_with_att = tf.keras.layers.Dense(decode_vocab_size,\n",
    "                                                           activation='softmax',\n",
    "                                                           name='classification_with_att')(concat_decode_out_with_att)\n",
    "\n",
    "    translation_model_with_att = tf.keras.models.Model(inputs=[source_input_with_att, target_input_with_att], outputs=[target_classification_with_att])\n",
    "\n",
    "    translation_model_with_att.compile(optimizer=\"Adam\",\n",
    "                                       loss='sparse_categorical_crossentropy',\n",
    "                                       metrics=['accuracy'])\n",
    "\n",
    "    return translation_model_with_att\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JMMluPO-urkE",
    "outputId": "9f6acca2-7dee-4a97-f626-725ba1c2bb6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " source_input_with_att (Inp  [(None, 6)]                  0         []                            \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " target_input_with_att (Inp  [(None, 13)]                 0         []                            \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " source_embedding_layer_wit  (None, 6, 100)               200200    ['source_input_with_att[0][0]'\n",
      " h_att (Embedding)                                                  ]                             \n",
      "                                                                                                  \n",
      " target_embedding_layer_wit  (None, 13, 100)              300400    ['target_input_with_att[0][0]'\n",
      " h_att (Embedding)                                                  ]                             \n",
      "                                                                                                  \n",
      " encoder_lstm_layer_with_at  [(None, 6, 100),             80400     ['source_embedding_layer_with_\n",
      " t (LSTM)                     (None, 100),                          att[0][0]']                   \n",
      "                              (None, 100)]                                                        \n",
      "                                                                                                  \n",
      " decoder_lstm_layer_with_at  (None, 13, 100)              80400     ['target_embedding_layer_with_\n",
      " t (LSTM)                                                           att[0][0]',                   \n",
      "                                                                     'encoder_lstm_layer_with_att[\n",
      "                                                                    0][1]',                       \n",
      "                                                                     'encoder_lstm_layer_with_att[\n",
      "                                                                    0][2]']                       \n",
      "                                                                                                  \n",
      " attention_layer (Attention  (None, 13, 100)              0         ['decoder_lstm_layer_with_att[\n",
      " )                                                                  0][0]',                       \n",
      "                                                                     'encoder_lstm_layer_with_att[\n",
      "                                                                    0][0]']                       \n",
      "                                                                                                  \n",
      " concat_layer_with_att (Con  (None, 13, 200)              0         ['decoder_lstm_layer_with_att[\n",
      " catenate)                                                          0][0]',                       \n",
      "                                                                     'attention_layer[0][0]']     \n",
      "                                                                                                  \n",
      " classification_with_att (D  (None, 13, 3004)             603804    ['concat_layer_with_att[0][0]'\n",
      " ense)                                                              ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1265204 (4.83 MB)\n",
      "Trainable params: 1265204 (4.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "translation_model_with_att = create_translation_model_with_att(encode_vocab_size, decode_vocab_size, embed_dim)\n",
    "\n",
    "translation_model_with_att.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XYjG5lVPNBsU",
    "outputId": "cf12d01d-2f47-4b15-800f-0812a255af85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "252/252 [==============================] - 12s 28ms/step - loss: 2.3906 - accuracy: 0.6828 - val_loss: 1.6072 - val_accuracy: 0.7652\n",
      "Epoch 2/40\n",
      "252/252 [==============================] - 4s 14ms/step - loss: 1.4837 - accuracy: 0.7707 - val_loss: 1.4521 - val_accuracy: 0.7753\n",
      "Epoch 3/40\n",
      "252/252 [==============================] - 3s 10ms/step - loss: 1.3540 - accuracy: 0.7819 - val_loss: 1.3410 - val_accuracy: 0.7912\n",
      "Epoch 4/40\n",
      "252/252 [==============================] - 3s 10ms/step - loss: 1.2241 - accuracy: 0.7977 - val_loss: 1.2534 - val_accuracy: 0.8024\n",
      "Epoch 5/40\n",
      "252/252 [==============================] - 3s 10ms/step - loss: 1.1259 - accuracy: 0.8093 - val_loss: 1.1994 - val_accuracy: 0.8096\n",
      "Epoch 6/40\n",
      "252/252 [==============================] - 3s 13ms/step - loss: 1.0468 - accuracy: 0.8186 - val_loss: 1.1525 - val_accuracy: 0.8174\n",
      "Epoch 7/40\n",
      "252/252 [==============================] - 2s 10ms/step - loss: 0.9738 - accuracy: 0.8274 - val_loss: 1.1144 - val_accuracy: 0.8233\n",
      "Epoch 8/40\n",
      "252/252 [==============================] - 3s 10ms/step - loss: 0.9058 - accuracy: 0.8352 - val_loss: 1.0747 - val_accuracy: 0.8297\n",
      "Epoch 9/40\n",
      "252/252 [==============================] - 2s 10ms/step - loss: 0.8423 - accuracy: 0.8427 - val_loss: 1.0444 - val_accuracy: 0.8358\n",
      "Epoch 10/40\n",
      "252/252 [==============================] - 3s 11ms/step - loss: 0.7830 - accuracy: 0.8497 - val_loss: 1.0190 - val_accuracy: 0.8400\n",
      "Epoch 11/40\n",
      "252/252 [==============================] - 3s 11ms/step - loss: 0.7279 - accuracy: 0.8565 - val_loss: 1.0002 - val_accuracy: 0.8435\n",
      "Epoch 12/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.6764 - accuracy: 0.8625 - val_loss: 0.9845 - val_accuracy: 0.8453\n",
      "Epoch 13/40\n",
      "252/252 [==============================] - 2s 10ms/step - loss: 0.6294 - accuracy: 0.8686 - val_loss: 0.9739 - val_accuracy: 0.8475\n",
      "Epoch 14/40\n",
      "252/252 [==============================] - 2s 10ms/step - loss: 0.5841 - accuracy: 0.8748 - val_loss: 0.9581 - val_accuracy: 0.8499\n",
      "Epoch 15/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.5426 - accuracy: 0.8803 - val_loss: 0.9473 - val_accuracy: 0.8524\n",
      "Epoch 16/40\n",
      "252/252 [==============================] - 3s 10ms/step - loss: 0.5028 - accuracy: 0.8874 - val_loss: 0.9377 - val_accuracy: 0.8548\n",
      "Epoch 17/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.4658 - accuracy: 0.8936 - val_loss: 0.9299 - val_accuracy: 0.8568\n",
      "Epoch 18/40\n",
      "252/252 [==============================] - 3s 10ms/step - loss: 0.4306 - accuracy: 0.8996 - val_loss: 0.9240 - val_accuracy: 0.8587\n",
      "Epoch 19/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.3987 - accuracy: 0.9059 - val_loss: 0.9162 - val_accuracy: 0.8596\n",
      "Epoch 20/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.3686 - accuracy: 0.9118 - val_loss: 0.9189 - val_accuracy: 0.8611\n",
      "Epoch 21/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.3408 - accuracy: 0.9184 - val_loss: 0.9162 - val_accuracy: 0.8627\n",
      "Epoch 22/40\n",
      "252/252 [==============================] - 2s 10ms/step - loss: 0.3156 - accuracy: 0.9236 - val_loss: 0.9108 - val_accuracy: 0.8635\n",
      "Epoch 23/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.2930 - accuracy: 0.9280 - val_loss: 0.9113 - val_accuracy: 0.8642\n",
      "Epoch 24/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.2711 - accuracy: 0.9325 - val_loss: 0.9093 - val_accuracy: 0.8645\n",
      "Epoch 25/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.2517 - accuracy: 0.9372 - val_loss: 0.9111 - val_accuracy: 0.8657\n",
      "Epoch 26/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.2344 - accuracy: 0.9413 - val_loss: 0.9091 - val_accuracy: 0.8681\n",
      "Epoch 27/40\n",
      "252/252 [==============================] - 2s 10ms/step - loss: 0.2191 - accuracy: 0.9441 - val_loss: 0.9163 - val_accuracy: 0.8684\n",
      "Epoch 28/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.2038 - accuracy: 0.9468 - val_loss: 0.9186 - val_accuracy: 0.8682\n",
      "Epoch 29/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.1915 - accuracy: 0.9501 - val_loss: 0.9224 - val_accuracy: 0.8689\n",
      "Epoch 30/40\n",
      "252/252 [==============================] - 3s 11ms/step - loss: 0.1800 - accuracy: 0.9526 - val_loss: 0.9270 - val_accuracy: 0.8675\n",
      "Epoch 31/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.1675 - accuracy: 0.9554 - val_loss: 0.9266 - val_accuracy: 0.8682\n",
      "Epoch 32/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.1590 - accuracy: 0.9563 - val_loss: 0.9311 - val_accuracy: 0.8701\n",
      "Epoch 33/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.1503 - accuracy: 0.9580 - val_loss: 0.9344 - val_accuracy: 0.8699\n",
      "Epoch 34/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.1420 - accuracy: 0.9601 - val_loss: 0.9377 - val_accuracy: 0.8705\n",
      "Epoch 35/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.1346 - accuracy: 0.9613 - val_loss: 0.9498 - val_accuracy: 0.8705\n",
      "Epoch 36/40\n",
      "252/252 [==============================] - 2s 10ms/step - loss: 0.1279 - accuracy: 0.9623 - val_loss: 0.9561 - val_accuracy: 0.8694\n",
      "Epoch 37/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.1225 - accuracy: 0.9630 - val_loss: 0.9594 - val_accuracy: 0.8718\n",
      "Epoch 38/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.1176 - accuracy: 0.9640 - val_loss: 0.9602 - val_accuracy: 0.8711\n",
      "Epoch 39/40\n",
      "252/252 [==============================] - 3s 12ms/step - loss: 0.1119 - accuracy: 0.9655 - val_loss: 0.9719 - val_accuracy: 0.8711\n",
      "Epoch 40/40\n",
      "252/252 [==============================] - 2s 9ms/step - loss: 0.1071 - accuracy: 0.9661 - val_loss: 0.9777 - val_accuracy: 0.8701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x7991fb3b5250>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_model_with_att.fit(x=[train_source_input_data, train_target_input_data],\n",
    "                               y=train_target_output_data,\n",
    "                               validation_data=([test_source_input_data, test_target_input_data],\n",
    "                                                test_target_output_data),\n",
    "                               epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz6en_QG6Mnw"
   },
   "source": [
    "Validation accuracy is about one percentage point better.\n",
    "\n",
    "**Question 1:** Why do you think the benefit of adding an attention layer is not larger?\n",
    "\n",
    "[Return to Top](#returnToTop)  \n",
    "<a id = 't5Example'></a>\n",
    "\n",
    "## 3. T5\n",
    "\n",
    "Now we turn to text generation with transformers. The T5 system was introduced [here](https://arxiv.org/pdf/1910.10683.pdf).  This model uses both the encoder and the decoder configurations of transformers and connects them together.  A big difference with this model is that it designed to accept text as an input and produce text as an output for a number of different tasks ranging from summarization and question answering to classification.  The system needs to be told which task to perform as the first part of the input text.  Be sure to look in *Appendix D* of the paper to see a complete set of the tasks that T5 base and large checkpoints can perform right out of the box and the data used to train them.\n",
    "\n",
    "Let's play a bit with Huggingface's (Large) implementation of T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 558,
     "referenced_widgets": [
      "9f3365bb70d043dbacbacc3c4d162fbe",
      "3cbe346c118843e2a68d7c61b297a8bc",
      "e772cd9900db4270b3f038d1532d1a01",
      "d3bf5eb21fed4c78817869c57ce16ad9",
      "bfcc3a6cba114a0aa490b3f33deb1ba2",
      "dfb2de3e574549de9fbcdaeff2879076",
      "be787abe715b4ea4bb3308a56dc0f306",
      "a0d194c0277542518e9ee50d261bfde7",
      "2d5c0df5d0bf43bab32a8c470b222bfb",
      "4568b0a9fca345808ad20ea504d3136e",
      "1ae905bf50f641dd8f7365c65b30ab68",
      "18645221e0f44fcfbab164e9308cb1be",
      "4397505db24243e294ec0c6c77af9865",
      "622063cfe9ff45a5abbf5a37e518e750",
      "22066ee8fc1e494c97f87ed2f013662f",
      "b93a2bce451a4b0891457926d473b868",
      "8e41b4099f844add97d50cb8dc428b81",
      "76721cfa1b284a07a68584acd5e59385",
      "d5fc55c5547b43d7becb618dc8d48b81",
      "b201f29ed8bf4f7dbb3d02a2b5017193",
      "aa53d53305f64a0787908976e2dfa3f2",
      "4316fba4026c454d8d6cb7f1bc1fb393",
      "b50191538b394c879ec8dce8a751c30f",
      "6867a5a51f354eb2b84f90ee1f62c7c2",
      "76158b2ac5dd4a798918142a131109f3",
      "d5200e8f7fb748a682595893fce26eb9",
      "35f3b4fa5b03482bb4a2f4532f94b007",
      "f7eecb2a3f754116a0b30ac49af2d34d",
      "a476fccd1bc141abb19544083a16dffe",
      "17fd2201baae4264a4274c983945f976",
      "6a4ffa38838747549b38decab9488a15",
      "7a4cae9bcfb34e32880c848f36f4f01e",
      "351e748b47234c4abc059cbab7833db0",
      "3a16d77cedb74ac293574371a18b0719",
      "4c1c99641c6f4212a4146b938e02c3c9",
      "5e18e951ff3d41259c9d100d85c57658",
      "4957dad6b91e4c849cf033a4398e3e5d",
      "4bb3cc799cb7418f86ad4ccd52f2b4d9",
      "20997f687ec04ace91bc56f91412ed03",
      "391520ebe75c439dafe7dcb9c7f50757",
      "cb608e727d9b4e418124db5b5a484745",
      "e87fcf3d0acb4d328bfffc3baa6c67f8",
      "4ca24f8bbb424e1eb7fdefea0f826b6e",
      "c773908b8bdc482dbefa30d81ddf31a5"
     ]
    },
    "id": "ZP02F_4E2Ngq",
    "outputId": "48c6072b-88e2-481c-b8fd-64a6ba4aa87f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3365bb70d043dbacbacc3c4d162fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18645221e0f44fcfbab164e9308cb1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50191538b394c879ec8dce8a751c30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a16d77cedb74ac293574371a18b0719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tft5_for_conditional_generation\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " shared (Embedding)          multiple                  32899072  \n",
      "                                                                 \n",
      " encoder (TFT5MainLayer)     multiple                  334939648 \n",
      "                                                                 \n",
      " decoder (TFT5MainLayer)     multiple                  435627520 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 737668096 (2.75 GB)\n",
      "Trainable params: 737668096 (2.75 GB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "\n",
    "t5_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukA3CNS6ZsZt"
   },
   "source": [
    "737 m trainable parameters. Quite a lot.\n",
    "\n",
    "Let's create a short text to use as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGxjgkL96Lh3"
   },
   "outputs": [],
   "source": [
    "ARTICLE = (\"Oh boy, what a lengthy and cumbersome excercise this was. \" \\\n",
    "           \"I had to look into every detail, check everything twice, \" \\\n",
    "           \" and then compare to prior results. Because of this tediousness \" \\\n",
    "           \" and extra work my homework was 2 days late.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-tEGC7MZz9F"
   },
   "source": [
    "Next, we need to specify the task we want T5 to perform and include it at the begining of the input text.  We add a task prompt to the begining of our input.  Because we are summarizing, we add the word *summarize:* to the begining of our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgFCtCMP2TAo"
   },
   "outputs": [],
   "source": [
    "t5_input_text = \"summarize: \" + ARTICLE\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoR7GwLsZ96P"
   },
   "source": [
    "First, we will generate a summary using the default output options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BooaIspD2YHx",
    "outputId": "4b4df848-5746-4981-8b10-125d97d426ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/tf_utils.py:836: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['homework was a lengthy and cumbersome excercise . because of this tedious']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'])\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False)\n",
    "       for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHlbG7UOaEnr"
   },
   "source": [
    "Not great. But let's get more sophisticated and prescribe a minimum length and use beam search to generate multiple outputs.  We also indicate the maximum length the output should be.  Finally, in order to reduce repetitive output we tell the model to avoid output that repeats trigrams (three word groupings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cl2LvRRz2cH1",
    "outputId": "e34b219c-885c-4595-d12f-4ab8208146e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i had to look into every detail, check everything twice, and then compare to prior results . because of this tediousness and extra work my homework was 2 days late .']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                   num_beams=3,\n",
    "                                   no_repeat_ngram_size=3,\n",
    "                                   min_length=20,\n",
    "                                   max_length=40)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5-eCY8DaQTM"
   },
   "source": [
    "That is a bit better thanks to our application of some hyperparameters.\n",
    "\n",
    "Lastly, can T5 perform machine translation? Yes, in some limited instances.  We need to specify the input and output languages. Keep in mind that the model has only been trained to translate in particular directions e.g. English to Romanian but NOT Romanian to English.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guJVGlqg5LWM"
   },
   "outputs": [],
   "source": [
    "t5_input_text = \"translate English to German: \" + ARTICLE\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CB6pE72B5PKb",
    "outputId": "3a7f18ec-3609-4ea5-98f3-528acc2a8e77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ich habe es nicht geschafft, meinen ersten Test zu schreiben, da ich nicht gengend Zeit hatte, um meinen Test zu bearbeiten.']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                   num_beams=3,\n",
    "                                   no_repeat_ngram_size=3,\n",
    "                                   min_length=10,\n",
    "                                   max_length=40)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1C3xL6ma5xC"
   },
   "source": [
    "Hmm... output language fluency is very good. But take the German output and feed it in to translate.google.com and see what this means. Is it anything like its English input? This hallucination might be mitigated by changing some of the hyperparameters like num_beams.\n",
    "\n",
    "Is a shorter example more accurate?  Maybe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SY93muRb5RiI"
   },
   "outputs": [],
   "source": [
    "t5_input_text = \"translate English to German: That was really not very good today; it was too difficult to solve.\"\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mDsF31UZbK1I",
    "outputId": "ba4ef34c-3f11-4474-bd0b-19f3ef67ea1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Das war heute wirklich nicht sehr gut; es war zu schwierig zu lsen.']\n"
     ]
    }
   ],
   "source": [
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                   num_beams=3,\n",
    "                                   no_repeat_ngram_size=3,\n",
    "                                   min_length=10,\n",
    "                                   max_length=40)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJy_BapabMYQ"
   },
   "source": [
    "That is not bad, though some mistakes are there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OihbtuDE24RP"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'prompts'></a>\n",
    "\n",
    "## 4. Prompt Engineering and Generative Large Language Models\n",
    "\n",
    "The development of very large language models such as [GPT3](https://arxiv.org/pdf/2005.14165.pdf) have led to increased interest in few shot and zero shot approaches to tasks.  These generative language models allow a user to provide a prompt with several examples followed by a question the model must answer.  GPT3, especially its 175 billion parameter model, demonstrates the feasibility of a zero shot model where the model can simply be presented with the prompt and in many instances provide the correct answer.  \n",
    "\n",
    "The implication of this zero shot capability is that a very large generative language model can be pre-trained and then shared by a large group of people because it requires no fine-tuning or parameter manipulation. Instead, the users work on the wording of their prompt and providing enough context that the model an perform the task correctly. [Liu et. al.](https://arxiv.org/pdf/2107.13586.pdf) characterize this as \"pre-train, prompt, and predict.\"\n",
    "\n",
    "There are multiple approaches to pre-train, prompt and predict.  Here we explore two of them.  First we look at cloze prompts.  These leverage the masked language model approach used in BERT an T5 where individual words or spans are masked and during pre-training the model learns to predict the maked tokens. Second we look at prefix prompts.  These leverage the next word prediction capability of decoder only models in the GPT family. Finally, we look at a zero-shot instruction fine tuned next token prediction model exemplifed by [Llama 3.1 8 billion parameter model](https://arxiv.org/pdf/2407.21783)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X4vUv5ZYB9rz"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'clozePrompts'></a>\n",
    "\n",
    "### 4.1 Cloze Prompts\n",
    "\n",
    "Cloze prompts take advantage of the masked language model task where an individual word or span of words anywhere in the input are masked and the language model learns to predict them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "ab2840cd786442448fea9df364dce1f6",
      "bd5005a36f1f4a799c364c65e17fade5",
      "5081e0c7935e4fb7b7e6d1ce6c91c011",
      "e7a10f807f49450da33d77f41db2cd27",
      "b368b928de9c426a909d07e66d1abef5",
      "97d5ed1f4399454db4791c8547f46098",
      "6bbb4907c86647429f5c10893e3877fc",
      "6dc29705bfc94fa9aa45fd8c94553b50",
      "f8dc7c4862444e13ac2ffc839f6d2e38",
      "6fbcfd830e144b8c87f6fe9b3523f364",
      "6369abdde5af42cea1475fe512f25a35",
      "316b479ae8ca423c8226020826cd1e41",
      "4edebe08af0444d991eb3e1e68847148",
      "1e354a7b5a1949ecb77e15b89f8e178f",
      "73e6627569d24833b86f0cfc40e90610",
      "79b6665d284c4a3ebaa1104920ca2594",
      "6b7cb085da8949519a7de5da26b7a607",
      "8665a8b550904179905255b5b51e7f9c",
      "0a1295f694a34f1a8b77f5406defaa92",
      "a9b7e93545214a57acf78bdc6fa03d9a",
      "b22335f765f74e73ab20ca67ecc0cfad",
      "9512911e58f7412fa1c4865089c44406",
      "e4ffa9f05a7c4596bc369a2dddd93a85",
      "8ed6b0b8f98f4a6b899f9c692c9797bf",
      "49a53f2a3b3640f59ac014b8ceb589c9",
      "ddc04c150b694f269cebec40dce58dec",
      "3573319da5b244f4bbdd450b81f0908e",
      "c0df749fe9da4c25b9fd3561fcf85139",
      "f162546883e24c83af7254094c2555de",
      "465788b4964d49f1a69e22eed8fa679b",
      "f11dee4630674860ae0d67cbc711d2db",
      "e9ee4e4c0b5844a0ad9f64ed3a489342",
      "43c10519b1054427be476a493583d08a",
      "5de4a0802bba46d89d41454650876cff",
      "7ac3910d0e8845668cb6a5055b16e2f9",
      "c16604c3d35c4f00939bd33a58284479",
      "98af746a7c2b4822b444b14de2e9c467",
      "8f379c3ca1a6418f8a7c815d7f6209f0",
      "20fa4bcdbfa347dab7ae7260b9c6172e",
      "de50a86fc6b54643bfaca60cbfcf8b5d",
      "587862aba6cb493e955d44887f1c7dff",
      "6dd89758bb214ec787626635c9cfabf6",
      "b56946a9400d4bfaa00d0d885a099c5f",
      "27ec3463a8f74985aaf0709429dee99b"
     ]
    },
    "id": "0ECNZWOiJK8r",
    "outputId": "47c313e4-2deb-43e6-91ca-c0f287992e28"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2840cd786442448fea9df364dce1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316b479ae8ca423c8226020826cd1e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ffa9f05a7c4596bc369a2dddd93a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de4a0802bba46d89d41454650876cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Delete the old model so we are managing memory\n",
    "del t5_tokenizer\n",
    "del t5_model\n",
    "\n",
    "#get a new model with a new checkpoint\n",
    "t5_model = TFT5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3imLWPK2_JR"
   },
   "source": [
    "\"\\<extra_id_0\\>\" is the special token (called a sentinel token) we can use with T5 to invoke its masked word modeling ability. There are up to 99 of these tokens. This means we can construct sentences, like a fill in the blank test, that allow us to probe the knowledge embedded in the model based on its pre-training.  Here's an example that works well.  After you've run it try substituting beagle for poodle and you'll see the model gets confused.\n",
    "\n",
    "Notice two that we are using a beam search approach and accepting the top three choices rather than just the first choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VErVi2AW3J84",
    "outputId": "df0516dd-55d0-48be-8515-9ed64fd9b388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shepherd', 'working', 'Working']\n"
     ]
    }
   ],
   "source": [
    "PROMPT_SENTENCE = ( \"An Australian <extra_id_0> is a type of working dog .\")\n",
    "t5_input_text = PROMPT_SENTENCE\n",
    "t5_inputs = t5_tokenizer([t5_input_text], return_tensors='tf')\n",
    "t5_summary_ids = t5_model.generate(t5_inputs['input_ids'],\n",
    "                                   num_beams=10,\n",
    "                                   #temperature=0.8,\n",
    "                                   no_repeat_ngram_size=2,\n",
    "                                   num_return_sequences=3,\n",
    "                                   min_length=1,\n",
    "                                   max_length=3)\n",
    "\n",
    "print([t5_tokenizer.decode(g, skip_special_tokens=True,\n",
    "                           clean_up_tokenization_spaces=False) for g in t5_summary_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3mYsy7VG9GM"
   },
   "outputs": [],
   "source": [
    "#Keep our memory free of old models\n",
    "del t5_tokenizer\n",
    "del t5_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_OmxvA13A09"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'prefixPrompt'></a>\n",
    "\n",
    "### 4.2 Prefix Prompts\n",
    "\n",
    "Prefix prompts are used with models that predict the next word given a large context window.  If you fill that window with the right information you can get the model to generate the output you want.  GPT3 relies on this approach to successfully perform.  You can either include a couple of examples of what you want the model to do and then ask your question or you can just ask your question.\n",
    "\n",
    "Let's take a look at a decoder-only generative pretrained text generation model: [OPT](https://arxiv.org/pdf/2205.01068.pdf). This model doesn't have separate input and output sequences, instead we will feed in one sequence (the prefix prompt) and ask the model to continue generating text to complete that same sequence.  The OPT model is intended to replicate the functionality of the GPT-3 model and comes in several size from 125 million parameters to 175 billion parameters.  We'll work with the 350 million parameter model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAdytHKkIXtx"
   },
   "source": [
    "As with T5, we'll just try out the pre-trained model and see what text it generates for a new starting sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xA7SCOBXM7nN"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFOPTForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "f06597caec444896915d901967c49f00",
      "d34f9880a41f4359bb9161c151473724",
      "ca550c81fc3d4eaa9aa4d41b931a9fe4",
      "cdb20c1c17a245c5bb62c26fdbd7c48c",
      "5f322de6c89f4e5faab867ef816eda1d",
      "0c8a50d3328f48a5bc6cd6d87cd5cf68",
      "2a0fa63f44724967a1c7f92942f01f54",
      "924259b76d31423393025b10568b7e14",
      "233c3d9152f94466a188395134cd564d",
      "7e1237fabf114fbca4d54a0aa607d040",
      "1f0367b27247491c9284cc6fce340a51",
      "93e1cc578d6b4b04877f7a177add59f2",
      "0ab5b53cb5154cee8662357334120aca",
      "44ef3d4bc73f44a2b5bf986a837f5cb4",
      "0733824accaa4dce995baffc49cff818",
      "db21d58763184c0d97589d499abd9670",
      "3c65154c413c48e1b34fdf617252db34",
      "e43c7dfbcb0e4f36bf4e379227c41c91",
      "bf1b58255b834a5aa702bc472e86bbcb",
      "f926a7f42ec0466b899cbbbdb8c1f9ad",
      "d6f9dfe69d3f43089bcfa553fa3b681e",
      "abfbca97e452455aa2d8964cffad642e",
      "ae7a9a22864d4d0ca7ae3e63d16921db",
      "1ea81dbcd6db457d8d5ae137bea3a6ee",
      "ed1174d1d89048dca66e852230b780bd",
      "506c15d4c3834b02a526e318d13cefc8",
      "72a2837160014dd5bf5b236dd311557c",
      "4d094401621d4b7980d6bf27f494d702",
      "122a6fd8f8d94c2e9418411f31d5f84c",
      "e0bfff12ecbb4d6b95f957a4221189d0",
      "ee08b0a1677143f4864098237c382aa3",
      "c5c7ce2c15234037a14a729b0b8fd717",
      "5193b22120f244b5b4ce730c25adeac4",
      "0861d8d716c84d5ab11fb84aec739d45",
      "85eb599a290f43dbae612d2f59039c28",
      "6d64a08855e346e18819cb6b582937a4",
      "a0ebd293948b406dbb51992923839af1",
      "b3e6480f0e7048d481b31eee352f3eae",
      "9a3035e2204f4da1b77f0f8d9f579466",
      "87b7bfe9d97744b692af288fb3246b20",
      "26494833719c4974809685267efddaf7",
      "a8d6feab2bc044a99eb5d27bcdc18c44",
      "e4fa0cc258184cb2a5fb538e3317c902",
      "ff2100afa5ab415ab61e8a92688777b1",
      "1f4108460b484241b732558804d3d314",
      "921aa2980e4948f09f79306b3b5f1db0",
      "57358ec8e4b044329030434551d160cd",
      "5a6eac05da784514b0fbe931c2746fb8",
      "89fb732b027a437ab74565fa0fd2bb0b",
      "4f708b756d9640a688af249478eec4dc",
      "88cb43520e6a4d03adeb9da7a2e1b709",
      "5e6dbb1b3158443694ad9a4b492f5188",
      "3dddb02c62d34e6f820340b3bb617922",
      "4add5e058f1f487ab70a81ace175e008",
      "d7a3bf5f38154abda42a7ad3b388ae6a",
      "8f4e215109b44c10a40a9ec7285f83c7",
      "efa95b5267a44ac1b9bdbd39b92a45b4",
      "f86288549575488283af1b0260b0be0e",
      "25e60a982bf24401a83bfaf8fe32c15a",
      "c42c6362438a43b182791b7876001650",
      "62e2fd7131034cde83c13b65e38e4201",
      "57e968cafc554db89a3cbd143aa52608",
      "9e0aafc5666a4c6d86fffa96d98de910",
      "b581f0d0bac043b7bc2cc3584b30316e",
      "082ae2f87b524999b0b8d2667d787667",
      "023f4e9be9ba4218a4c798d635520ac2",
      "652fe3e1412348e498056634a61e8585",
      "d9f67e0a231341e5aef4e1c5f32bf9a0",
      "de7d20a3e02c4278ba3d88ade2fda36f",
      "bb49083a98c44127a65f26c016603c07",
      "21ced399ab3e4b338f05f60a26efd8d1",
      "da6cd8c94d284184a9ea16b9558ac4d9",
      "4077c29b6eb344dbaa3e7be63388de1f",
      "095f9309b6414beb956e41ee195098e7",
      "a4feaa0d061c4c529dab11315139960b",
      "0a40f814cfd74ab79b428962a2a7c538",
      "929f320e82e0462db9006c55d94e8b81"
     ]
    },
    "id": "iTl3MurxM7d2",
    "outputId": "f58d6f95-32d1-4ab5-dde1-9bcbd3ada5b4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06597caec444896915d901967c49f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e1cc578d6b4b04877f7a177add59f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7a9a22864d4d0ca7ae3e63d16921db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0861d8d716c84d5ab11fb84aec739d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f4108460b484241b732558804d3d314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f4e215109b44c10a40a9ec7285f83c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tf_model.h5:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFOPTForCausalLM.\n",
      "\n",
      "All the layers of TFOPTForCausalLM were initialized from the model checkpoint at facebook/opt-350m.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOPTForCausalLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652fe3e1412348e498056634a61e8585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "model = TFOPTForCausalLM.from_pretrained(\"facebook/opt-350m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMlBEtVwhhbf"
   },
   "source": [
    "We'll give it a prompt now and see what it generates next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rseHCa6q1OP9"
   },
   "outputs": [],
   "source": [
    "prefix_prompt = 'Yesterday, I went to the store to buy '\n",
    "input_ids = tokenizer.encode(prefix_prompt, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpI1EL511OTA",
    "outputId": "86bcd037-2e7b-4f97-fc72-c70dc27e753d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: Yesterday, I went to the store to buy ichiro and some of my other favorite cards.\n",
      "I had a really good time!\n",
      "First, I met up\n",
      "\n",
      "1: Yesterday, I went to the store to buy  a new belt.\n",
      "The sales clerk looked at me and said \"did you forget your belt?\"  So she took\n",
      "\n",
      "2: Yesterday, I went to the store to buy  a new pair of shoes. While in there was this weird dude walking around and staring at me like he knew what he\n"
     ]
    }
   ],
   "source": [
    "generated_text_outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=35,\n",
    "    num_return_sequences=3,\n",
    "    repetition_penalty=1.5,\n",
    "    top_p=0.92,\n",
    "    temperature=.85,\n",
    "    do_sample=True,\n",
    "    top_k=125,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "#Print output for each sequence generated above\n",
    "for i, beam in enumerate(generated_text_outputs):\n",
    "  print()\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam, skip_special_tokens=True, clean_up_tokenization_spaces=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5TRA7t_1H7MG"
   },
   "source": [
    "Now let's try a long prompt to give the model a lot of context to work with and see how well it performs.  We'll ask it to generate a recipe and see how well if follows instructions.  We'll try it with several smaller models avaiable on HuggingFace. Finally, we'll include the output for that same prompt from chatGPT for comparison purposes.\n",
    "\n",
    "In order to do so without exceeding the memory, you should **STOP the notebook and Reconnect it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTwUaW0loG2A"
   },
   "outputs": [],
   "source": [
    "!pip install pydot --quiet\n",
    "!pip install transformers --quiet\n",
    "!pip install sentencepiece --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39fbiIfm5Tgp"
   },
   "source": [
    "We'll also use the HuggingFace AutoTokenizer and AutoModelXXX classes as they'll let us just specify new checkpoints rather than having to find the specific model type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aKaWneKqEyXo"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V5fZ426uze4g"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5n_MGBm1j4YV"
   },
   "source": [
    "Now let's try the Facebook OPT model that is designed to be an open source model equivalent to GPT-3.  We have limited compute resources so we'll use the 1.3 billion parameter model.  For comparison purposes in our the full GPT-3 model has 175 billion parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364,
     "referenced_widgets": [
      "6a8a5548ebc641e0874ca986e507f2fa",
      "03b993b07e38458c80f33ea7fec759fb",
      "baa1a5ccc2f34053a499a9f4ff656755",
      "2b3749be2e794f28a70c7e3c4d99e572",
      "f868a5dc710a4efea09d11c63a2d8765",
      "88b417b2f1bc47d1b29ce68127a77211",
      "f4d3a52208344d4c994ddcf4f1c56e4a",
      "69133b789898487e9ed32d4c84c2a2a3",
      "256a6f408dd04ffa8b0c6a27689db8b7",
      "67aa613f82c149dd9d9341a37bb50244",
      "6677fa19b4dd4a3ab8b1340d438c6cb5",
      "807091a0f9d146f3991aea9e7497ab8e",
      "ce15e276d5bf4a19893ac2212694291f",
      "747cb24b5bdf44da8079922c9272364e",
      "481fd89fff7342ae96704fadc6f75195",
      "b036f1000f2b472fb22ecb3388516254",
      "d003452d6c7d4f98ac629bcbc52b88a4",
      "91fc261b26774a4c8fa849512b518014",
      "46bfd2417ae74a7ab4579d90568934e5",
      "7f59e344ffe545e286799424d0592011",
      "450daa736b2a4d0c8a97861a63c270f2",
      "2536c00cd48c416094ea5f543d61b199",
      "f6f10ea00c114c25b09bd8e1c76961c0",
      "e5e0573e579449e5908794dcb7477a58",
      "82724c3866064c7e8017dc37f9708984",
      "422b53d6babb438f95e0acd300c8659b",
      "f0a292eb0f274172abf217e510694c5c",
      "530670af3fc746f38867f7c68b4f61ce",
      "ccfd536c6f824f4db917aa0cbb978927",
      "ce2c50b10b8f4d76a561d99822f2ab1a",
      "c137c184b30f4cca8e78f0db21aa7dc3",
      "02551268dac84803be638793903c2d55",
      "4a273aad9ae04b99b02d572165f0b6ba",
      "6241c1b458e748f9afb88ebde2524c93",
      "b281ee17f0584c9bb864c87ce47d150a",
      "55b880d5c1344d6ebe9374c6789ddbc9",
      "c05762051e254f8ebccb623639251de7",
      "fe65c8b26e5b4e15a1461e94d24bcc3d",
      "c2770ee6c71445f681b282444ca001fa",
      "22664f514b6d4a7aa7d37c85b12117d0",
      "7b81fbf040614e5385b4c8cbcf399548",
      "d897b684964a4b71b127343e7dcc1776",
      "2d25cf39975f4f3d8872013ccad066dc",
      "b2d592e948084c9aab2b6348499f2571",
      "5875688379cb4cd7a6041fc52755e6fb",
      "4eca897a6e5c46b986ede1ca65ca5f18",
      "d49de8e263444fad947c83e3aa50b374",
      "2d6048cc3ba9424ebbc7f78df9f28fb4",
      "13ee131dd788407d90dcb00bdf190742",
      "7075c749b37343449147e59363d3299a",
      "fb5cad31fcaf4970b8e717d3b9e60244",
      "d826c0e041124e83a182a2d9e7aa1325",
      "7d5e177942324a069f8d8d176e01dbdc",
      "e6fa7ffcdfd249229ccfa660fecf271a",
      "5f174562be5c4cd59e9134e3ec01ecd9",
      "9cdf1575f32d40afb777d6334d98a037",
      "e5b9d860551e4152969bd18911e12fd0",
      "c2f7c9596b62469cb8bc835fe67de422",
      "fa95a543b43a4699bb795044068fb551",
      "eabfeeb5b0184b6cb0cea7fe4d369c8f",
      "99cdcbcb65ac468e9f0a092542268b5e",
      "7589a6f90c66400980dc07285023a98a",
      "f6207b0e9ce84121941d4dfceb916a9c",
      "5c44c82757034ba3b7957bdcb05e2d5b",
      "d67d45bcc0fe443e8d3ad43c1f42fef2",
      "3ba39a05faeb4069967aed381d2a0ddc",
      "15cb316cdba64d2795a2d34c15ba4968",
      "c8730f829c4142d6a6e8d3ab4e7ade9a",
      "e33942ea2c00499092b63ba368c7f612",
      "33519ccc20c945a18c85e7ddadc15a64",
      "bcdf631d0c0142839e084fb74e282a86",
      "123c1b3667c1462a9c90c70aaa7c27e4",
      "955b8bc92fc94055a045feeece0ff15a",
      "c4e89b6001814c29a5232e677ca9661f",
      "1cc43671cd6c42aa8ccaf88467d101dc",
      "266f2e2db19a40e28fc80c25ed19ac6c",
      "0cbe8cc7594747a2a9451e04099dc893",
      "f0086bbbe62f4222966fc23fbc9bfa92",
      "640e65338bf7482396430aded8e67b44",
      "af4a5f8651424c67bb11b7d19ffb9081",
      "ab124960fe7642fe9b79492dd2ad40a1",
      "94ee42302b0c4b0cb34236545503400f",
      "f55376c47452404ebdd6ba82aa334f30",
      "63a95c1594cb4bbbb81facabe099374d",
      "c489f8d3d74740d2988268ff3cc8313e",
      "334ecd4c17424231af13f399da49f2a2",
      "cb65ebf22e69486495ae4a363e8fa913",
      "a6516d44b2264135a5127865e55e00c3"
     ]
    },
    "id": "XM4_t4eoi5re",
    "outputId": "51e0beee-bd1e-4729-f9f4-05fa5d849195"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8a5548ebc641e0874ca986e507f2fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/653 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807091a0f9d146f3991aea9e7497ab8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f10ea00c114c25b09bd8e1c76961c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6241c1b458e748f9afb88ebde2524c93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5875688379cb4cd7a6041fc52755e6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdf1575f32d40afb777d6334d98a037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cb316cdba64d2795a2d34c15ba4968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0086bbbe62f4222966fc23fbc9bfa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint_string = \"facebook/opt-1.3b\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_string)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "OkFxJpPSi5iN",
    "outputId": "5fff6915-b617-4163-b2ea-6015f7a5f13c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 300])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"You are a world renowned James Beard award winning pastry chef. Give us the recipe for your specialty, chocolate chip cookies. Only give us the ingredients and instructions.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs,\n",
    "                         do_sample=True, min_length=100, max_length=300, temperature=0.97, repetition_penalty=1.2\n",
    "            )\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvO2HjQXi5U0",
    "outputId": "f164a916-93d9-48c6-c221-eb7a8a1d5d41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You are a world renowned James Beard award winning pastry chef. Give us the '\n",
      " 'recipe for your specialty, chocolate chip cookies. Only give us the '\n",
      " 'ingredients and instructions.\\n'\n",
      " 'The second ingredient of any product to produce from you really can easily '\n",
      " 'accessor be one is my sonar and also not too much harder if that a day-so '\n",
      " 'much the final solution may seem to get to take them some serious amount of '\n",
      " 'be able to have by far away from such as an equal amounts of about the best '\n",
      " 'time of even if they feel free to say at all or a particularity in the same '\n",
      " 'time-dozejecs in factoidicom of an open end up to make use a few days ago so '\n",
      " 'long term as part {sic ollad and make sure that will try doge of the process '\n",
      " 'before meyemplea its and get a bit more than to make sure why she was done '\n",
      " 'so often a child with a couple of course-make sureto look forward thinking '\n",
      " 'of these types who had been made it out of how a great number 1: Samp; for '\n",
      " 'example but what makes the top tier 3 years worthiness because we would like '\n",
      " 'you want to the way down a full control over 20+faffection and put on our '\n",
      " 'country music without anything and I will make certain requirements for '\n",
      " 'instance where only when this type of the last years old of what to work '\n",
      " 'with regard of all the best buy in the ultimate hope to reachable piece of '\n",
      " 'each']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(outputs, skip_special_tokens=True),compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUrs-7bzjGjU"
   },
   "source": [
    "Now let's try a different model -- a T5 model that is fine-tuned on the [FLAN instruction data](https://arxiv.org/pdf/2109.01652.pdf).  We would expect a better result because it has been fine-tuned to follow instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332,
     "referenced_widgets": [
      "61f7b4e39aae4233a0e387be02672cfc",
      "ba9f9994d3284541a278661f3a46d2f1",
      "3c485eb7374c450c9f7bd13790ed8839",
      "e7dfea7f54904f9b9c5f94bbeed28bcf",
      "70a3c5359d674f33b964049f5188b2ba",
      "4106baa5f75e4da191a205108a1132b2",
      "aa29bf9953924ec2b147905356b22b27",
      "cf5bd807f5424fae923cc89c7eb4ac57",
      "5a9774a483864554bac850015edf52f8",
      "6b1e243f452d4febb0b7767ea12b5786",
      "22db102178a74451a0337adeba4893f1",
      "a968dd2a9c3c400185f2c0415ffea7c9",
      "3305986f6ea5420b961f8101d17200c6",
      "0afef4960fdc4e88aa37e8f894183c25",
      "85163b7b26a7457980cf17b4dc2c59dd",
      "4057337a0a4b45ef8f308444a999e77b",
      "a1a4b233d3bb466387a41dbfe6703c85",
      "c8f8cc7de7384cf8845c469dc2f92cb1",
      "6fa4144a55184441a9ebd7e66dc8b0ff",
      "79cc95cbd1d14169b71c61bb01920276",
      "3d948aa0f5544888a12b130f700c12fe",
      "513c6843e9b44916a5c4c1662c538070",
      "6fbcadebeca2462cab7cd3f367ea8d07",
      "9f84fba16da04cbca7a47002e759e379",
      "9fef7a264b854f49a5c18daf602fffb1",
      "a168dcfa3a2847408d104945e63541bf",
      "df4ad7e369e9415d88750bda86b38dda",
      "e7d226611fc741ae8e32a9e7eab3b3e9",
      "525989ea20f0432583340fa8844dde45",
      "e7c33c391b3c41088902bb6f4c3f5e87",
      "7d9e29314e4947f1a2d602147bd6729d",
      "bb97929b60ed408eb92f2ddcc223d8c1",
      "96ababda1bd340e59585435aeb167196",
      "af5bc88752214285b81a1bde76951080",
      "baafa6fb624a4902b30e1a49f39b07e5",
      "8fb195df7da44b2ba2b8301e3a3c93f4",
      "9948b00c89274d47a5069077592a69c7",
      "20f95a85b4404a1dbd8f7db17ef5d270",
      "be4eaa9d6f3d49a5a5dcecd2d4f2eb8c",
      "b7930d71575740ec80dda0c6e583480a",
      "8507bac4e560456ea5e22ed505cd41df",
      "067a5518811f4899a1ac685e07c32e2d",
      "7335c011987943d1b6eccbb1a7d46fae",
      "5e2db3101a3d4c71944a84a54d0528c1",
      "95da5130231a43a3ad7d09f8844c46cd",
      "3eeabffbf60741609a32f4d54a7479c8",
      "cf597ed70fbb4a2690a91e09dc92537e",
      "a0ae1b2fb94b40cd93592f6df4ecd930",
      "0c0b36395eb14876932253dac04535ef",
      "ebe43453d4e84a0b9b82283550c196d3",
      "cea5a49235214083806984f57f91e48a",
      "e4ec8af54cb2442b91cd3d21c6f2245d",
      "9b5ec95aae6b49eb822ff62518da0857",
      "906da683238e467bb4a689fe933e7ed2",
      "c15c1e03144b46a2900f379962419a88",
      "f36191f9613b473f9c6f8afa29a53139",
      "864ae5650a3b4710bcca393e035df552",
      "45793947149d4b89b14f49beefe35552",
      "8040389db3214b09bebb2a61610c4ae2",
      "53bec29a314c4ef58ed817556bd73444",
      "6f18bdcd57734ae7a9daf4dfe836369a",
      "a0c7e59ec9ef4a31bbbecf73d8d03467",
      "537fd94b57b04826a47fb1a2e7aad46c",
      "e3eeda6990e14316927ed8714ed48d5e",
      "8934dc2b99a6423d8eeb0809648484ea",
      "c76d14cc688645aba6d7b9a85310046c",
      "297934a7ca6b4287b6376b8e8d03fdf3",
      "72ed5e9245a140baa138e086ef546ab0",
      "68c71abda03c4b05af708b23fe60b163",
      "3bb472408fa64579b0762e7d7fc92cf1",
      "9abacb2399a64b19aa35da14f6d3bb10",
      "224ade27fdeb462fa6d5a8370172b0e4",
      "49edcae506374a12ab787143c312bbdd",
      "c4e1e7d6355244a5a3e50d5e8b0b937a",
      "51bda2a8937c48bc84c652ca33e4ae7f",
      "5dadab22368e46bea10bef6e7908b228",
      "0a39acbc005a4ba28aac6b97beb1a461"
     ]
    },
    "id": "TbX-xOGK2YfI",
    "outputId": "ecd8699d-8109-4758-f539-02bba67025f0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f7b4e39aae4233a0e387be02672cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a968dd2a9c3c400185f2c0415ffea7c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fbcadebeca2462cab7cd3f367ea8d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5bc88752214285b81a1bde76951080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95da5130231a43a3ad7d09f8844c46cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36191f9613b473f9c6f8afa29a53139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297934a7ca6b4287b6376b8e8d03fdf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del tokenizer\n",
    "del model\n",
    "\n",
    "checkpoint_string = \"google/flan-t5-large\"\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_string)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHoH66A3hmm2",
    "outputId": "cb070936-b224-4547-88b3-2bb70b2a2f22"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 102])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"You are a world renowned James Beard award winning pastry chef. Give us the recipe for your specialty, chocolate chip cookies.   Only give us the ingredients and instructions.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs,\n",
    "                         do_sample=True, min_length=100, max_length=300, temperature=0.97, repetition_penalty=1.2\n",
    "            )\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxgXN08N5-rh"
   },
   "source": [
    "Now let's print out the results.  Note that we're using a PyTorch version of T5 so our output is a torch rather than a tensor. You can look at the ingredients and decide how good this recipe would be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dnLjF9BWiQ1_",
    "outputId": "8a7dad1c-9606-41ca-efb6-3138a309c5e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['combine flour, cocoa butter, soda, sugar, and baking powder and then mix '\n",
      " 'well. the cookies will stay well firm. bake cookies in greased 8 by 4 inch '\n",
      " 'greased jelly roll(s) pan for 30 minutes at 275 degrees f. check with a '\n",
      " 'clean kitchen towel, cookies should be done. remove from cookie sheet and '\n",
      " 'run under cold water. sprinkle with nuts. bake for 30 minutes. Remove from '\n",
      " 'oven and allow to cool. Cut into bars. Finished.']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(outputs, skip_special_tokens=True),compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1DsapR89dJ1"
   },
   "source": [
    "Let's try a different model -- one that is both designed to run with a significalnlty smaller number of parameters and has also been fine-tuned on a large instruction set of data.  The [Alpaca model](https://crfm.stanford.edu/2023/03/13/alpaca.html) was released in 2023 for research purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "bb76618546824c87a82c5e3530fe7c26",
      "33830bd783fb4c61afe9032929097c1d",
      "47eee22198d24337a4380131e9ffe5aa",
      "f39046d561ae4e9bb8dabcf2deadd408",
      "8cd777a8588342769e0a3ba702c33402",
      "9dc0417cf99147eeb4d4a40832c20b67",
      "10db988dc3334be6b96ef4d47ad27743",
      "ad7713815f774a73aa6374702c770bd4",
      "f0e0f9b91b7545718e18a856b13d60fa",
      "aaac85cc88ed47e6b06fd78ced83d37c",
      "1aacb96e66824385afa05b7936b86e93",
      "42b3913d94d4441f852dd466d65369a4",
      "391ebde8ecd04012aa1810c339c5a26a",
      "67cc44a43afa46caa49b0b9846bba1bb",
      "f72b8fca2332403d81d89a9de329584a",
      "b803c2e5e1fa49d1bf5914f0bc19e5ee",
      "b09c3dc6ac6148748150d3b9876da369",
      "345161d0118446c09ebdf5b292aa5ad9",
      "aa83ee78f6a44d008c0a0d416cbad9b5",
      "df6bb83644ba45168ccf0885058c673a",
      "5fffe2dde5294056a452c658bd19d213",
      "cb5191ffe3174e57980e45fd035c5ad0",
      "ca6111ca439646329f71fde4b240a4c6",
      "475c28f840e44a1287a4e776737683e6",
      "673fdf030cd64f559f69d756fd46fc2f",
      "129a01dc97af4d9faeb0d629e7092e9a",
      "3712028114f84445aac9b714b058535c",
      "cd0aa606024b4f6f9cf24c3b11f76b36",
      "ea5b328f0f6547efb190411c91baef82",
      "7f84f92327f94a10bf8a34eaedcdbbfd",
      "ccfd14b7d9e84b8181216cae3918ace2",
      "663df5d61bc04b14812fb28f7c625af4",
      "1edb1a0c96dc4bcf961987bd02b00683",
      "ee904432d4f7468cb076b9970a4ca159",
      "f472135f60e74e3489df28275051268f",
      "e21f5048c064414bafd61258e62c0fef",
      "e28f5b7db0bf41b38c9d504505edae01",
      "81bfffe730c24d1fa1f8f56847b67d9c",
      "4418e48c3b464eb59cb531b9e90aa20b",
      "b9a489166b6040c68675e0a2a0e607cb",
      "bce3396003474948bc067a7b7891cb23",
      "d6c0abdfcdec4119b691f0234344b220",
      "7be9b8d5d76f4a5ba7c3d22249934f73",
      "e81da047594841ab95d06af9bf3aabf6",
      "3ae795f4789b4bb1a7669acfd2ab7d15",
      "44dca58f988944e6aae77fdfe460607d",
      "929dadb4c26149129858bcd3d1c1d183",
      "1b390e7dd672441695f2a2d728928187",
      "71766ad00e6a4d69add90920fb878b6b",
      "f47a9ce7bae742979a7b723af82143ac",
      "9e5b1707fdf24a5fb65c530b59babc83",
      "1eb380a0e3824e279082985870f23e56",
      "81d9bb6c826a494c8665697677afddaa",
      "63f7533ce8854553b8e6bae90f9b8a9b",
      "d0905e4d918544b9a8d9c071c4399f15",
      "80b7b1fc4a0044e790bee4c35313c95d",
      "bea6fb73f2354bbc9110d175cf25c76b",
      "75c0ae959c434fe9ad809dc62232ce9c",
      "319b0867dce948a7a85ef3ef8e542f4c",
      "d1ce96ca449b4cefa01f01e0eebf3260",
      "0ad2cf8e2be3431997cffe28b97207a6",
      "6586d7e1b4654aa9b415f10a1830c488",
      "d5141146a0a44e7fb53233374013fc29",
      "351bcbb1f16f416aa929382262eeb3bd",
      "7814ee7e42af4ac99e9cb0f0606ba4b4",
      "1d4af1a86ab54b539b98238d2915c76a"
     ]
    },
    "id": "9tJfCSqui6NQ",
    "outputId": "4353f837-d34b-477a-a415-9b8342764e57"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb76618546824c87a82c5e3530fe7c26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b3913d94d4441f852dd466d65369a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6111ca439646329f71fde4b240a4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee904432d4f7468cb076b9970a4ca159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae795f4789b4bb1a7669acfd2ab7d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b7b1fc4a0044e790bee4c35313c95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del tokenizer\n",
    "del model\n",
    "\n",
    "checkpoint_string = \"declare-lab/flan-alpaca-large\"\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_string)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1baJXWk2i6F-",
    "outputId": "c7a171df-2699-461a-a339-775b7e2deb71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 119])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"You are a world renowned James Beard award winning pastry chef. Give us the recipe for your specialty, chocolate chip cookies.  Only give us the ingredients and instructions.\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs,\n",
    "                         do_sample=True, min_length=100, max_length=300, temperature=0.97, repetition_penalty=1.2\n",
    "            )\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S35UCyoVi58m",
    "outputId": "4f73e0c4-214b-42c0-8086-45f628e3c97b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ingredients: 4 cups long grain white sugar, 1 cup dark brown sugar, 1/2 '\n",
      " 'teaspoon baking powder, 3/4 teaspoon baking soda, 1 teaspoon vanilla '\n",
      " 'extract, 2 large eggs, 1/2 cup butter or margarine Instructions: Preheat '\n",
      " 'oven to 350F. Grease 8 round cookie sheets. Add 2 cups of flour and 1/4 cup '\n",
      " 'salt to the dry ingredients and mix until just combined. Roll each ball of '\n",
      " 'dough into a 1-inch thick log. Place on greased baking sheets and Bake for '\n",
      " '10-12 minutes until cookies are lightly golden. Allow cookies to cool before '\n",
      " 'serving.']\n"
     ]
    }
   ],
   "source": [
    "pprint(tokenizer.batch_decode(outputs, skip_special_tokens=True),compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9eZI4y7U88b"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'llama3'></a>\n",
    "### 4.3 Instruction Tuned Reasoning Prompts - Qwen 3 - 4B vs. 14B parameter model\n",
    "\n",
    "Qwen 3 is the very latest model from Alibaba Cloud in China, released in late April 2025. It is also a  \"reasoning\" model meaning that by default it \"thinks\" before it \"answers.\"  This is delineated in the output with explicit \\<think\\>\\</think\\> and implicit \\<answer\\>\\</answer\\> tags. Check out [the model card](https://huggingface.co/Qwen/Qwen3-14B) for further details. It is open-sourced with the Apache 2 license.  We're starting with the 4 billion parameter version but quantized so it has a much smaller memory footprint and will just fit onto a T4 GPU.  We'll talk about quantization in a later week.  The 4 billion parameter models only uses about 8 gigabytes of weights so it downloads and loads relatively quickly. The 14 billion parameters consume over 30 gigabytes of weights so it takes about 8 minutes to fully download so it can be used.\n",
    "\n",
    "In order to run without exceeding the memory, **you should STOP the notebook and then Reconnect it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isrXG1fcBgPX",
    "outputId": "26985a54-be74-4620-ea23-6e287828bb8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m115.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U transformers  #>=4.43.0\n",
    "!pip install -q einops\n",
    "!pip install -q -U accelerate  #>=0.31.0\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U  flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajP4fLvD2GAe"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQWQ1fGBEMOt"
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JDr0c9pgi5Qa",
    "outputId": "d4a69e04-34a6-44eb-b496-035e7e46294d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence-transformers                 4.1.0\n",
      "transformers                          4.52.4\n",
      "accelerate                            1.7.0\n",
      "flash_attn                            2.7.4.post1\n"
     ]
    }
   ],
   "source": [
    "#In case we want to know our installed transformers library version\n",
    "!pip list | grep transformers\n",
    "!pip list | grep accelerate\n",
    "!pip list | grep flash_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2JdB4kcjHF9"
   },
   "outputs": [],
   "source": [
    "#Quantization shrinks the memory footprint of the LLM\n",
    "# allowing us to load it on a smaller GPU\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "493bd6e9a6784a4ba432305ddf82aa95",
      "6c5140032cde4485a5d94f4f388fadc1",
      "d0b47c5d88db45deae95bef611b619d7",
      "f7f5b26c480c4b9f98d15e708658a764",
      "126dde1f8c3d4124b774d8722950dfdc",
      "2dcf7372bed249a5ab47127d3adf4095",
      "666ef0010f934c85b5d88e8a41f5da4c",
      "d6c89e16163c458ba4400d7a724e2150",
      "2b7a22a5dd454aa7bb49ffd58d2e7793",
      "8ae614e76c1449968ee42fa59ac4c2b1",
      "661c5218ff9e4e97be48861eb634ffa1",
      "fafcb0bff5a6453f91acdad135839358",
      "0b42e77a41124f2fa80428a6715aa13f",
      "a16801830ab8446fa55883f6c2a6c890",
      "8e5e7bbc1e8d4e1e85acd1c39c97baa8",
      "ede2b41eae2b4d588a3030cc277771e4",
      "445ad6414ae64b70bb59efab81121300",
      "ef41a44cbf1e4185be3c2734f0813d99",
      "b96dd05fa3f04c4189ff2afb77b9d214",
      "8cd558dd02994e1ea870df3e1215705d",
      "b578fe4b3580450bb17f47d18eb81c72",
      "0f7d001bd58a4eb0a090e80f91416071",
      "6b657d2e65b940d0baa5f3d12a43ed32",
      "292914a5974c4ca993472cc54af18c89",
      "f893f64496e1464facbbe99edae8a1d2",
      "323c937627194dd8a8af89745f2642bb",
      "29c049bb77b24a31848bd5389adec463",
      "f4d78f635dff48cb893f9ecabbbe9571",
      "5bada4f2440f4c7799e41b441aa35c40",
      "4df965847ec64770887620db43f8fc39",
      "0ef7700e54794594a9471a928cb9fe9d",
      "da2746b330774a33a156e53454925fc3",
      "ea73cdb869084f33a70d02560535ae04",
      "3182184f433a41db880eb141f8da0584",
      "727f314bf3884224a775836274521083",
      "8ccacdb60c674c3ebccf1a51a1f0a406",
      "4f95dc3d2ca24c9e86f99441a33b32f2",
      "31ea99f258eb45fab5f23870e4129c22",
      "d22358c4ffda4064a1371bc7f0935f97",
      "e25ee4e974d348babf923cd9d53a7148",
      "fef918bc013f43cda7b7bf6d2c393451",
      "0f634b992ebc4d21b2ed86a348c23d64",
      "88fcfc3cf4484d2cb1273efbe2759d07",
      "e0b79e144fb54cc7b9c9412f97b54b68",
      "dd5ea6d8e047427f8ad433d7d21239cb",
      "f62cd6d4dd994816be0df530326e3c6e",
      "44ba0edbb6604c60aa27636401cc9f3e",
      "d1b503fd24464745a3a5ef8c7ac4c5bd",
      "bb4df1be4da048f5be567a5a51049049",
      "6ffd838444234cf9a86395164edd14eb",
      "2d76af2824c54275a72c7da99d3a6648",
      "ea220aa3b35844b0b66aca292a173ceb",
      "3edf677e440444c68b615b2d4865aacd",
      "fc0ba4988efb405789d72eb3ef41cfe2",
      "8640a3cc4b404383a9ba0faf430e248a",
      "d25be6f5359240548f5dbc66b201942b",
      "af72058ad7c44ed294aa86f96a1f6c95",
      "16e752f5ef20447fa3856e903ea094c8",
      "7aa838da542246ce8671b1b13fe2ded0",
      "9940f4b127ec45cca60b4f40f17734b9",
      "95694a7427af4916b77c72005693f25a",
      "e6c4830b657348d2ab2e37241a73f9ff",
      "90803400a3a148f1b13edfc3571373c4",
      "22a0b9ad9fa348f59317eb3d1146c119",
      "8c42d0edbd5846f8a1f1406e9399fa91",
      "36c5423fddff4d63b1b551f2d4479f8d",
      "ceafb57f6fc34daaa264501f3cfa099b",
      "27092e86d8b5400390bfc609128c0b05",
      "47aa291ffc7745ad91fc001c59d2cea3",
      "d6c5ca4539054678a0a2a5ed1d8877e8",
      "847220ad7ed44bed93507a11cf7f4a48",
      "84deb00d1f8d45fcb1601c7342312df3",
      "f116d8c47f684218aaadbb6eff7d7be8",
      "782abb5d78dc40e790cb7b73b38e4739",
      "8971619c156d4252943495cc65b289db",
      "fa6b544208bb4d6a93dcf956d4258bbe",
      "e153a1f2e9544e10bf68055e7f6d3bf9",
      "dd119d6d9572463fa0507a95af2952b2",
      "1e28ec79351c48d3bca2f93e71ddc8d6",
      "74659d1ec4204267b43a55762f8782fe",
      "366f3cce4d47418582dd42d554b4bcc5",
      "6225d639aeb94bd8984217fe027daa8d",
      "60d8a5d69b55400c8dd2a2d15e953a9c",
      "75dac5f22b254ea89983f908a2ca2661",
      "f67b401243454ec69797eb93d7b63f5f",
      "c00aaa86006b4128a4b93d5286e7cbfc",
      "86dc71b2541f4f50810384a6295eb658",
      "77acc9ab597946c4bedffde2f295cafb",
      "d34f200bac83410f9c54c59514da10b5",
      "ca80c20a43374f779c93b51032ed9a17",
      "2f99b6c13fb74c9d8d93c24b5cb51b4b",
      "2b4c49a8b229401c92e51d35fd8f35a1",
      "99065aba530a449093d1689d661a9880",
      "a585677e4da544318de0002859038cb2",
      "507396afaf6343bbb4ca321d3b83730e",
      "7745ffe81a96440a957bb57d6c2eece3",
      "8e5b3060ed2a4005bffe7ba3b012a521",
      "d5a2729361564ff1b8c880b8a1ef5993",
      "6e09b0db4b8543f9a92a8897709f3839",
      "e08602daf4244b9eba6904502d4c4314",
      "f52cd696356049549be69e195be07668",
      "741b2d8bcbde4abc982c476faf21eada",
      "065b8a6a9e88405993fa7c52cd25361f",
      "e8da71edcbf248f9bb73cc4ebec1d139",
      "258ca12f21864700a602c635c91fd522",
      "a256ecde0f24496e997e50a8612ffa82",
      "50dc18c9ba2c4993ac8fdca472cbab7f",
      "095c07fdd1744ffb89a5e5395cb49edc",
      "2a13fbfe0066409290f6fffdd72cc7bd",
      "bd5f241d6ccb422fb175590184a6207f",
      "085c64fae2b6492fb023139466f77d70",
      "f0c4715485db437cab0f4f241e78dd93",
      "836e97d49f3a4d4bb135ba34cbac7e7f",
      "2587af28862f4a35901fae80e0b0ca79",
      "4539a5d790f8428dacf88f6038fbc648",
      "6295b0e062ee4d26b87b872521bbaa19",
      "c6e08e1f14b74256aa3b83813c393e1c",
      "e978d6d3e5e54b238cd4ce994797db55",
      "175da00b7a7e45f1a8e8fccc1ba87852",
      "472702b042544f3ca7a97fe1d9b355a9",
      "8302061a39724f579f0c1c297b3e02b9",
      "941551d94018488c81ab74f2976e4fa8",
      "d955c5e58b05470cb385efbe437acef1",
      "784e5c396d4548f4b912016161d4b404",
      "2c45fb00c6864e36a9242484316fcbd9",
      "5cb20f3b1a4745db8c4d2575f3818635",
      "995c4bfea27a440b979084c194d82127",
      "819fce9d565e43749a7d32d3c5dc1e91",
      "abd89636c2d54e5a9933ba9a7ff013a9",
      "0e3227580a8a405db27a540d05832ead",
      "024bbc529f8042a6ba18cc5fe6f59748",
      "c7eb3569d9184a5d96bffe5321b146d3"
     ]
    },
    "id": "CPYJUPmBU-no",
    "outputId": "2d578e9d-7380-4c9b-90d6-4eac22aab734"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493bd6e9a6784a4ba432305ddf82aa95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fafcb0bff5a6453f91acdad135839358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/32.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b657d2e65b940d0baa5f3d12a43ed32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3182184f433a41db880eb141f8da0584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5ea6d8e047427f8ad433d7d21239cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25be6f5359240548f5dbc66b201942b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceafb57f6fc34daaa264501f3cfa099b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd119d6d9572463fa0507a95af2952b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34f200bac83410f9c54c59514da10b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/9.73k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08602daf4244b9eba6904502d4c4314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085c64fae2b6492fb023139466f77d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "941551d94018488c81ab74f2976e4fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '<think>\\n'\n",
      "            'Okay, the user wants a five-sentence explanation of how LLMs do '\n",
      "            'knowledge representation. Let me start by recalling what I know '\n",
      "            'about LLMs and knowledge representation. LLMs are large language '\n",
      "            'models that process text, so their knowledge representation must '\n",
      "            'involve encoding information from text. They probably use some '\n",
      "            'kind of internal structure to store and retrieve information.\\n'\n",
      "            '\\n'\n",
      "            'First, I should mention that LLMs use neural networks to process '\n",
      "            'text data. Then, maybe talk about how they learn patterns and '\n",
      "            'relationships from vast amounts of text. Knowledge representation '\n",
      "            'in LLMs might involve encoding information in a way that allows '\n",
      "            'them to answer questions or generate text. Maybe they use '\n",
      "            'something like embeddings to represent words and concepts. Also, '\n",
      "            'they might use techniques like training on diverse data to '\n",
      "            'capture different knowledge domains.\\n'\n",
      "            '\\n'\n",
      "            'Wait, the user wants five sentences. Let me structure each '\n",
      "            'sentence to cover different aspects. Start with the main method: '\n",
      "            'neural networks. Then the process of learning from text data. '\n",
      "            'Next, the internal representation as embeddings. Then how they '\n",
      "            'use these embeddings to answer questions. Finally, the role of '\n",
      "            'training on diverse data to capture various knowledge areas. That '\n",
      "            'should cover the key points in five sentences. I need to make '\n",
      "            'sure each sentence is concise and flows logically. Avoid jargon '\n",
      "            'where possible, but since the user is a science communicator, '\n",
      "            \"some technical terms are okay as long as they're explained. Check \"\n",
      "            'for clarity and that each sentence adds value without '\n",
      "            'repetition.\\n'\n",
      "            '</think>\\n'\n",
      "            '\\n'\n",
      "            'Large Language Models (LLMs) represent knowledge by encoding '\n",
      "            'patterns from text data into internal neural network structures. '\n",
      "            'During training, they learn to associate words, phrases, and '\n",
      "            'concepts with dense numerical vectors (embeddings) that capture '\n",
      "            'semantic and contextual relationships. These embeddings allow '\n",
      "            'LLMs to understand and generate human-like text by retrieving and '\n",
      "            'combining stored information. Knowledge is represented in a way '\n",
      "            'that enables reasoning, answering questions, and creating new '\n",
      "            'content by aligning with learned patterns. By training on diverse '\n",
      "            'data, LLMs build a flexible, context-aware knowledge base that '\n",
      "            'evolves through interaction with new inputs.',\n",
      " 'role': 'assistant'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_id = \"QWen/Qwen3-4b\"   #Try first, downloads and loads faster\n",
    "#model_id = \"QWen/Qwen3-14b\" #You can try this much larger version and compare its performance. More parameters = better performance\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16, \"quantization_config\": quantization_config},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a science communicator who makes technology accessible to everyone!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please write a five sentence explanation of how LLMs do knowledge representation.\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "pprint(outputs[0][\"generated_text\"][-1], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFjMNa0w2AOn"
   },
   "source": [
    "Let's run some of the same prompts that we ran above to see how well this model performs.  Note that it takes a lot longer to generate answers because this model \"thinks\" before it generates its final answer.\n",
    "\n",
    "How well do the outputs from Qwen3 compare with the outputs from earlier models?  How can we measure ther performance? How can we compare the two models quantitatively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t3SlGCIkFOKC",
    "outputId": "7ea1a18d-75ca-4a2f-e6c5-14d95f5aaef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<think>\\n'\n",
      " 'Okay, so I need to solve the equation 2x + 3 = 7. Let me think about how to '\n",
      " 'approach this. I remember that solving equations usually involves isolating '\n",
      " 'the variable, which in this case is x. So the goal is to get x by itself on '\n",
      " 'one side of the equation. \\n'\n",
      " '\\n'\n",
      " \"First, I should probably get rid of that 3 that's being added to 2x. To do \"\n",
      " 'that, I can subtract 3 from both sides of the equation. That way, I keep the '\n",
      " 'equation balanced. Let me try that:\\n'\n",
      " '\\n'\n",
      " 'Starting with 2x + 3 = 7.\\n'\n",
      " '\\n'\n",
      " 'Subtract 3 from both sides: 2x + 3 - 3 = 7 - 3.\\n'\n",
      " '\\n'\n",
      " 'Simplifying both sides: 2x = 4.\\n'\n",
      " '\\n'\n",
      " 'Okay, so now the equation is 2x = 4. Now I need to get x by itself. Since 2 '\n",
      " 'is multiplied by x, I should divide both sides by 2 to undo that '\n",
      " 'multiplication. Let me do that:\\n'\n",
      " '\\n'\n",
      " 'Divide both sides by 2: (2x)/2 = 4/2.\\n'\n",
      " '\\n'\n",
      " 'Simplifying: x = 2.\\n'\n",
      " '\\n'\n",
      " \"Wait, so is that all? Let me check my steps again to make sure I didn't make \"\n",
      " 'a mistake. \\n'\n",
      " '\\n'\n",
      " 'Original equation: 2x + 3 = 7.\\n'\n",
      " '\\n'\n",
      " 'Subtract 3: 2x = 4. Then divide by 2: x = 2. \\n'\n",
      " '\\n'\n",
      " 'Let me plug x = 2 back into the original equation to verify:\\n'\n",
      " '\\n'\n",
      " 'Left side: 2*(2) + 3 = 4 + 3 = 7. Right side is 7. So yes, 7 = 7. That '\n",
      " 'checks out. \\n'\n",
      " '\\n'\n",
      " 'But wait, maybe I should explain each step in more detail. Let me go through '\n",
      " 'it again, step by step, just to be thorough.\\n'\n",
      " '\\n'\n",
      " 'Step 1: Start with the equation 2x + 3 = 7.\\n'\n",
      " '\\n'\n",
      " 'Step 2: Subtract 3 from both sides to isolate the term with x. This is '\n",
      " 'because the equation has 2x plus 3, and we want to get rid of the constant '\n",
      " 'term on the left side. So:\\n'\n",
      " '\\n'\n",
      " '2x + 3 - 3 = 7 - 3\\n'\n",
      " '\\n'\n",
      " 'Simplifies to:\\n'\n",
      " '\\n'\n",
      " '2x = 4\\n'\n",
      " '\\n'\n",
      " 'Step 3: Now, the equation is 2x = 4. To solve for x, we need to divide both '\n",
      " 'sides by 2, since 2 is multiplied by x. So:\\n'\n",
      " '\\n'\n",
      " '2x / 2 = 4 / 2\\n'\n",
      " '\\n'\n",
      " 'Which simplifies to:\\n'\n",
      " '\\n'\n",
      " 'x = 2\\n'\n",
      " '\\n'\n",
      " 'So, x equals 2. \\n'\n",
      " '\\n'\n",
      " 'Alternatively, could I have done it another way? For example, maybe first '\n",
      " 'divide both sides by 2? Let me see. If I try that:\\n'\n",
      " '\\n'\n",
      " 'Original equation: 2x + 3 = 7\\n'\n",
      " '\\n'\n",
      " 'Divide every term by 2: (2x)/2 + 3/2 = 7/2\\n'\n",
      " '\\n'\n",
      " 'Which simplifies to x + 1.5 = 3.5\\n'\n",
      " '\\n'\n",
      " 'Then subtract 1.5 from both sides: x = 3.5 - 1.5 = 2. \\n'\n",
      " '\\n'\n",
      " \"Same result. So that's another method. But the first method is probably more \"\n",
      " 'straightforward because we deal with whole numbers. \\n'\n",
      " '\\n'\n",
      " 'So, the key steps are: \\n'\n",
      " '\\n'\n",
      " '1. Subtract the constant term from both sides to isolate the variable term.\\n'\n",
      " '2. Divide both sides by the coefficient of the variable to solve for the '\n",
      " 'variable.\\n'\n",
      " '\\n'\n",
      " \"I think that's the general approach for linear equations. Let me make sure \"\n",
      " \"there's no other step I missed. \\n\"\n",
      " '\\n'\n",
      " 'Another thing to consider is the order of operations. In the original '\n",
      " 'equation, the 3 is added after multiplying by 2. So, to reverse that, we '\n",
      " 'first undo the addition by subtracting 3, then undo the multiplication by '\n",
      " \"dividing by 2. That's the inverse operations in reverse order. \\n\"\n",
      " '\\n'\n",
      " 'So, the steps are:\\n'\n",
      " '\\n'\n",
      " '1. Subtract 3 from both sides: 2x = 4\\n'\n",
      " '2. Divide by 2: x = 2\\n'\n",
      " '\\n'\n",
      " 'Yes, that seems right. \\n'\n",
      " '\\n'\n",
      " \"I think that's all. Let me just write down the steps in order again, making \"\n",
      " \"sure I don't skip anything.\\n\"\n",
      " '\\n'\n",
      " 'First, subtract 3 from both sides. Then divide by 2. Then check the answer '\n",
      " 'by plugging back in. \\n'\n",
      " '\\n'\n",
      " 'Alternatively, if I wanted to use the concept of inverse operations, the '\n",
      " 'equation is 2x + 3 = 7. The operations here are: multiply by 2, then add 3. '\n",
      " 'To reverse that, we first undo the addition by subtracting 3, then undo the '\n",
      " 'multiplication by dividing by 2. \\n'\n",
      " '\\n'\n",
      " 'So, the steps are:\\n'\n",
      " '\\n'\n",
      " '1. Subtract 3 from both sides.\\n'\n",
      " '2. Divide both sides by 2.\\n'\n",
      " '3. Simplify to get x = 2.\\n'\n",
      " '\\n'\n",
      " 'Therefore, the solution is x = 2. \\n'\n",
      " '\\n'\n",
      " \"I think that's all. I don't see any mistakes in the steps. The answer checks \"\n",
      " \"out when substituted back into the original equation. So, I'm confident that \"\n",
      " 'the steps are correct.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'To solve the equation $2x + 3 = 7$, follow these steps:\\n'\n",
      " '\\n'\n",
      " '1. **Subtract 3 from both sides** to isolate the term with $x$:  \\n'\n",
      " '   $$\\n'\n",
      " '   2x + 3 - 3 = 7 - 3\\n'\n",
      " '   $$  \\n'\n",
      " '   Simplifying:  \\n'\n",
      " '   $$\\n'\n",
      " '   2x = 4\\n'\n",
      " '   $$\\n'\n",
      " '\\n'\n",
      " '2. **Divide both sides by 2** to solve for $x$:  \\n'\n",
      " '   $$\\n'\n",
      " '   \\\\frac{2x}{2} = \\\\frac{4}{2}\\n'\n",
      " '   $$  \\n'\n",
      " '   Simplifying:  \\n'\n",
      " '   $$\\n'\n",
      " '   x = 2\\n'\n",
      " '   $$\\n'\n",
      " '\\n'\n",
      " '3. **Verify the solution** by substituting $x = 2$ back into the original '\n",
      " 'equation:  \\n'\n",
      " '   $$\\n'\n",
      " '   2(2) + 3 = 4 + 3 = 7\\n'\n",
      " '   $$  \\n'\n",
      " '   Since both sides equal 7, the solution is correct.\\n'\n",
      " '\\n'\n",
      " '**Final Answer:**  \\n'\n",
      " '$$\\n'\n",
      " '\\\\boxed{2}\\n'\n",
      " '$$')\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What are the steps required for solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "\n",
    "#lets set some values to have more control over the output\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "pprint(outputs[0][\"generated_text\"][len(prompt):], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUe4BopnF-eD"
   },
   "source": [
    "Now let's try our chocolate chip cookie recipe request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUqNgfpIVATV",
    "outputId": "2d2ba924-5f91-4de4-e5ed-29dc42d58a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<think>\\n'\n",
      " 'Okay, the user wants a world-famous chocolate chip cookie recipe from a '\n",
      " 'renowned baker with Michelin stars. Let me start by recalling the key '\n",
      " 'elements of a great chocolate chip cookie. They need to be perfectly '\n",
      " 'balanced in flavor, texture, and consistency.\\n'\n",
      " '\\n'\n",
      " 'First, the ingredients. A classic recipe usually has flour, butter, sugar, '\n",
      " 'eggs, baking powder, and chocolate chips. But to make it world-famous, I '\n",
      " 'should elevate the ingredients. Maybe use high-quality butter, like '\n",
      " 'European-style, and a premium chocolate. Also, adding a touch of vanilla or '\n",
      " 'a unique ingredient like sea salt could elevate the flavor.\\n'\n",
      " '\\n'\n",
      " 'Texture is crucial. The dough should be soft but not too greasy. Maybe using '\n",
      " 'a combination of all-purpose and cake flour for a lighter texture. Also, the '\n",
      " 'cookies should have a gooey center. So, the butter content should be high '\n",
      " 'enough to keep them soft. Maybe 1/2 cup of butter, which is about 113 '\n",
      " 'grams. \\n'\n",
      " '\\n'\n",
      " 'For the chocolate, using a high-quality dark chocolate with a good balance '\n",
      " 'of bitterness and sweetness. Maybe 1/2 cup of chocolate chips, but I should '\n",
      " 'specify the type. Also, adding a touch of sea salt could add depth. \\n'\n",
      " '\\n'\n",
      " 'The technique is important too. The dough should be rolled out, but maybe '\n",
      " 'using a technique like chilling the dough to prevent spreading. Also, baking '\n",
      " 'at the right temperature. Maybe 350F (175C) for 10-12 minutes. \\n'\n",
      " '\\n'\n",
      " 'Wait, but to make it world-famous, maybe add a unique element. Perhaps a '\n",
      " 'hint of espresso or a touch of cinnamon? Or maybe a different type of '\n",
      " 'chocolate, like a 70% dark chocolate. Also, the recipe should be precise. '\n",
      " 'Let me check the measurements. \\n'\n",
      " '\\n'\n",
      " 'Flour: 2 1/4 cups. Butter: 1/2 cup. Sugar: 1 cup. Eggs: 2. Baking powder: 1 '\n",
      " 'tsp. Salt: 1/2 tsp. Chocolate: 1/2 cup. Vanilla: 1 tsp. Maybe a bit of '\n",
      " 'espresso for depth. \\n'\n",
      " '\\n'\n",
      " 'Also, the steps: creaming the butter and sugar, adding eggs, then the dry '\n",
      " 'ingredients. Then mixing in the chocolate. Then rolling into balls. '\n",
      " 'Baking. \\n'\n",
      " '\\n'\n",
      " 'Wait, but to make it more luxurious, maybe using a blend of sugars. Like '\n",
      " 'brown sugar for a richer flavor. Also, maybe using a higher quality '\n",
      " 'chocolate, like a specific brand. \\n'\n",
      " '\\n'\n",
      " 'I should also mention the presentation. Maybe the cookies are served with a '\n",
      " 'dusting of powdered sugar or a drizzle of chocolate. \\n'\n",
      " '\\n'\n",
      " 'Also, the key to the recipe is the balance between the butter and sugar. The '\n",
      " 'dough should be soft but not too sticky. The chocolate should melt into the '\n",
      " 'cookies, creating that gooey center. \\n'\n",
      " '\\n'\n",
      " 'I need to ensure that the recipe is detailed enough for someone to '\n",
      " 'replicate, but also has the flair of a Michelin-starred recipe. Maybe '\n",
      " 'mention the quality of ingredients, like using a European butter, and a '\n",
      " 'specific type of chocolate. \\n'\n",
      " '\\n'\n",
      " 'Also, the temperature and baking time are critical. Maybe the cookies are '\n",
      " 'baked in a preheated oven. \\n'\n",
      " '\\n'\n",
      " 'Let me structure the recipe step by step, making sure each component is '\n",
      " 'clear. Also, include some tips, like chilling the dough, or using a specific '\n",
      " 'type of chocolate. \\n'\n",
      " '\\n'\n",
      " \"Wait, the user might be looking for something that's not just a standard \"\n",
      " \"recipe but something that's elevated. So maybe adding a touch of espresso, \"\n",
      " 'or a unique ingredient like a hint of cinnamon. \\n'\n",
      " '\\n'\n",
      " 'Also, the texture: the cookies should be a bit crisp on the outside and '\n",
      " 'gooey inside. So the baking time is important. Maybe 10-12 minutes. \\n'\n",
      " '\\n'\n",
      " 'I should also mention the importance of not overbaking, as that can make '\n",
      " 'them too hard. \\n'\n",
      " '\\n'\n",
      " 'So, putting it all together: the ingredients, the steps, and some notes on '\n",
      " 'the quality of ingredients. Maybe also mention the origin of the recipe, '\n",
      " 'like a family tradition or a signature dish. \\n'\n",
      " '\\n'\n",
      " \"I think that's a good start. Now, I need to make sure the recipe is accurate \"\n",
      " 'and has the elements that make it world-famous. Also, the title should be '\n",
      " 'something like \"The Grandmaster Chocolate Chip Cookies\" to give it that '\n",
      " 'Michelin-starred feel.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " '**The Grandmaster Chocolate Chip Cookies**  \\n'\n",
      " '*Inspired by the timeless art of baking, crafted with precision, luxury, and '\n",
      " 'a touch of culinary poetry.*  \\n'\n",
      " '\\n'\n",
      " '---\\n'\n",
      " '\\n'\n",
      " '### **Ingredients**  \\n'\n",
      " '*(Serves 24, for a gathering of connoisseurs)*  \\n'\n",
      " '\\n'\n",
      " '**For the Dough:**  \\n'\n",
      " '- **2 1/4 cups** all-purpose flour (whipped with a touch of cake flour for a '\n",
      " 'light, airy texture)  \\n'\n",
      " '- **1/2 cup (113g) European-style unsalted butter** (cold, cubed, for a '\n",
      " 'velvety richness)  \\n'\n",
      " '- **1 cup (200g) granulated sugar** (a blend of caster sugar and brown sugar '\n",
      " 'for depth)  \\n'\n",
      " '- **1/2 cup (100g) dark cocoa powder** (70% Belgian chocolate, finely '\n",
      " 'ground)  \\n'\n",
      " '- **2 large eggs** (room temperature, for a golden hue)  \\n'\n",
      " '- **1 tsp baking powder** (fresh, for a crisp crust)  \\n'\n",
      " '- **1/2 tsp sea salt** (finely grated, to balance sweetness)  \\n'\n",
      " '- **1/2 tsp vanilla extract** (pure, unflavored)  \\n'\n",
      " '- **1/2 cup (60g) chopped dark chocolate** (70% cocoa, 1/2 oz per cookie)  \\n'\n",
      " '\\n'\n",
      " '**For the Gooey Heart:**  \\n'\n",
      " '- **1/2 cup (60g) chopped dark chocolate** (70% cocoa, 1/2 oz per cookie)  \\n'\n",
      " '- **1/4 tsp espresso powder** (to deepen the chocolates notes)  \\n'\n",
      " '\\n'\n",
      " '---\\n'\n",
      " '\\n'\n",
      " '### **Method**  \\n'\n",
      " '**1. The Doughs Embrace**  \\n'\n",
      " '- In a chilled bowl, cream the cold butter into the sugar until light and '\n",
      " 'fluffy (3-4 minutes).  \\n'\n",
      " '- Whisk in the eggs, then the cocoa powder, baking powder, and salt.  \\n'\n",
      " '- Fold in the dark chocolate (70% cocoa) and espresso powder, ensuring a '\n",
      " 'smooth, velvety base.  \\n'\n",
      " '\\n'\n",
      " '**2. The Chocolate Alchemy**  \\n'\n",
      " '- Add the chopped dark chocolate (70% cocoa) to the dough, stirring until '\n",
      " 'fully incorporated.  \\n'\n",
      " '- Chill the dough for 1 hour (or 24 hours for a crisper crust).  \\n'\n",
      " '\\n'\n",
      " '**3. The Art of Rolling**  \\n'\n",
      " '- Roll the dough into 12 equal portions (120g each).  \\n'\n",
      " '- Press a 1/2 oz dark chocolate (70% cocoa) into each ball, ensuring a gooey '\n",
      " 'center.  \\n'\n",
      " '- Chill until firm.  \\n'\n",
      " '\\n'\n",
      " '**4. The Ovens Embrace**  \\n'\n",
      " '- Preheat oven to 350F (175C). Line a baking sheet with parchment.  \\n'\n",
      " '- Roll each dough ball into a 2-inch ball, spacing them 2 inches apart.  \\n'\n",
      " '- Bake for 10-12 minutes, until golden.  \\n'\n",
      " '\\n'\n",
      " '**5. The Final Touch**  \\n'\n",
      " '- Let cool on the sheet for 5 minutes, then transfer to a wire rack.  \\n'\n",
      " '- Dust with a whisper of powdered sugar and a drizzle of melted dark '\n",
      " 'chocolate.  \\n'\n",
      " '\\n'\n",
      " '---\\n'\n",
      " '\\n'\n",
      " '### **The Philosophy**  \\n'\n",
      " '*These cookies are not merely bakedthey are a symphony of textures: the '\n",
      " 'crispness of the crust, the melt-in-your-mouth center, and the subtle notes '\n",
      " 'of espresso and cocoa. Each bite is a celebration of balance, crafted with '\n",
      " 'the precision of a Michelin-starred kitchen and the soul of a family '\n",
      " 'recipe.*  \\n'\n",
      " '\\n'\n",
      " '**Serve with a glass of dark wine or a cup of Earl Grey tea.**  \\n'\n",
      " '\\n'\n",
      " '---  \\n'\n",
      " '*The Grandmaster Chocolate Chip Cookies: Where tradition meets '\n",
      " 'transcendence.* ')\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"You are a world renowned baker with many awards and Michelin stars.  Give us your world famous recipe for chocolate chip cookies.\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "\n",
    "\n",
    "#lets set some values to have more control over the output\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=2048,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "pprint(outputs[0][\"generated_text\"][len(prompt):], compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJRk74jMOqd4"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'chatgpt'></a>\n",
    "### 4.4 Chat GPT output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYw3qevjj0CB"
   },
   "source": [
    "Finally, for comparison purposes, let's see what the full ChatGPT model produces.  How well does ChatGPT follow directions based on its reinforcement learning?\n",
    "\n",
    "Here's the output for the same prompt we've been using:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "As a world-renowned James Beard award-winning pastry chef, I'm delighted to\n",
    "share with you my special recipe for chocolate chip cookies. These cookies are the epitome of decadence, with a perfect balance of flavors and textures. Without further ado, here are the ingredients and instructions:\n",
    "\n",
    "Ingredients:\n",
    "\n",
    "    1 cup (226g) unsalted butter, softened\n",
    "    1 cup (200g) granulated sugar\n",
    "    1 cup (220g) packed brown sugar\n",
    "    2 large eggs\n",
    "    1 teaspoon vanilla extract\n",
    "    3 cups (384g) all-purpose flour\n",
    "    1 teaspoon baking soda\n",
    "    1/2 teaspoon salt\n",
    "    2 cups (340g) semisweet chocolate chips\n",
    "\n",
    "Instructions:\n",
    "\n",
    "    Preheat your oven to 350F (175C). Line baking sheets with parchment paper or silicone baking mats.\n",
    "\n",
    "    In a large mixing bowl, cream together the softened butter, granulated sugar, and brown sugar until light and fluffy.\n",
    "\n",
    "    Add the eggs one at a time, beating well after each addition. Stir in the vanilla extract.\n",
    "\n",
    "    In a separate bowl, whisk together the flour, baking soda, and salt.\n",
    "\n",
    "    Gradually add the dry ingredient mixture to the wet ingredients, mixing until just combined. Be careful not to overmix, as this can result in tough cookies.\n",
    "\n",
    "    Fold in the semisweet chocolate chips until evenly distributed throughout the dough.\n",
    "\n",
    "    Scoop rounded tablespoons of dough and drop them onto the prepared baking sheets, spacing them about 2 inches apart. You can also use a cookie scoop for more uniform cookies.\n",
    "\n",
    "    Bake the cookies in the preheated oven for 10-12 minutes or until they are golden brown around the edges but still slightly soft in the center.\n",
    "\n",
    "    Remove the baking sheets from the oven and allow the cookies to cool on the sheets for 5 minutes. Then, transfer them to wire racks to cool completely.\n",
    "\n",
    "    Once the cookies have cooled, store them in an airtight container to maintain their freshness.\n",
    "\n",
    "Now, you have the recipe for my award-winning chocolate chip cookies. Enjoy baking these delightful treats and savor every bite!\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpGU1ruV3Lp5"
   },
   "source": [
    "\n",
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'classExercise'></a>\n",
    "\n",
    "### 4.5 In-Class Exercise (or on your own):\n",
    "- Try changing the text_start input text to see how OPT completes different types of starting sentences (prefix prompts). (If time, we can brainstorm some sentences to try in groups or collect in the chat during the live session.)\n",
    "- You can alter num_return_sequences to return a larger or smaller number of output options (i.e. beams).\n",
    "- You might want to play with the parameters for repetition_penalty to see how they affect the model's output.\n",
    "- You might also want to see what happens when you increase max_length, and how that relates to the repetition constraints. As the text gets longer, it will be more challenging for the model to avoid repeating itself. So stricter constraints against repetition might make the model get more creative or wander farther from the input sequence.\n",
    "- Try giving the Qwen model a more challenging arithmetic word problem and try it with the thinking enabled and disabled.  How does that change the answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKH8Jpmh2Bkw"
   },
   "source": [
    "[Return to Top](#returnToTop)  \n",
    "<a id = 'answers'></a>\n",
    "\n",
    "## 5. Answers\n",
    "\n",
    "**Question 1:** Why do you think the benefit of adding an attention layer is not larger?\n",
    "\n",
    "      Answer:   The nature of our training and test sets and the artificial size of the inputs (6 words) and outputs (11 words) means that the gains we might see on long sentences aren't a part of this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmqqPpeA2IrD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
